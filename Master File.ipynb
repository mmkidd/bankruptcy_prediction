{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "data = pd.read_csv('data.csv', index_col = 0)\n",
    "y = data['X65']\n",
    "X = data.drop(['X65'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Interpolate missing vales\n",
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp = imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "X = pd.DataFrame(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.49988</td>\n",
       "      <td>0.47225</td>\n",
       "      <td>1.9447</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.99601</td>\n",
       "      <td>1.6996</td>\n",
       "      <td>0.49788</td>\n",
       "      <td>...</td>\n",
       "      <td>2304.6</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.42002</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.350</td>\n",
       "      <td>3.4000</td>\n",
       "      <td>60.9870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.69592</td>\n",
       "      <td>0.26713</td>\n",
       "      <td>1.5548</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.43695</td>\n",
       "      <td>1.3090</td>\n",
       "      <td>0.30408</td>\n",
       "      <td>...</td>\n",
       "      <td>6332.7</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.81774</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.69484</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.270</td>\n",
       "      <td>2.7185</td>\n",
       "      <td>5.2078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.30734</td>\n",
       "      <td>0.45879</td>\n",
       "      <td>2.4928</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.14988</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.86610</td>\n",
       "      <td>1.0571</td>\n",
       "      <td>0.57353</td>\n",
       "      <td>...</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.14207</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.435</td>\n",
       "      <td>4.2228</td>\n",
       "      <td>5.5497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2       3        4        5         6        7   \\\n",
       "1  0.209120  0.49988  0.47225  1.9447  14.7860  0.00000  0.258340  0.99601   \n",
       "2  0.248660  0.69592  0.26713  1.5548  -1.1523  0.00000  0.309060  0.43695   \n",
       "3  0.081483  0.30734  0.45879  2.4928  51.9520  0.14988  0.092704  1.86610   \n",
       "\n",
       "       8        9    ...          54        55       56       57       58  \\\n",
       "1  1.6996  0.49788   ...      2304.6  0.121300  0.42002  0.85300  0.00000   \n",
       "2  1.3090  0.30408   ...      6332.7  0.241140  0.81774  0.76599  0.69484   \n",
       "3  1.0571  0.57353   ...     20545.0  0.054015  0.14207  0.94598  0.00000   \n",
       "\n",
       "       59      60       61      62       63  \n",
       "1  4.1486  3.2732  107.350  3.4000  60.9870  \n",
       "2  4.9909  3.9510  134.270  2.7185   5.2078  \n",
       "3  4.5746  3.6147   86.435  4.2228   5.5497  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>4.340500e+04</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.340500e+04</td>\n",
       "      <td>4.340500e+04</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>4.340500e+04</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>4.340500e+04</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>4.340500e+04</td>\n",
       "      <td>43405.000000</td>\n",
       "      <td>43405.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.035160</td>\n",
       "      <td>0.590212</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>6.314702</td>\n",
       "      <td>-3.853466e+02</td>\n",
       "      <td>-0.056107</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>12.640779</td>\n",
       "      <td>2.652166</td>\n",
       "      <td>0.626868</td>\n",
       "      <td>...</td>\n",
       "      <td>7.672188e+03</td>\n",
       "      <td>-2.621959e+01</td>\n",
       "      <td>-0.010510</td>\n",
       "      <td>3.002644e+01</td>\n",
       "      <td>1.333288</td>\n",
       "      <td>4.480858e+02</td>\n",
       "      <td>17.033202</td>\n",
       "      <td>1.502328e+03</td>\n",
       "      <td>9.343074</td>\n",
       "      <td>72.788592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.993833</td>\n",
       "      <td>5.842209</td>\n",
       "      <td>5.438928</td>\n",
       "      <td>294.978029</td>\n",
       "      <td>6.118020e+04</td>\n",
       "      <td>7.200663</td>\n",
       "      <td>5.712548</td>\n",
       "      <td>505.346176</td>\n",
       "      <td>62.926207</td>\n",
       "      <td>14.669245</td>\n",
       "      <td>...</td>\n",
       "      <td>7.005229e+04</td>\n",
       "      <td>5.320062e+03</td>\n",
       "      <td>13.672969</td>\n",
       "      <td>5.329289e+03</td>\n",
       "      <td>122.094599</td>\n",
       "      <td>3.153355e+04</td>\n",
       "      <td>552.399187</td>\n",
       "      <td>1.390628e+05</td>\n",
       "      <td>123.985521</td>\n",
       "      <td>2347.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-463.890000</td>\n",
       "      <td>-430.870000</td>\n",
       "      <td>-479.960000</td>\n",
       "      <td>-0.403110</td>\n",
       "      <td>-1.190300e+07</td>\n",
       "      <td>-508.410000</td>\n",
       "      <td>-517.480000</td>\n",
       "      <td>-141.410000</td>\n",
       "      <td>-3.496000</td>\n",
       "      <td>-479.910000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.805200e+06</td>\n",
       "      <td>-1.108300e+06</td>\n",
       "      <td>-1667.300000</td>\n",
       "      <td>-1.986900e+02</td>\n",
       "      <td>-327.970000</td>\n",
       "      <td>-1.244000e+01</td>\n",
       "      <td>-12.656000</td>\n",
       "      <td>-2.336500e+06</td>\n",
       "      <td>-1.543200</td>\n",
       "      <td>-10677.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.269010</td>\n",
       "      <td>0.021540</td>\n",
       "      <td>1.050800</td>\n",
       "      <td>-4.949000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005783</td>\n",
       "      <td>0.431070</td>\n",
       "      <td>1.018500</td>\n",
       "      <td>0.295530</td>\n",
       "      <td>...</td>\n",
       "      <td>2.757300e+01</td>\n",
       "      <td>8.959000e-03</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>8.755400e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.729400e+00</td>\n",
       "      <td>4.515600</td>\n",
       "      <td>4.220500e+01</td>\n",
       "      <td>3.100200</td>\n",
       "      <td>2.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.049636</td>\n",
       "      <td>0.471940</td>\n",
       "      <td>0.196580</td>\n",
       "      <td>1.574000</td>\n",
       "      <td>-1.184000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059653</td>\n",
       "      <td>1.074100</td>\n",
       "      <td>1.195500</td>\n",
       "      <td>0.505990</td>\n",
       "      <td>...</td>\n",
       "      <td>1.088400e+03</td>\n",
       "      <td>5.270100e-02</td>\n",
       "      <td>0.119650</td>\n",
       "      <td>9.511200e-01</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>1.040000e+01</td>\n",
       "      <td>6.649500</td>\n",
       "      <td>7.153500e+01</td>\n",
       "      <td>5.102900</td>\n",
       "      <td>4.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.129560</td>\n",
       "      <td>0.688270</td>\n",
       "      <td>0.403260</td>\n",
       "      <td>2.806000</td>\n",
       "      <td>5.046100e+01</td>\n",
       "      <td>0.089441</td>\n",
       "      <td>0.150810</td>\n",
       "      <td>2.639100</td>\n",
       "      <td>2.063000</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>...</td>\n",
       "      <td>4.993700e+03</td>\n",
       "      <td>1.287800e-01</td>\n",
       "      <td>0.284570</td>\n",
       "      <td>9.928000e-01</td>\n",
       "      <td>0.236280</td>\n",
       "      <td>2.389000e+01</td>\n",
       "      <td>10.443000</td>\n",
       "      <td>1.177400e+02</td>\n",
       "      <td>8.650900</td>\n",
       "      <td>10.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>94.280000</td>\n",
       "      <td>480.960000</td>\n",
       "      <td>28.336000</td>\n",
       "      <td>53433.000000</td>\n",
       "      <td>1.250100e+06</td>\n",
       "      <td>543.250000</td>\n",
       "      <td>649.230000</td>\n",
       "      <td>53432.000000</td>\n",
       "      <td>9742.300000</td>\n",
       "      <td>1099.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123700e+06</td>\n",
       "      <td>2.931500e+02</td>\n",
       "      <td>552.640000</td>\n",
       "      <td>1.108300e+06</td>\n",
       "      <td>23853.000000</td>\n",
       "      <td>4.818700e+06</td>\n",
       "      <td>108000.000000</td>\n",
       "      <td>2.501600e+07</td>\n",
       "      <td>23454.000000</td>\n",
       "      <td>294770.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  43405.000000  43405.000000  43405.000000  43405.000000  4.340500e+04   \n",
       "mean       0.035160      0.590212      0.114431      6.314702 -3.853466e+02   \n",
       "std        2.993833      5.842209      5.438928    294.978029  6.118020e+04   \n",
       "min     -463.890000   -430.870000   -479.960000     -0.403110 -1.190300e+07   \n",
       "25%        0.003434      0.269010      0.021540      1.050800 -4.949000e+01   \n",
       "50%        0.049636      0.471940      0.196580      1.574000 -1.184000e+00   \n",
       "75%        0.129560      0.688270      0.403260      2.806000  5.046100e+01   \n",
       "max       94.280000    480.960000     28.336000  53433.000000  1.250100e+06   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  43405.000000  43405.000000  43405.000000  43405.000000  43405.000000   \n",
       "mean      -0.056107      0.093478     12.640779      2.652166      0.626868   \n",
       "std        7.200663      5.712548    505.346176     62.926207     14.669245   \n",
       "min     -508.410000   -517.480000   -141.410000     -3.496000   -479.910000   \n",
       "25%        0.000000      0.005783      0.431070      1.018500      0.295530   \n",
       "50%        0.000000      0.059653      1.074100      1.195500      0.505990   \n",
       "75%        0.089441      0.150810      2.639100      2.063000      0.709090   \n",
       "max      543.250000    649.230000  53432.000000   9742.300000   1099.500000   \n",
       "\n",
       "           ...                  54            55            56            57  \\\n",
       "count      ...        4.340500e+04  4.340500e+04  43405.000000  4.340500e+04   \n",
       "mean       ...        7.672188e+03 -2.621959e+01     -0.010510  3.002644e+01   \n",
       "std        ...        7.005229e+04  5.320062e+03     13.672969  5.329289e+03   \n",
       "min        ...       -1.805200e+06 -1.108300e+06  -1667.300000 -1.986900e+02   \n",
       "25%        ...        2.757300e+01  8.959000e-03      0.014632  8.755400e-01   \n",
       "50%        ...        1.088400e+03  5.270100e-02      0.119650  9.511200e-01   \n",
       "75%        ...        4.993700e+03  1.287800e-01      0.284570  9.928000e-01   \n",
       "max        ...        6.123700e+06  2.931500e+02    552.640000  1.108300e+06   \n",
       "\n",
       "                 58            59             60            61            62  \\\n",
       "count  43405.000000  4.340500e+04   43405.000000  4.340500e+04  43405.000000   \n",
       "mean       1.333288  4.480858e+02      17.033202  1.502328e+03      9.343074   \n",
       "std      122.094599  3.153355e+04     552.399187  1.390628e+05    123.985521   \n",
       "min     -327.970000 -1.244000e+01     -12.656000 -2.336500e+06     -1.543200   \n",
       "25%        0.000000  5.729400e+00       4.515600  4.220500e+01      3.100200   \n",
       "50%        0.006389  1.040000e+01       6.649500  7.153500e+01      5.102900   \n",
       "75%        0.236280  2.389000e+01      10.443000  1.177400e+02      8.650900   \n",
       "max    23853.000000  4.818700e+06  108000.000000  2.501600e+07  23454.000000   \n",
       "\n",
       "                  63  \n",
       "count   43405.000000  \n",
       "mean       72.788592  \n",
       "std      2347.072100  \n",
       "min    -10677.000000  \n",
       "25%         2.207800  \n",
       "50%         4.389200  \n",
       "75%        10.461000  \n",
       "max    294770.000000  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Description\n",
    "from scipy.stats import describe\n",
    "\n",
    "#Data Preview\n",
    "display(X[1:4])\n",
    "\n",
    "#Full data descriptive statistics\n",
    "pd.DataFrame(X).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c693cb41d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c693bd9358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Correlation map\n",
    "import seaborn as sns; sns.set()\n",
    "sns.heatmap(pd.DataFrame(X).corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Base Model - Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training ROC AUC Score is 0.6293266230185585\n",
      "The validation ROC AUC Score is 0.5993803471284129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 1.0, 1: 50.0},\n",
       "            criterion='gini', max_depth=24, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=42,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=23,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit base model, parameter search\n",
    "\n",
    "#Scale Data, split 60/20/20\n",
    "from sklearn.model_selection import train_test_split\n",
    "X0 = preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X0, y, test_size = 0.2, random_state = 23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 23) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Do GridSearchCV to determine parameters\n",
    "parameters = {'max_depth':[12,18,24,30],'min_samples_leaf':[10,18,26,34,42,50],\n",
    "            'class_weight':[{0: 1., 1: 1.},\n",
    "                            {0: 1., 1: 20.},\n",
    "                            {0: 1., 1: 30.},\n",
    "                            {0: 1., 1: 40.},\n",
    "                            {0: 1., 1: 50.}]}\n",
    "\n",
    "model = RandomForestClassifier(random_state = 23)\n",
    "scorer = make_scorer(roc_auc_score) #AUC scoring\n",
    "grid_obj = GridSearchCV(model, parameters, scoring=scorer, verbose=1, cv=3)\n",
    "\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator.\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Fit the new model.\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the new model.\n",
    "best_train_predictions = best_clf.predict(X_train)\n",
    "best_val_predictions = best_clf.predict(X_val)\n",
    "\n",
    "print('The training ROC AUC Score is', roc_auc_score(best_train_predictions, y_train))\n",
    "print('The validation ROC AUC Score is', roc_auc_score(best_val_predictions, y_val))\n",
    "\n",
    "# Let's also explore what parameters ended up being used in the new model.\n",
    "best_clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.65575933, 1.53961531, 1.72866488, 1.50357938, 1.47852182,\n",
       "        1.25756232, 1.86437551, 1.89105884, 1.56599649, 1.746689  ,\n",
       "        1.58990852, 1.55954893, 1.93564208, 1.86150861, 1.56223369,\n",
       "        1.45100768, 1.42980377, 1.51629337, 1.59364613, 1.46436954,\n",
       "        1.45605334, 1.3909591 , 1.31679217, 1.3855137 , 1.38522871,\n",
       "        1.31423863, 1.17244959, 1.09915813, 1.22675951, 1.22799412,\n",
       "        1.33692543, 1.17890366, 1.17205977, 1.22020594, 1.39236132,\n",
       "        1.15045079, 1.40125593, 1.37232629, 1.42575153, 1.42919143,\n",
       "        1.29419963, 1.45073883, 1.72908537, 1.42399152, 1.20319684,\n",
       "        1.23166768, 1.20650403, 2.31813264, 2.03139997, 1.48341155,\n",
       "        1.44995983, 1.43676416, 1.32097077, 1.75645566, 1.72817262,\n",
       "        1.29190707, 1.26178416, 1.19846209, 1.48473509, 1.30458434,\n",
       "        2.30234051, 1.37119937, 1.32877811, 1.29091485, 1.22786776,\n",
       "        1.20357299, 1.62231064, 1.26305103, 1.6002361 , 1.27475015,\n",
       "        1.22024592, 1.14684542, 1.76828392, 1.69108295, 1.73273683,\n",
       "        1.44508775, 1.59692136, 1.67004808, 1.65132944, 1.8440396 ,\n",
       "        1.36413399, 1.27989769, 1.17452105, 1.12396042, 1.40565666,\n",
       "        1.29702234, 1.46743258, 1.20708855, 1.09756239, 1.03172755,\n",
       "        1.38830201, 1.28162885, 1.23546211, 1.25644183, 1.39392749,\n",
       "        1.14170432, 1.29730082, 1.20710993, 1.27530281, 1.10050615,\n",
       "        1.0709823 , 1.14446195, 1.40556335, 1.1934046 , 1.24852475,\n",
       "        1.1718568 , 1.24180071, 1.32767145, 1.50976642, 1.64465785,\n",
       "        1.25360441, 1.45798302, 1.22218529, 1.11553256, 1.26065954,\n",
       "        1.23851267, 1.16103689, 1.2949183 , 1.13482666, 1.16134866]),\n",
       " 'mean_score_time': array([0.0219299 , 0.0182964 , 0.02161741, 0.02261456, 0.02060986,\n",
       "        0.0176305 , 0.03169513, 0.02260399, 0.01994554, 0.02236342,\n",
       "        0.02193371, 0.0202179 , 0.02159921, 0.02294779, 0.01928322,\n",
       "        0.01993004, 0.01927249, 0.01995524, 0.01795157, 0.01863368,\n",
       "        0.01795371, 0.01993736, 0.0202601 , 0.01861779, 0.02328507,\n",
       "        0.02226766, 0.01701299, 0.01795133, 0.01794465, 0.019291  ,\n",
       "        0.0199484 , 0.01695617, 0.02027861, 0.02062154, 0.02162083,\n",
       "        0.01961827, 0.0225989 , 0.02277724, 0.02194357, 0.02692731,\n",
       "        0.02195795, 0.0222764 , 0.02559169, 0.0189414 , 0.01960746,\n",
       "        0.01794219, 0.02096462, 0.04354954, 0.05452156, 0.03092694,\n",
       "        0.02092004, 0.02725975, 0.02494438, 0.03124889, 0.03026303,\n",
       "        0.02060771, 0.01945138, 0.02626387, 0.02579188, 0.02827032,\n",
       "        0.02825801, 0.0196054 , 0.02924339, 0.02525568, 0.02027941,\n",
       "        0.0282592 , 0.02827756, 0.02028807, 0.02392936, 0.01895301,\n",
       "        0.01877499, 0.01762056, 0.02826985, 0.02892447, 0.02228228,\n",
       "        0.02469436, 0.0273176 , 0.03429961, 0.02768548, 0.02527595,\n",
       "        0.02262656, 0.02226543, 0.02028092, 0.02159993, 0.01993235,\n",
       "        0.02261051, 0.02940702, 0.02013652, 0.02060779, 0.01928592,\n",
       "        0.01994634, 0.02029101, 0.01729488, 0.02293078, 0.03657913,\n",
       "        0.01894053, 0.01863448, 0.02115711, 0.01993799, 0.01860825,\n",
       "        0.01728797, 0.02162234, 0.02061137, 0.01965396, 0.01828289,\n",
       "        0.01759171, 0.025769  , 0.02959506, 0.02393707, 0.02658431,\n",
       "        0.01830149, 0.02326488, 0.01959467, 0.01896286, 0.01961342,\n",
       "        0.01863718, 0.02029936, 0.01964466, 0.01663907, 0.0199546 ]),\n",
       " 'mean_test_score': array([0.59119264, 0.58596976, 0.57713577, 0.53842086, 0.53994889,\n",
       "        0.52949081, 0.60739192, 0.60095774, 0.56782129, 0.54002359,\n",
       "        0.53799081, 0.52302219, 0.60740933, 0.59733842, 0.56864958,\n",
       "        0.54042807, 0.53799081, 0.52421283, 0.60781386, 0.59733842,\n",
       "        0.56864958, 0.54042807, 0.53799081, 0.52421283, 0.7420769 ,\n",
       "        0.75646913, 0.77645978, 0.78707948, 0.78959752, 0.79169948,\n",
       "        0.71536084, 0.74986887, 0.76128143, 0.78113123, 0.78029277,\n",
       "        0.79010656, 0.71761976, 0.75100348, 0.76210968, 0.78143357,\n",
       "        0.78029277, 0.79010656, 0.72131261, 0.75100348, 0.76210968,\n",
       "        0.78143357, 0.78029277, 0.79010656, 0.75800402, 0.76759793,\n",
       "        0.77865355, 0.789412  , 0.79720183, 0.79522945, 0.72120211,\n",
       "        0.75997707, 0.77305646, 0.79103101, 0.79480906, 0.79556705,\n",
       "        0.71492734, 0.75816194, 0.77222724, 0.79295614, 0.79381869,\n",
       "        0.79291843, 0.71660221, 0.75816194, 0.77222724, 0.79295614,\n",
       "        0.79381869, 0.79291843, 0.76995974, 0.7856622 , 0.78896229,\n",
       "        0.79894178, 0.79956449, 0.7974168 , 0.72794741, 0.77125473,\n",
       "        0.78253314, 0.80240688, 0.79623534, 0.79720804, 0.70772434,\n",
       "        0.7619804 , 0.7822912 , 0.80287313, 0.79665998, 0.79720804,\n",
       "        0.70826481, 0.75981689, 0.7822912 , 0.80287313, 0.79665998,\n",
       "        0.79720804, 0.76879157, 0.77484614, 0.79430632, 0.79347286,\n",
       "        0.79985057, 0.79299129, 0.72546298, 0.76466638, 0.77688715,\n",
       "        0.79738584, 0.80751872, 0.80342884, 0.71202343, 0.75475868,\n",
       "        0.7768249 , 0.79653566, 0.80761818, 0.80425796, 0.71425591,\n",
       "        0.75429373, 0.7768249 , 0.79653566, 0.80761818, 0.80425796]),\n",
       " 'mean_train_score': array([0.63436294, 0.61448415, 0.59535294, 0.55474794, 0.55355323,\n",
       "        0.54265053, 0.65841637, 0.62294636, 0.5828422 , 0.55272526,\n",
       "        0.55233669, 0.53598142, 0.66064325, 0.62317837, 0.58121596,\n",
       "        0.55211846, 0.55233669, 0.53800407, 0.66084527, 0.62317837,\n",
       "        0.58121596, 0.55211846, 0.55233669, 0.53800407, 0.94392762,\n",
       "        0.93555417, 0.92263184, 0.91608222, 0.90644634, 0.89344404,\n",
       "        0.95695201, 0.9404726 , 0.93070443, 0.92262171, 0.9067569 ,\n",
       "        0.89517922, 0.95885012, 0.94268527, 0.93060323, 0.92233814,\n",
       "        0.9067569 , 0.89517922, 0.9588602 , 0.94268527, 0.93060323,\n",
       "        0.92233814, 0.9067569 , 0.89517922, 0.94929359, 0.93886828,\n",
       "        0.93027186, 0.91426972, 0.90943349, 0.89768758, 0.96282033,\n",
       "        0.94881854, 0.93322056, 0.92300518, 0.91094278, 0.90237032,\n",
       "        0.96511745, 0.95093843, 0.93434198, 0.92293366, 0.91107472,\n",
       "        0.90244132, 0.964723  , 0.95093843, 0.93434198, 0.92293366,\n",
       "        0.91107472, 0.90244132, 0.9451282 , 0.93645682, 0.92695578,\n",
       "        0.92053221, 0.90566715, 0.8955759 , 0.96407031, 0.95208029,\n",
       "        0.93636803, 0.92442701, 0.91110953, 0.89710359, 0.96516262,\n",
       "        0.95365451, 0.93715718, 0.92575056, 0.91155388, 0.89710359,\n",
       "        0.96827379, 0.9536142 , 0.93715718, 0.92575056, 0.91155388,\n",
       "        0.89710359, 0.93927877, 0.93551891, 0.92290671, 0.91382886,\n",
       "        0.90005734, 0.88483019, 0.9608774 , 0.95384521, 0.93545832,\n",
       "        0.92574369, 0.90612297, 0.89316638, 0.96227658, 0.95338656,\n",
       "        0.93479973, 0.92438131, 0.90779758, 0.89328778, 0.9659219 ,\n",
       "        0.95429542, 0.93479973, 0.92438131, 0.90779758, 0.89328778]),\n",
       " 'param_class_weight': masked_array(data=[{0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0}, {0: 1.0, 1: 1.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 20.0}, {0: 1.0, 1: 20.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 30.0}, {0: 1.0, 1: 30.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 40.0}, {0: 1.0, 1: 40.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0},\n",
       "                    {0: 1.0, 1: 50.0}, {0: 1.0, 1: 50.0}],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[12, 12, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 24, 24,\n",
       "                    24, 24, 24, 24, 30, 30, 30, 30, 30, 30, 12, 12, 12, 12,\n",
       "                    12, 12, 18, 18, 18, 18, 18, 18, 24, 24, 24, 24, 24, 24,\n",
       "                    30, 30, 30, 30, 30, 30, 12, 12, 12, 12, 12, 12, 18, 18,\n",
       "                    18, 18, 18, 18, 24, 24, 24, 24, 24, 24, 30, 30, 30, 30,\n",
       "                    30, 30, 12, 12, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18,\n",
       "                    24, 24, 24, 24, 24, 24, 30, 30, 30, 30, 30, 30, 12, 12,\n",
       "                    12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 24, 24, 24, 24,\n",
       "                    24, 24, 30, 30, 30, 30, 30, 30],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[10, 18, 26, 34, 42, 50, 10, 18, 26, 34, 42, 50, 10, 18,\n",
       "                    26, 34, 42, 50, 10, 18, 26, 34, 42, 50, 10, 18, 26, 34,\n",
       "                    42, 50, 10, 18, 26, 34, 42, 50, 10, 18, 26, 34, 42, 50,\n",
       "                    10, 18, 26, 34, 42, 50, 10, 18, 26, 34, 42, 50, 10, 18,\n",
       "                    26, 34, 42, 50, 10, 18, 26, 34, 42, 50, 10, 18, 26, 34,\n",
       "                    42, 50, 10, 18, 26, 34, 42, 50, 10, 18, 26, 34, 42, 50,\n",
       "                    10, 18, 26, 34, 42, 50, 10, 18, 26, 34, 42, 50, 10, 18,\n",
       "                    26, 34, 42, 50, 10, 18, 26, 34, 42, 50, 10, 18, 26, 34,\n",
       "                    42, 50, 10, 18, 26, 34, 42, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'class_weight': {0: 1.0, 1: 1.0},\n",
       "   'max_depth': 12,\n",
       "   'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 12, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 12, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 12, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 12, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 12, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 18, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 18, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 18, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 18, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 18, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 18, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 24, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 24, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 24, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 24, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 24, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 24, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 30, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 30, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 30, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 30, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 30, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 1.0}, 'max_depth': 30, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 12, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 12, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 12, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 12, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 12, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 12, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 18, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 18, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 18, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 18, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 18, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 18, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 24, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 24, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 24, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 24, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 24, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 24, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 30, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 30, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 30, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 30, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 30, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 20.0}, 'max_depth': 30, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 12, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 12, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 12, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 12, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 12, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 12, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 18, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 18, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 18, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 18, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 18, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 18, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 24, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 24, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 24, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 24, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 24, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 24, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 30, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 30, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 30, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 30, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 30, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 30.0}, 'max_depth': 30, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 12, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 12, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 12, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 12, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 12, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 12, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 18, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 18, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 18, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 18, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 18, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 18, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 24, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 24, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 24, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 24, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 24, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 24, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 30, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 30, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 30, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 30, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 30, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 40.0}, 'max_depth': 30, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 12, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 12, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 12, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 12, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 12, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 12, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 18, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 18, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 18, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 18, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 18, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 18, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 24, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 24, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 24, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 24, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 24, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 24, 'min_samples_leaf': 50},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 30, 'min_samples_leaf': 10},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 30, 'min_samples_leaf': 18},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 30, 'min_samples_leaf': 26},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 30, 'min_samples_leaf': 34},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0}, 'max_depth': 30, 'min_samples_leaf': 42},\n",
       "  {'class_weight': {0: 1.0, 1: 50.0},\n",
       "   'max_depth': 30,\n",
       "   'min_samples_leaf': 50}],\n",
       " 'rank_test_score': array([103, 104, 105, 113, 112, 117,  99, 100, 108, 111, 114, 120,  98,\n",
       "        101, 106, 109, 114, 118,  97, 101, 106, 109, 114, 118,  84,  78,\n",
       "         59,  44,  41,  36,  91,  83,  72,  51,  52,  38,  89,  81,  69,\n",
       "         49,  52,  38,  87,  81,  69,  49,  52,  38,  77,  67,  55,  42,\n",
       "         18,  25,  88,  73,  61,  37,  26,  24,  92,  75,  62,  32,  28,\n",
       "         34,  90,  75,  62,  32,  28,  34,  65,  45,  43,  12,  11,  13,\n",
       "         85,  64,  46,   9,  23,  15,  96,  71,  47,   7,  19,  15,  95,\n",
       "         74,  47,   7,  19,  15,  66,  60,  27,  30,  10,  31,  86,  68,\n",
       "         56,  14,   3,   6,  94,  79,  57,  21,   1,   4,  93,  80,  57,\n",
       "         21,   1,   4]),\n",
       " 'split0_test_score': array([0.61246987, 0.5653753 , 0.58577502, 0.50968523, 0.54836568,\n",
       "        0.55084746, 0.61483071, 0.59073856, 0.57372895, 0.52905569,\n",
       "        0.54358354, 0.54600484, 0.6185836 , 0.58952791, 0.5749396 ,\n",
       "        0.52905569, 0.54358354, 0.54957634, 0.6185836 , 0.58952791,\n",
       "        0.5749396 , 0.52905569, 0.54358354, 0.54957634, 0.73101276,\n",
       "        0.75735535, 0.77437288, 0.79628953, 0.78552381, 0.78268977,\n",
       "        0.69892451, 0.74796587, 0.75021632, 0.79271804, 0.78074034,\n",
       "        0.77221584, 0.70576121, 0.7456655 , 0.75172931, 0.79271804,\n",
       "        0.78074034, 0.77221584, 0.7168385 , 0.7456655 , 0.75172931,\n",
       "        0.79271804, 0.78074034, 0.77221584, 0.74373715, 0.76935545,\n",
       "        0.75277701, 0.77227894, 0.79020611, 0.7801042 , 0.72108303,\n",
       "        0.7561447 , 0.75579771, 0.77941666, 0.7891729 , 0.78131485,\n",
       "        0.71702385, 0.74948676, 0.75579771, 0.77312284, 0.7891729 ,\n",
       "        0.78131485, 0.72204787, 0.74948676, 0.75579771, 0.77312284,\n",
       "        0.7891729 , 0.78131485, 0.75621571, 0.77535616, 0.77536934,\n",
       "        0.79571517, 0.7915535 , 0.77600695, 0.71394136, 0.75276251,\n",
       "        0.76367684, 0.79135234, 0.78482588, 0.77249592, 0.69759556,\n",
       "        0.74017092, 0.76307218, 0.78796225, 0.78482588, 0.77249592,\n",
       "        0.70516313, 0.74017092, 0.76307218, 0.78796225, 0.78482588,\n",
       "        0.77249592, 0.75047663, 0.76398445, 0.77585966, 0.78222976,\n",
       "        0.79161924, 0.76815022, 0.7237528 , 0.7594796 , 0.75714379,\n",
       "        0.79232508, 0.79379341, 0.79168893, 0.7103094 , 0.74725081,\n",
       "        0.7593837 , 0.79462809, 0.79518415, 0.79168893, 0.69995772,\n",
       "        0.74725081, 0.7593837 , 0.79462809, 0.79518415, 0.79168893]),\n",
       " 'split0_train_score': array([0.64799778, 0.59757323, 0.5982405 , 0.526062  , 0.56917476,\n",
       "        0.56007282, 0.65527934, 0.61632344, 0.58431442, 0.54487268,\n",
       "        0.55825243, 0.55400485, 0.66680846, 0.61571665, 0.58428419,\n",
       "        0.54305229, 0.55825243, 0.56007282, 0.66680846, 0.61571665,\n",
       "        0.58428419, 0.54305229, 0.55825243, 0.56007282, 0.94271528,\n",
       "        0.9333319 , 0.92486395, 0.92039123, 0.90736498, 0.89450246,\n",
       "        0.95867435, 0.94043928, 0.92730882, 0.92011283, 0.90629838,\n",
       "        0.89146639, 0.95870249, 0.94331999, 0.92909688, 0.92011283,\n",
       "        0.90629838, 0.89146639, 0.95873273, 0.94331999, 0.92909688,\n",
       "        0.92011283, 0.90629838, 0.89146639, 0.949387  , 0.93504504,\n",
       "        0.92881867, 0.9160229 , 0.90809708, 0.89683912, 0.96108385,\n",
       "        0.94926605, 0.93101748, 0.92307189, 0.90515172, 0.90266102,\n",
       "        0.96286145, 0.95217491, 0.93101748, 0.92098147, 0.90515172,\n",
       "        0.90266102, 0.96167809, 0.95217491, 0.93101748, 0.92098147,\n",
       "        0.90515172, 0.90266102, 0.94513638, 0.93045765, 0.92678778,\n",
       "        0.92438778, 0.90456279, 0.89144697, 0.96157807, 0.95142949,\n",
       "        0.93186102, 0.92128496, 0.91027516, 0.89216425, 0.96253932,\n",
       "        0.9520321 , 0.93450194, 0.92661891, 0.91027516, 0.89216425,\n",
       "        0.96278329, 0.9520321 , 0.93450194, 0.92661891, 0.91027516,\n",
       "        0.89216425, 0.93777153, 0.93148888, 0.92035604, 0.91308399,\n",
       "        0.89876181, 0.88338044, 0.96089102, 0.95400994, 0.93002504,\n",
       "        0.92342234, 0.90463581, 0.89266893, 0.96041867, 0.95185487,\n",
       "        0.92777717, 0.92251529, 0.90796378, 0.89266893, 0.96111617,\n",
       "        0.95185487, 0.92777717, 0.92251529, 0.90796378, 0.89266893]),\n",
       " 'split1_test_score': array([0.57148101, 0.59332567, 0.57038835, 0.55097087, 0.52305825,\n",
       "        0.51820388, 0.61165049, 0.60309487, 0.56905382, 0.52427184,\n",
       "        0.5315534 , 0.50728155, 0.60916283, 0.60309487, 0.57032788,\n",
       "        0.52427184, 0.5315534 , 0.50728155, 0.61037643, 0.60309487,\n",
       "        0.57032788, 0.52427184, 0.5315534 , 0.50728155, 0.7550434 ,\n",
       "        0.74704476, 0.77821319, 0.78714863, 0.78954514, 0.78604658,\n",
       "        0.72248848, 0.74848319, 0.77637365, 0.77579878, 0.77763832,\n",
       "        0.78768344, 0.7202427 , 0.74848319, 0.77637365, 0.77579878,\n",
       "        0.77763832, 0.78768344, 0.7202427 , 0.74848319, 0.77637365,\n",
       "        0.77579878, 0.77763832, 0.78768344, 0.76234624, 0.77760427,\n",
       "        0.7926809 , 0.79521963, 0.79936542, 0.79429047, 0.73712908,\n",
       "        0.76744673, 0.78405206, 0.80685905, 0.79051173, 0.79307687,\n",
       "        0.72898823, 0.766475  , 0.78156441, 0.80685905, 0.79051173,\n",
       "        0.79307687, 0.72898823, 0.766475  , 0.78156441, 0.80685905,\n",
       "        0.79051173, 0.79307687, 0.7824629 , 0.78687184, 0.78799003,\n",
       "        0.79997435, 0.80202082, 0.80296188, 0.74277803, 0.77595039,\n",
       "        0.78608151, 0.80912186, 0.79709748, 0.79646639, 0.71375998,\n",
       "        0.78147415, 0.78510979, 0.80912186, 0.79709748, 0.79646639,\n",
       "        0.70775249, 0.78147415, 0.78510979, 0.80912186, 0.79709748,\n",
       "        0.79646639, 0.78303263, 0.78669895, 0.80949743, 0.81031329,\n",
       "        0.80465495, 0.80717665, 0.74012175, 0.76230705, 0.78723889,\n",
       "        0.80507484, 0.81102098, 0.80789374, 0.71921902, 0.76096827,\n",
       "        0.78723889, 0.80507484, 0.81102098, 0.80789374, 0.73220574,\n",
       "        0.76096827, 0.78723889, 0.80507484, 0.81102098, 0.80789374]),\n",
       " 'split1_train_score': array([0.61996976, 0.61390916, 0.58903044, 0.56666667, 0.52848485,\n",
       "        0.52909091, 0.65939394, 0.61512128, 0.56906067, 0.53390916,\n",
       "        0.54481825, 0.51939394, 0.65333333, 0.61939394, 0.56481825,\n",
       "        0.53390916, 0.54481825, 0.51939394, 0.65393939, 0.61939394,\n",
       "        0.56481825, 0.53390916, 0.54481825, 0.51939394, 0.94418426,\n",
       "        0.93207254, 0.91836785, 0.91783582, 0.90424799, 0.89447659,\n",
       "        0.95649949, 0.93839713, 0.92969074, 0.92271047, 0.90354716,\n",
       "        0.89656689, 0.95704508, 0.93839713, 0.92969074, 0.92271047,\n",
       "        0.90354716, 0.89656689, 0.95704508, 0.93839713, 0.92969074,\n",
       "        0.92271047, 0.90354716, 0.89656689, 0.94680754, 0.93779473,\n",
       "        0.93257082, 0.9131476 , 0.9086755 , 0.90069475, 0.96332547,\n",
       "        0.95062532, 0.93241422, 0.91493012, 0.90922109, 0.90145063,\n",
       "        0.96432052, 0.9500179 , 0.93577847, 0.91493012, 0.90922109,\n",
       "        0.90145063, 0.96432052, 0.9500179 , 0.93577847, 0.91493012,\n",
       "        0.90922109, 0.90145063, 0.94894977, 0.93800598, 0.92469027,\n",
       "        0.91852635, 0.90430902, 0.89566716, 0.96451278, 0.9513591 ,\n",
       "        0.9342762 , 0.92406076, 0.90533701, 0.89830034, 0.96301905,\n",
       "        0.95027063, 0.93457855, 0.92406076, 0.90533701, 0.89830034,\n",
       "        0.96862477, 0.95027063, 0.93457855, 0.92406076, 0.90533701,\n",
       "        0.89830034, 0.94153943, 0.94067753, 0.92718855, 0.9182678 ,\n",
       "        0.90460418, 0.89071943, 0.96173521, 0.9551823 , 0.93755109,\n",
       "        0.92434643, 0.9015671 , 0.89080607, 0.96223429, 0.95269759,\n",
       "        0.93755109, 0.92434643, 0.9015671 , 0.89080607, 0.96559583,\n",
       "        0.95269759, 0.93755109, 0.92434643, 0.9015671 , 0.89080607]),\n",
       " 'split2_test_score': array([0.5896244 , 0.59921219, 0.57524272, 0.55461165, 0.54842274,\n",
       "        0.51941748, 0.59569236, 0.60904188, 0.56067961, 0.56674757,\n",
       "        0.53883495, 0.5157767 , 0.59447877, 0.59939361, 0.56067961,\n",
       "        0.56796117, 0.53883495, 0.5157767 , 0.59447877, 0.59939361,\n",
       "        0.56067961, 0.56796117, 0.53883495, 0.5157767 , 0.7401756 ,\n",
       "        0.76500815, 0.77679356, 0.77779813, 0.79372455, 0.80636481,\n",
       "        0.7246725 , 0.75315816, 0.75725513, 0.77487482, 0.78249985,\n",
       "        0.8104248 , 0.72685779, 0.75886327, 0.75822683, 0.77578194,\n",
       "        0.78249985, 0.8104248 , 0.72685779, 0.75886327, 0.75822683,\n",
       "        0.77578194, 0.78249985, 0.8104248 , 0.76793146, 0.75583252,\n",
       "        0.79050709, 0.80074072, 0.80203534, 0.81129727, 0.70539241,\n",
       "        0.7563398 , 0.77932231, 0.78681817, 0.80474434, 0.812313  ,\n",
       "        0.69876785, 0.75852509, 0.77932231, 0.79888951, 0.80177289,\n",
       "        0.80436623, 0.69876785, 0.75852509, 0.77932231, 0.79888951,\n",
       "        0.80177289, 0.80436623, 0.77120234, 0.79476083, 0.80353075,\n",
       "        0.80113644, 0.80512069, 0.81328587, 0.72712434, 0.78505503,\n",
       "        0.79784501, 0.80674821, 0.8067852 , 0.82266758, 0.71181911,\n",
       "        0.7642989 , 0.79869576, 0.81153799, 0.80805926, 0.82266758,\n",
       "        0.71187958, 0.75780762, 0.79869576, 0.81153799, 0.80805926,\n",
       "        0.82266758, 0.77286802, 0.77385616, 0.79756436, 0.78787617,\n",
       "        0.80327887, 0.80365111, 0.71251309, 0.77221396, 0.78628212,\n",
       "        0.79475789, 0.81774452, 0.81070603, 0.70654142, 0.75605798,\n",
       "        0.78385494, 0.78990352, 0.81665187, 0.81319369, 0.71060552,\n",
       "        0.75466297, 0.78385494, 0.78990352, 0.81665187, 0.81319369]),\n",
       " 'split2_train_score': array([0.63512128, 0.63197005, 0.59878788, 0.57151515, 0.56300007,\n",
       "        0.53878788, 0.66057583, 0.63739436, 0.59515152, 0.57939394,\n",
       "        0.55393939, 0.53454545, 0.66178795, 0.63442452, 0.59454545,\n",
       "        0.57939394, 0.55393939, 0.53454545, 0.66178795, 0.63442452,\n",
       "        0.59454545, 0.57939394, 0.55393939, 0.53454545, 0.9448833 ,\n",
       "        0.94125808, 0.92466371, 0.91001961, 0.90772605, 0.89135306,\n",
       "        0.95568219, 0.94258138, 0.93511373, 0.92504183, 0.91042517,\n",
       "        0.89750437, 0.96080278, 0.94633868, 0.93302206, 0.92419112,\n",
       "        0.91042517, 0.89750437, 0.96080278, 0.94633868, 0.93302206,\n",
       "        0.92419112, 0.91042517, 0.89750437, 0.95168622, 0.94376507,\n",
       "        0.92942608, 0.91363867, 0.9115279 , 0.89552886, 0.96405168,\n",
       "        0.94656424, 0.93622999, 0.93101353, 0.91845553, 0.9029993 ,\n",
       "        0.96817038, 0.95062248, 0.93622999, 0.9328894 , 0.91885135,\n",
       "        0.90321233, 0.96817038, 0.95062248, 0.93622999, 0.9328894 ,\n",
       "        0.91885135, 0.90321233, 0.94129846, 0.94090682, 0.92938929,\n",
       "        0.91868248, 0.90812963, 0.89961356, 0.96612008, 0.95345229,\n",
       "        0.94296687, 0.9279353 , 0.91771641, 0.90084617, 0.96992949,\n",
       "        0.95866079, 0.94239104, 0.92657201, 0.91904946, 0.90084617,\n",
       "        0.9734133 , 0.95853985, 0.94239104, 0.92657201, 0.91904946,\n",
       "        0.90084617, 0.93852534, 0.93439033, 0.92117555, 0.91013479,\n",
       "        0.89680603, 0.88039069, 0.96000597, 0.9523434 , 0.93879884,\n",
       "        0.92946229, 0.91216599, 0.89602414, 0.96417679, 0.95560721,\n",
       "        0.93907094, 0.92628221, 0.91386185, 0.89638834, 0.9710537 ,\n",
       "        0.95833379, 0.93907094, 0.92628221, 0.91386185, 0.89638834]),\n",
       " 'std_fit_time': array([0.14214162, 0.13555861, 0.26621194, 0.03346556, 0.17698937,\n",
       "        0.03516348, 0.128758  , 0.18202888, 0.03784688, 0.17405277,\n",
       "        0.20663688, 0.02634215, 0.09157015, 0.07835436, 0.04696485,\n",
       "        0.04139687, 0.02198006, 0.13573916, 0.06153181, 0.03892208,\n",
       "        0.0381076 , 0.02614077, 0.03786039, 0.09332638, 0.01627223,\n",
       "        0.0356556 , 0.04960785, 0.02546271, 0.12393822, 0.05005083,\n",
       "        0.05917357, 0.01543896, 0.01400317, 0.03913567, 0.13360127,\n",
       "        0.05275788, 0.08953445, 0.06873249, 0.06298932, 0.03467775,\n",
       "        0.11416746, 0.30758082, 0.11324906, 0.12774882, 0.02648177,\n",
       "        0.05531993, 0.0786106 , 0.25646358, 0.25417775, 0.09310318,\n",
       "        0.01818219, 0.0495018 , 0.12097765, 0.44463756, 0.07367709,\n",
       "        0.00956836, 0.05235523, 0.04232039, 0.13399849, 0.20659907,\n",
       "        0.31893693, 0.06625498, 0.05723945, 0.13113108, 0.03063562,\n",
       "        0.05863765, 0.21115584, 0.09532462, 0.04618461, 0.07185322,\n",
       "        0.06752113, 0.03508282, 0.22665706, 0.24493211, 0.38113234,\n",
       "        0.22372288, 0.1374757 , 0.03024421, 0.11008609, 0.43169895,\n",
       "        0.05519817, 0.06695868, 0.02939319, 0.05899699, 0.07661734,\n",
       "        0.03798264, 0.33673163, 0.07663362, 0.00584449, 0.01027608,\n",
       "        0.18184969, 0.0438993 , 0.10896864, 0.0617415 , 0.04432241,\n",
       "        0.14157164, 0.07811475, 0.0093239 , 0.11795027, 0.03331168,\n",
       "        0.01102729, 0.05091018, 0.17936895, 0.0397917 , 0.03738243,\n",
       "        0.11797035, 0.14738628, 0.14298627, 0.10612696, 0.3293606 ,\n",
       "        0.19438976, 0.19128023, 0.1415392 , 0.0083432 , 0.01599591,\n",
       "        0.05346282, 0.04716813, 0.13105042, 0.07607363, 0.01209749]),\n",
       " 'std_score_time': array([2.94680088e-03, 4.61545377e-04, 3.29691184e-03, 3.08477030e-03,\n",
       "        1.90051981e-03, 1.23284334e-03, 1.16899094e-02, 3.07874476e-03,\n",
       "        2.15372590e-03, 1.87730130e-03, 4.24446884e-03, 3.80333360e-04,\n",
       "        2.47440095e-03, 8.14481347e-04, 2.04782682e-03, 1.40762336e-03,\n",
       "        2.06006215e-03, 2.15543447e-03, 7.86741172e-07, 1.70723832e-03,\n",
       "        1.41034595e-03, 1.62795900e-03, 4.00872241e-03, 2.04872640e-03,\n",
       "        1.23929366e-03, 9.33425481e-04, 7.97224298e-04, 2.17665629e-03,\n",
       "        1.64077114e-03, 2.05967258e-03, 1.64311126e-03, 1.41354916e-03,\n",
       "        1.24180039e-03, 9.32528813e-04, 3.28587086e-03, 2.03870784e-03,\n",
       "        1.87549812e-03, 2.45147115e-03, 1.62898200e-03, 5.33864245e-03,\n",
       "        4.23790799e-03, 1.24387984e-03, 3.30672946e-03, 8.24679698e-04,\n",
       "        4.60504126e-04, 8.25617159e-04, 2.16187356e-03, 1.46210797e-02,\n",
       "        3.94355841e-02, 5.68504400e-03, 2.15083139e-03, 2.35022087e-03,\n",
       "        5.63616946e-03, 4.18014348e-03, 4.89741395e-03, 2.06423192e-03,\n",
       "        3.84027874e-04, 1.10559197e-02, 9.26626364e-03, 1.12320976e-02,\n",
       "        5.71921504e-03, 1.70476794e-03, 1.03504870e-02, 8.24217603e-03,\n",
       "        1.24343328e-03, 6.77960599e-03, 7.72962351e-03, 2.06064412e-03,\n",
       "        1.62032259e-03, 8.10306392e-04, 1.39763472e-03, 9.61201309e-04,\n",
       "        4.64051443e-03, 6.46309211e-03, 4.76425141e-03, 5.34604657e-03,\n",
       "        4.38708899e-03, 9.21507485e-03, 5.72304844e-03, 1.25643838e-03,\n",
       "        3.29583574e-03, 2.05213399e-03, 9.42321645e-04, 3.85698682e-03,\n",
       "        8.18646923e-04, 3.84728366e-03, 1.36399989e-02, 1.27885594e-03,\n",
       "        3.10043504e-03, 2.61342886e-03, 2.13729173e-03, 1.69181015e-03,\n",
       "        4.82835278e-04, 5.07146860e-03, 1.98715980e-02, 3.53748796e-03,\n",
       "        1.69912548e-03, 2.38015334e-03, 2.83077708e-03, 1.70428049e-03,\n",
       "        9.37632122e-04, 5.16743343e-03, 1.24383705e-03, 2.47760718e-03,\n",
       "        4.69347951e-04, 4.71876929e-04, 3.51738576e-03, 7.33578713e-03,\n",
       "        3.55261227e-03, 1.69728830e-03, 9.32034388e-04, 2.50419807e-03,\n",
       "        9.34042407e-04, 2.92526920e-03, 1.24328445e-03, 1.70889365e-03,\n",
       "        1.68965922e-03, 3.08415760e-03, 4.82415524e-04, 3.54058239e-03]),\n",
       " 'std_test_score': array([0.01677081, 0.01476067, 0.00642277, 0.0203752 , 0.01194351,\n",
       "        0.01511086, 0.00837339, 0.00762357, 0.00539819, 0.01899575,\n",
       "        0.00494756, 0.01661848, 0.00991856, 0.00572629, 0.00594134,\n",
       "        0.01956488, 0.00494756, 0.01826846, 0.01000619, 0.00572629,\n",
       "        0.00594134, 0.01956488, 0.00494756, 0.01826846, 0.00990242,\n",
       "        0.00736004, 0.00158551, 0.00754924, 0.00334814, 0.01045923,\n",
       "        0.01165739, 0.00233524, 0.01105193, 0.0082025 , 0.00200973,\n",
       "        0.01569256, 0.00881008, 0.00567505, 0.01042913, 0.00798002,\n",
       "        0.00200973, 0.01569256, 0.00415973, 0.00567505, 0.01042913,\n",
       "        0.00798002, 0.00200973, 0.01569256, 0.01034351, 0.0089745 ,\n",
       "        0.01832057, 0.01232381, 0.0050658 , 0.01275182, 0.01295634,\n",
       "        0.00528245, 0.01235662, 0.01159283, 0.00704593, 0.01277686,\n",
       "        0.01242582, 0.00694037, 0.01165443, 0.01439788, 0.00565048,\n",
       "        0.00941135, 0.01292413, 0.00694037, 0.01165443, 0.01439788,\n",
       "        0.00565048, 0.00941135, 0.01075163, 0.00796796, 0.01151738,\n",
       "        0.00233055, 0.00580473, 0.01571603, 0.01178724, 0.01359506,\n",
       "        0.01417296, 0.00787724, 0.00898556, 0.02048921, 0.00720644,\n",
       "        0.01694195, 0.01467919, 0.01059053, 0.00949003, 0.02048921,\n",
       "        0.00276581, 0.01692219, 0.01467919, 0.01059053, 0.00949003,\n",
       "        0.02048921, 0.01360023, 0.0092998 , 0.01392482, 0.01212905,\n",
       "        0.00584798, 0.01762567, 0.01133556, 0.00545991, 0.01396733,\n",
       "        0.00552691, 0.01008673, 0.00838109, 0.0053155 , 0.00567514,\n",
       "        0.01241098, 0.00633869, 0.00908846, 0.00914796, 0.01341618,\n",
       "        0.00560637, 0.01241098, 0.00633869, 0.00908846, 0.00914796]),\n",
       " 'std_train_score': array([0.01145495, 0.01404832, 0.00447626, 0.02038037, 0.01790436,\n",
       "        0.01293985, 0.00227009, 0.01022806, 0.01070229, 0.0193815 ,\n",
       "        0.00560034, 0.01416628, 0.00556043, 0.0080927 , 0.01232848,\n",
       "        0.01964456, 0.00560034, 0.01678619, 0.00529589, 0.0080927 ,\n",
       "        0.01232848, 0.01964456, 0.00560034, 0.01678619, 0.0009035 ,\n",
       "        0.0040659 , 0.0030162 , 0.00441203, 0.00156144, 0.00147858,\n",
       "        0.00126276, 0.00170838, 0.00326597, 0.00201323, 0.00282659,\n",
       "        0.00265312, 0.00153762, 0.00327304, 0.00172747, 0.00168564,\n",
       "        0.00282659, 0.00265312, 0.00153672, 0.00327304, 0.00172747,\n",
       "        0.00168564, 0.00282659, 0.00265312, 0.0019928 , 0.00363997,\n",
       "        0.00164442, 0.00125579, 0.00149968, 0.00219264, 0.00126317,\n",
       "        0.00168786, 0.00220306, 0.0065662 , 0.00556603, 0.00066482,\n",
       "        0.00223942, 0.00090849, 0.00235799, 0.00746066, 0.00574439,\n",
       "        0.00073579, 0.0026657 , 0.00090849, 0.00235799, 0.00746066,\n",
       "        0.00574439, 0.00073579, 0.00312364, 0.00440426, 0.00192204,\n",
       "        0.00272705, 0.00174432, 0.00333462, 0.00188048, 0.00097057,\n",
       "        0.00476912, 0.00272731, 0.00508819, 0.003644  , 0.00337637,\n",
       "        0.00361228, 0.00370103, 0.00119502, 0.00567064, 0.003644  ,\n",
       "        0.00434677, 0.00355643, 0.00370103, 0.00119502, 0.00567064,\n",
       "        0.003644  , 0.00162788, 0.0038352 , 0.00304614, 0.0033618 ,\n",
       "        0.00331276, 0.00433951, 0.00070602, 0.00116481, 0.00387554,\n",
       "        0.00265637, 0.00445293, 0.00215911, 0.00153454, 0.00160749,\n",
       "        0.00500432, 0.00153804, 0.00502069, 0.00232058, 0.00406352,\n",
       "        0.00287621, 0.00500432, 0.00153804, 0.00502069, 0.00232058])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display verbose output\n",
    "grid_obj.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for measuring performance\n",
    "from sklearn import metrics\n",
    "\n",
    "#y_train, y_val are target train/val (or test) vectors\n",
    "#y_train_pred, y_val_pred are predicted train/val (or test) vectors\n",
    "#vtype = 1 if prediction vector is more than one column of probabilities\n",
    "#vtype = 2 if prediction vector is one column of probabilities\n",
    "\n",
    "# Recall and precision stats, along with confusion mat are based on\n",
    "# 50th percentile acceptance threshold\n",
    "\n",
    "def perf_metrics(y_train, y_val, y_train_pred, y_val_pred, vtype): \n",
    "# Making predictions\n",
    "    if(vtype == 1):\n",
    "        y_train_pred = y_train_pred[:,1]\n",
    "        y_val_pred = y_val_pred[:,1]\n",
    "    if(vtype == 2):\n",
    "        y_train_pred = y_train_pred[:,] \n",
    "        y_val_pred = y_val_pred[:,] \n",
    "        \n",
    "        \n",
    "    train_accuracy = roc_auc_score(y_train, y_train_pred)\n",
    "    val_accuracy = roc_auc_score(y_val, y_val_pred)\n",
    "    print('The training accuracy is', train_accuracy)\n",
    "    print('The validation accuracy is', val_accuracy)\n",
    "\n",
    "    \n",
    "    train_mat = metrics.confusion_matrix(y_train,y_train_pred > 0.5)\n",
    "    val_mat = metrics.confusion_matrix(y_val,y_val_pred > 0.5)\n",
    "    print(\"Training Set Confusion Mat.:\")\n",
    "    print(train_mat)\n",
    "    print(\"Precision: \", train_mat[1][1]/(train_mat[0][1]+train_mat[1][1]))\n",
    "    print(\"Recall: \", train_mat[1][1]/(train_mat[1][0]+train_mat[1][1]))\n",
    "\n",
    "    print(\"Valid Set Confusion Mat.:\")\n",
    "    print(val_mat)\n",
    "    print(\"Precision: \", val_mat[1][1]/(val_mat[0][1]+val_mat[1][1]))\n",
    "    print(\"Recall: \", val_mat[1][1]/(val_mat[1][0]+val_mat[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9151621758654759\n",
      "The validation accuracy is 0.7981988380634697\n",
      "Training Set Confusion Mat.:\n",
      "[[21339  3467]\n",
      " [   37  1200]]\n",
      "Precision:  0.257124491107778\n",
      "Recall:  0.9700889248181084\n",
      "Valid Set Confusion Mat.:\n",
      "[[7053 1205]\n",
      " [ 109  314]]\n",
      "Precision:  0.20671494404213298\n",
      "Recall:  0.7423167848699763\n"
     ]
    }
   ],
   "source": [
    "#Measure Performance of Base Model\n",
    "\n",
    "class_weight = {0: 1.0, 1: 50.0}\n",
    "model = RandomForestClassifier(max_depth=24, min_samples_leaf=42, min_samples_split=2, class_weight = class_weight)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49, 29, 14, 39, 32, 58, 27, 17, 7, 43, 6, 16, 52, 2, 42, 61, 62, 10, 13, 56, 36, 30, 59, 22, 60, 3, 31, 55, 20, 53, 0, 1, 54, 47, 21, 46, 50, 51, 44, 19, 35, 9, 63, 57, 28, 8, 40, 34, 48, 4, 24, 38, 41, 11, 18, 5, 15, 37, 45, 33, 23, 12, 25, 26]\n",
      "[0.003 0.004 0.004 0.004 0.004 0.005 0.005 0.005 0.006 0.006 0.006 0.006\n",
      " 0.006 0.006 0.006 0.006 0.006 0.006 0.007 0.007 0.007 0.008 0.008 0.008\n",
      " 0.008 0.008 0.008 0.008 0.008 0.009 0.009 0.009 0.009 0.01  0.01  0.01\n",
      " 0.01  0.01  0.011 0.011 0.011 0.012 0.012 0.013 0.013 0.013 0.014 0.015\n",
      " 0.016 0.018 0.019 0.02  0.022 0.026 0.026 0.028 0.03  0.031 0.033 0.043\n",
      " 0.048 0.049 0.066 0.136]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c693c60f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize Feature Importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "objects = ('X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16',\n",
    "          'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32',\n",
    "          'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48',\n",
    "          'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64')\n",
    "y_pos = np.arange(len(objects))\n",
    "plt.bar(y_pos, model.feature_importances_, align='center', alpha=0.5)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance of Tuned Random Forest')\n",
    "\n",
    "#important features: \n",
    "print(sorted(range(len(model.feature_importances_)), key=lambda k: model.feature_importances_[k]))\n",
    "print(np.round(np.sort(model.feature_importances_),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Explained Variance is quite low, so no PCA will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.79414746 8.81241902 4.83342383 4.67726878 3.27716161 3.08908343\n",
      " 2.77527864 2.492677   2.23993046 1.77848461 1.51127596 1.22313769\n",
      " 1.19200325 1.13004969 1.03810709 1.00314963 1.0004075  1.00001409\n",
      " 0.99800436 0.99094144 0.96027382 0.95205015 0.92003566 0.80111412\n",
      " 0.77638035 0.66694294 0.66251014 0.6466528  0.62058589 0.47114766\n",
      " 0.38738519 0.25774557 0.17024031 0.14574545 0.12730505 0.09402322\n",
      " 0.0920836  0.075643   0.06087903 0.05843403]\n",
      "63.80414354380271\n"
     ]
    }
   ],
   "source": [
    "import visuals as vs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Apply PCA by fitting the good data with the same number of dimensions as features\n",
    "pca = PCA(n_components=40)\n",
    "pca.fit(X0)\n",
    "print(pca.explained_variance_)\n",
    "print(np.sum(pca.explained_variance_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Outlier Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for removal of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Removal Function\n",
    "# Input: X and y vectors, p is the probability cutoff (e.g. 5 for 5/95th percentile)\n",
    "# Returns outlier removed X and y, as well as outlier count.\n",
    "def outlier_remove(X, y, p):\n",
    "    \n",
    "    outlier_count = []\n",
    "    outliers = [] #For troubleshooting\n",
    "    for feature in X.keys():\n",
    "    \n",
    "        # Calculate pth percentile of the data for the given feature\n",
    "        Q1 = np.percentile(X[feature],p)\n",
    "    \n",
    "        # Calculate 1-pth percentile of the data for the given feature\n",
    "        Q2 = np.percentile(X[feature],100-p)\n",
    "    \n",
    "    \n",
    "        # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n",
    "        step = (Q2-Q1)*1.5\n",
    "    \n",
    "        # Display the outliers\n",
    "        #print(\"Data points considered outliers for the feature '{}':\".format(feature)) #For troubleshooting\n",
    "        #display(X[~((data[feature] >= Q1 - step) & (X[feature] <= Q2 + step))]) #For troubleshooting\n",
    "        outliers = np.append(outliers, np.where(~((X[feature] >= Q1 - step) & (X[feature] <= Q2 + step))))\n",
    "        outlier_count = np.append(outlier_count, len(np.where(~((X[feature] >= Q1 - step) & (X[feature] <= Q2 + step)))[0]))\n",
    "    \n",
    "    outliers = outliers.astype(int)\n",
    "\n",
    "    #Remove outliers\n",
    "    good_X = X.drop(X.index[outliers]).reset_index(drop = True)\n",
    "    y = y.drop(X.index[outliers]).reset_index(drop = True)\n",
    "\n",
    "    #Scale variables\n",
    "    good_X = preprocessing.scale(good_X)\n",
    "    \n",
    "    return good_X, y, outlier_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assess the impact of outlier removal by re-fitting the base model on each of the outlier-removed datasets. Note that all datasets perform similarly but better than the full dataset base model, therefore we can exclude the minimum amount of data possible to boost performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New size: 31242\n",
      "Bankrupt ratio: 0.04362716855515012\n",
      "Metrics: 5% Outlier Criteria\n",
      "The training accuracy is 0.9251843331269195\n",
      "The validation accuracy is 0.8119394779273564\n",
      "Training Set Confusion Mat.:\n",
      "[[15790  2121]\n",
      " [   26   807]]\n",
      "Precision:  0.27561475409836067\n",
      "Recall:  0.9687875150060024\n",
      "Valid Set Confusion Mat.:\n",
      "[[5210  758]\n",
      " [  70  211]]\n",
      "Precision:  0.21775025799793601\n",
      "Recall:  0.7508896797153025\n",
      "\n",
      "New size: 36120\n",
      "Bankrupt ratio: 0.04496124031007752\n",
      "Metrics: 2.5% Outlier Criteria\n",
      "The training accuracy is 0.9284986058425613\n",
      "The validation accuracy is 0.8231927631405438\n",
      "Training Set Confusion Mat.:\n",
      "[[18295  2406]\n",
      " [   26   945]]\n",
      "Precision:  0.2820053715308863\n",
      "Recall:  0.9732234809474768\n",
      "Valid Set Confusion Mat.:\n",
      "[[6023  871]\n",
      " [  75  255]]\n",
      "Precision:  0.22646536412078153\n",
      "Recall:  0.7727272727272727\n",
      "\n",
      "New size: 39895\n",
      "Bankrupt ratio: 0.04654718636420604\n",
      "Metrics: 1% Outlier Criteria\n",
      "The training accuracy is 0.9176490142101494\n",
      "The validation accuracy is 0.8429682992994395\n",
      "Training Set Confusion Mat.:\n",
      "[[19695  3097]\n",
      " [   33  1112]]\n",
      "Precision:  0.26419577096697555\n",
      "Recall:  0.97117903930131\n",
      "Valid Set Confusion Mat.:\n",
      "[[6572 1059]\n",
      " [  61  287]]\n",
      "Precision:  0.21322436849925705\n",
      "Recall:  0.8247126436781609\n",
      "\n",
      "New size: 41398\n",
      "Bankrupt ratio: 0.04674138847287308\n",
      "Metrics: 0.5% Outlier Criteria\n",
      "The training accuracy is 0.9240044597383174\n",
      "The validation accuracy is 0.8189403553299494\n",
      "Training Set Confusion Mat.:\n",
      "[[20839  2842]\n",
      " [   37  1120]]\n",
      "Precision:  0.2826855123674912\n",
      "Recall:  0.9680207433016422\n",
      "Valid Set Confusion Mat.:\n",
      "[[6898  982]\n",
      " [  95  305]]\n",
      "Precision:  0.236985236985237\n",
      "Recall:  0.7625\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers\n",
    "X1, y1, _ = outlier_remove(X, y, 5)\n",
    "X2, y2, _ = outlier_remove(X, y, 2.5)\n",
    "X3, y3, _ = outlier_remove(X, y, 1)\n",
    "X4, y4, _ = outlier_remove(X, y, 0.5)\n",
    "\n",
    "#5% outliers\n",
    "print(\"\")\n",
    "print(\"New size:\", len(y1))\n",
    "print(\"Bankrupt ratio:\", np.sum(y1)/len(y1))\n",
    "## Sample Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.2, random_state = 23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 23) \n",
    "## Fit Model\n",
    "class_weight = {0: 1.0, 1: 50.0}\n",
    "model = RandomForestClassifier(max_depth=24, min_samples_leaf=42, min_samples_split=2, class_weight = class_weight, random_state = 23)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "## Examine Results\n",
    "print(\"Metrics: 5% Outlier Criteria\")\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)\n",
    "\n",
    "#2.5% outliers\n",
    "print(\"\")\n",
    "print(\"New size:\", len(y2))\n",
    "print(\"Bankrupt ratio:\", np.sum(y2)/len(y2))\n",
    "## Sample Data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size = 0.2, random_state = 23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 23) \n",
    "## Fit Model\n",
    "class_weight = {0: 1.0, 1: 50.0}\n",
    "model = RandomForestClassifier(max_depth=24, min_samples_leaf=42, min_samples_split=2, class_weight = class_weight, random_state = 23)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "## Examine Results\n",
    "print(\"Metrics: 2.5% Outlier Criteria\")\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)\n",
    "\n",
    "#1% outliers\n",
    "print(\"\")\n",
    "print(\"New size:\", len(y3))\n",
    "print(\"Bankrupt ratio:\", np.sum(y3)/len(y3))\n",
    "## Sample Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, y3, test_size = 0.2, random_state = 23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 23) \n",
    "## Fit Model\n",
    "class_weight = {0: 1.0, 1: 50.0}\n",
    "model = RandomForestClassifier(max_depth=24, min_samples_leaf=42, min_samples_split=2, class_weight = class_weight, random_state = 23)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "## Examine Results\n",
    "print(\"Metrics: 1% Outlier Criteria\")\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)\n",
    "\n",
    "#0.5% outliers\n",
    "print(\"\")\n",
    "print(\"New size:\", len(y4))\n",
    "print(\"Bankrupt ratio:\", np.sum(y4)/len(y4))\n",
    "## Sample Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X4, y4, test_size = 0.2, random_state = 23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 23) \n",
    "## Fit Model\n",
    "class_weight = {0: 1.0, 1: 50.0}\n",
    "model = RandomForestClassifier(max_depth=24, min_samples_leaf=42, min_samples_split=2, class_weight = class_weight, random_state = 23)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "## Examine Results\n",
    "print(\"Metrics: 0.5% Outlier Criteria\")\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, all models will be fit with the same subset of test data excluded, but occasionally with different train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_processed, y_processed, _ = outlier_remove(X, y, 0.5) #Remove outliers and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size = 0.2, random_state = 23)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 23) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib #joblib allows easy save/reload of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVM_models/linear_model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 1.0, 1: 25.0} #Balanced class weights\n",
    "clf = svm.LinearSVC(C=1, class_weight = class_weight, random_state = 23)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Store model\n",
    "joblib.dump(clf, 'SVM_models/linear_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.7318863004694676\n",
      "The validation accuracy is 0.7496065989847717\n",
      "Training Set Confusion Mat.:\n",
      "[[19886  3795]\n",
      " [  435   722]]\n",
      "Precision:  0.15984060216958157\n",
      "Recall:  0.6240276577355229\n",
      "Valid Set Confusion Mat.:\n",
      "[[6613 1267]\n",
      " [ 136  264]]\n",
      "Precision:  0.17243631613324625\n",
      "Recall:  0.66\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "clf = joblib.load('SVM_models/linear_model.pkl')\n",
    "#Predictions\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_val_pred = clf.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Kernel parameter tuning. Note that max iterations are lowered since an approximation should be sufficient to determine best parameter set. This may take more than an hour to run, so output is included in the paper appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "[CV] C=1, degree=1, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, degree=1, gamma=0.5, total=  19.8s\n",
      "[CV] C=1, degree=1, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.2s remaining:    0.0s\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, degree=1, gamma=0.5, total=  14.2s\n",
      "[CV] C=1, degree=1, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, degree=1, gamma=0.5, total=  14.2s\n",
      "[CV] C=1, degree=1, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=1, degree=1, gamma=0.125, total=  41.9s\n",
      "[CV] C=1, degree=1, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=1, degree=1, gamma=0.125, total=  41.7s\n",
      "[CV] C=1, degree=1, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=1, degree=1, gamma=0.125, total=  42.5s\n",
      "[CV] C=1, degree=1, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, degree=1, gamma=0.03125, total=  43.4s\n",
      "[CV] C=1, degree=1, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, degree=1, gamma=0.03125, total=  43.4s\n",
      "[CV] C=1, degree=1, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, degree=1, gamma=0.03125, total=  43.4s\n",
      "[CV] C=1, degree=1, gamma=0.0078125 ..................................\n",
      "[CV] ................... C=1, degree=1, gamma=0.0078125, total=  42.3s\n",
      "[CV] C=1, degree=1, gamma=0.0078125 ..................................\n",
      "[CV] ................... C=1, degree=1, gamma=0.0078125, total=  44.1s\n",
      "[CV] C=1, degree=1, gamma=0.0078125 ..................................\n",
      "[CV] ................... C=1, degree=1, gamma=0.0078125, total=  44.5s\n",
      "[CV] C=1, degree=5, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, degree=5, gamma=0.5, total=  10.7s\n",
      "[CV] C=1, degree=5, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, degree=5, gamma=0.5, total=  10.9s\n",
      "[CV] C=1, degree=5, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=1, degree=5, gamma=0.5, total=  10.1s\n",
      "[CV] C=1, degree=5, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=1, degree=5, gamma=0.125, total=  11.8s\n",
      "[CV] C=1, degree=5, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=1, degree=5, gamma=0.125, total=  13.9s\n",
      "[CV] C=1, degree=5, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=1, degree=5, gamma=0.125, total=  11.6s\n",
      "[CV] C=1, degree=5, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, degree=5, gamma=0.03125, total=  39.7s\n",
      "[CV] C=1, degree=5, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, degree=5, gamma=0.03125, total=  38.4s\n",
      "[CV] C=1, degree=5, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, degree=5, gamma=0.03125, total=  38.4s\n",
      "[CV] C=1, degree=5, gamma=0.0078125 ..................................\n",
      "[CV] ................... C=1, degree=5, gamma=0.0078125, total=  52.7s\n",
      "[CV] C=1, degree=5, gamma=0.0078125 ..................................\n",
      "[CV] ................... C=1, degree=5, gamma=0.0078125, total=  49.5s\n",
      "[CV] C=1, degree=5, gamma=0.0078125 ..................................\n",
      "[CV] ................... C=1, degree=5, gamma=0.0078125, total=  49.0s\n",
      "[CV] C=1, degree=10, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=1, degree=10, gamma=0.5, total=   7.2s\n",
      "[CV] C=1, degree=10, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=1, degree=10, gamma=0.5, total=   7.8s\n",
      "[CV] C=1, degree=10, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=1, degree=10, gamma=0.5, total=   7.1s\n",
      "[CV] C=1, degree=10, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=1, degree=10, gamma=0.125, total=   8.5s\n",
      "[CV] C=1, degree=10, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=1, degree=10, gamma=0.125, total=   9.6s\n",
      "[CV] C=1, degree=10, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=1, degree=10, gamma=0.125, total=  10.3s\n",
      "[CV] C=1, degree=10, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, degree=10, gamma=0.03125, total=  34.0s\n",
      "[CV] C=1, degree=10, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, degree=10, gamma=0.03125, total=  33.8s\n",
      "[CV] C=1, degree=10, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, degree=10, gamma=0.03125, total=  33.8s\n",
      "[CV] C=1, degree=10, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1, degree=10, gamma=0.0078125, total=  48.2s\n",
      "[CV] C=1, degree=10, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1, degree=10, gamma=0.0078125, total=  49.6s\n",
      "[CV] C=1, degree=10, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1, degree=10, gamma=0.0078125, total=  49.0s\n",
      "[CV] C=5, degree=1, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=5, degree=1, gamma=0.5, total=   5.9s\n",
      "[CV] C=5, degree=1, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=5, degree=1, gamma=0.5, total=   5.7s\n",
      "[CV] C=5, degree=1, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=5, degree=1, gamma=0.5, total=   5.6s\n",
      "[CV] C=5, degree=1, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=5, degree=1, gamma=0.125, total=  10.5s\n",
      "[CV] C=5, degree=1, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=5, degree=1, gamma=0.125, total=  10.4s\n",
      "[CV] C=5, degree=1, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=5, degree=1, gamma=0.125, total=  10.7s\n",
      "[CV] C=5, degree=1, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=5, degree=1, gamma=0.03125, total=  35.4s\n",
      "[CV] C=5, degree=1, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=5, degree=1, gamma=0.03125, total=  36.1s\n",
      "[CV] C=5, degree=1, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=5, degree=1, gamma=0.03125, total=  37.4s\n",
      "[CV] C=5, degree=1, gamma=0.0078125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, degree=1, gamma=0.0078125, total=  36.4s\n",
      "[CV] C=5, degree=1, gamma=0.0078125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, degree=1, gamma=0.0078125, total=  37.8s\n",
      "[CV] C=5, degree=1, gamma=0.0078125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, degree=1, gamma=0.0078125, total=  37.2s\n",
      "[CV] C=5, degree=5, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=5, degree=5, gamma=0.5, total=   9.3s\n",
      "[CV] C=5, degree=5, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=5, degree=5, gamma=0.5, total=  10.0s\n",
      "[CV] C=5, degree=5, gamma=0.5 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... C=5, degree=5, gamma=0.5, total=   8.9s\n",
      "[CV] C=5, degree=5, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=5, degree=5, gamma=0.125, total=   9.4s\n",
      "[CV] C=5, degree=5, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=5, degree=5, gamma=0.125, total=  10.4s\n",
      "[CV] C=5, degree=5, gamma=0.125 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=5, degree=5, gamma=0.125, total=   9.4s\n",
      "[CV] C=5, degree=5, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=5, degree=5, gamma=0.03125, total=  25.7s\n",
      "[CV] C=5, degree=5, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=5, degree=5, gamma=0.03125, total=  27.1s\n",
      "[CV] C=5, degree=5, gamma=0.03125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=5, degree=5, gamma=0.03125, total=  26.5s\n",
      "[CV] C=5, degree=5, gamma=0.0078125 ..................................\n",
      "[CV] ................... C=5, degree=5, gamma=0.0078125, total=  47.1s\n",
      "[CV] C=5, degree=5, gamma=0.0078125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, degree=5, gamma=0.0078125, total=  47.8s\n",
      "[CV] C=5, degree=5, gamma=0.0078125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, degree=5, gamma=0.0078125, total=  47.3s\n",
      "[CV] C=5, degree=10, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=5, degree=10, gamma=0.5, total=   7.2s\n",
      "[CV] C=5, degree=10, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=5, degree=10, gamma=0.5, total=   7.9s\n",
      "[CV] C=5, degree=10, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=5, degree=10, gamma=0.5, total=   6.8s\n",
      "[CV] C=5, degree=10, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=5, degree=10, gamma=0.125, total=   7.4s\n",
      "[CV] C=5, degree=10, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=5, degree=10, gamma=0.125, total=   7.9s\n",
      "[CV] C=5, degree=10, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=5, degree=10, gamma=0.125, total=   7.0s\n",
      "[CV] C=5, degree=10, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=5, degree=10, gamma=0.03125, total=  29.8s\n",
      "[CV] C=5, degree=10, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=5, degree=10, gamma=0.03125, total=  29.8s\n",
      "[CV] C=5, degree=10, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=5, degree=10, gamma=0.03125, total=  29.2s\n",
      "[CV] C=5, degree=10, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=5, degree=10, gamma=0.0078125, total=  47.4s\n",
      "[CV] C=5, degree=10, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=5, degree=10, gamma=0.0078125, total=  48.3s\n",
      "[CV] C=5, degree=10, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=5, degree=10, gamma=0.0078125, total=  48.4s\n",
      "[CV] C=10, degree=1, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=10, degree=1, gamma=0.5, total=   5.7s\n",
      "[CV] C=10, degree=1, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=10, degree=1, gamma=0.5, total=   5.3s\n",
      "[CV] C=10, degree=1, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=10, degree=1, gamma=0.5, total=   5.2s\n",
      "[CV] C=10, degree=1, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, degree=1, gamma=0.125, total=   6.9s\n",
      "[CV] C=10, degree=1, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, degree=1, gamma=0.125, total=   6.9s\n",
      "[CV] C=10, degree=1, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, degree=1, gamma=0.125, total=   6.8s\n",
      "[CV] C=10, degree=1, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, degree=1, gamma=0.03125, total=  25.2s\n",
      "[CV] C=10, degree=1, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, degree=1, gamma=0.03125, total=  26.4s\n",
      "[CV] C=10, degree=1, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, degree=1, gamma=0.03125, total=  26.1s\n",
      "[CV] C=10, degree=1, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, degree=1, gamma=0.0078125, total=  37.6s\n",
      "[CV] C=10, degree=1, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, degree=1, gamma=0.0078125, total=  38.2s\n",
      "[CV] C=10, degree=1, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, degree=1, gamma=0.0078125, total=  37.2s\n",
      "[CV] C=10, degree=5, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=10, degree=5, gamma=0.5, total=   9.3s\n",
      "[CV] C=10, degree=5, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=10, degree=5, gamma=0.5, total=   9.8s\n",
      "[CV] C=10, degree=5, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=10, degree=5, gamma=0.5, total=   8.9s\n",
      "[CV] C=10, degree=5, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, degree=5, gamma=0.125, total=   9.2s\n",
      "[CV] C=10, degree=5, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, degree=5, gamma=0.125, total=  10.5s\n",
      "[CV] C=10, degree=5, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, degree=5, gamma=0.125, total=   9.0s\n",
      "[CV] C=10, degree=5, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, degree=5, gamma=0.03125, total=  25.5s\n",
      "[CV] C=10, degree=5, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, degree=5, gamma=0.03125, total=  24.4s\n",
      "[CV] C=10, degree=5, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, degree=5, gamma=0.03125, total=  24.1s\n",
      "[CV] C=10, degree=5, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, degree=5, gamma=0.0078125, total=  45.8s\n",
      "[CV] C=10, degree=5, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, degree=5, gamma=0.0078125, total=  46.4s\n",
      "[CV] C=10, degree=5, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, degree=5, gamma=0.0078125, total=  47.1s\n",
      "[CV] C=10, degree=10, gamma=0.5 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=10, degree=10, gamma=0.5, total=   7.6s\n",
      "[CV] C=10, degree=10, gamma=0.5 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=10, degree=10, gamma=0.5, total=   8.4s\n",
      "[CV] C=10, degree=10, gamma=0.5 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=10, degree=10, gamma=0.5, total=   6.7s\n",
      "[CV] C=10, degree=10, gamma=0.125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, degree=10, gamma=0.125, total=   8.1s\n",
      "[CV] C=10, degree=10, gamma=0.125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, degree=10, gamma=0.125, total=   7.9s\n",
      "[CV] C=10, degree=10, gamma=0.125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, degree=10, gamma=0.125, total=   7.8s\n",
      "[CV] C=10, degree=10, gamma=0.03125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, degree=10, gamma=0.03125, total=  28.7s\n",
      "[CV] C=10, degree=10, gamma=0.03125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, degree=10, gamma=0.03125, total=  29.0s\n",
      "[CV] C=10, degree=10, gamma=0.03125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, degree=10, gamma=0.03125, total=  28.6s\n",
      "[CV] C=10, degree=10, gamma=0.0078125 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=10, degree=10, gamma=0.0078125, total=  46.8s\n",
      "[CV] C=10, degree=10, gamma=0.0078125 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=10, degree=10, gamma=0.0078125, total=  48.4s\n",
      "[CV] C=10, degree=10, gamma=0.0078125 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=10, degree=10, gamma=0.0078125, total=  47.6s\n",
      "[CV] C=15, degree=1, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=15, degree=1, gamma=0.5, total=   5.7s\n",
      "[CV] C=15, degree=1, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=15, degree=1, gamma=0.5, total=   5.2s\n",
      "[CV] C=15, degree=1, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=15, degree=1, gamma=0.5, total=   5.6s\n",
      "[CV] C=15, degree=1, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=15, degree=1, gamma=0.125, total=   6.1s\n",
      "[CV] C=15, degree=1, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=15, degree=1, gamma=0.125, total=   6.0s\n",
      "[CV] C=15, degree=1, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=15, degree=1, gamma=0.125, total=   6.0s\n",
      "[CV] C=15, degree=1, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=15, degree=1, gamma=0.03125, total=  14.5s\n",
      "[CV] C=15, degree=1, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=15, degree=1, gamma=0.03125, total=  14.8s\n",
      "[CV] C=15, degree=1, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=15, degree=1, gamma=0.03125, total=  15.2s\n",
      "[CV] C=15, degree=1, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=15, degree=1, gamma=0.0078125, total=  36.7s\n",
      "[CV] C=15, degree=1, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=15, degree=1, gamma=0.0078125, total=  37.2s\n",
      "[CV] C=15, degree=1, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=15, degree=1, gamma=0.0078125, total=  38.4s\n",
      "[CV] C=15, degree=5, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=15, degree=5, gamma=0.5, total=   9.5s\n",
      "[CV] C=15, degree=5, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=15, degree=5, gamma=0.5, total=   9.8s\n",
      "[CV] C=15, degree=5, gamma=0.5 .......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=15, degree=5, gamma=0.5, total=   9.2s\n",
      "[CV] C=15, degree=5, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=15, degree=5, gamma=0.125, total=   9.3s\n",
      "[CV] C=15, degree=5, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=15, degree=5, gamma=0.125, total=  10.0s\n",
      "[CV] C=15, degree=5, gamma=0.125 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=15, degree=5, gamma=0.125, total=   9.5s\n",
      "[CV] C=15, degree=5, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=15, degree=5, gamma=0.03125, total=  22.6s\n",
      "[CV] C=15, degree=5, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=15, degree=5, gamma=0.03125, total=  24.0s\n",
      "[CV] C=15, degree=5, gamma=0.03125 ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=15, degree=5, gamma=0.03125, total=  23.3s\n",
      "[CV] C=15, degree=5, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=15, degree=5, gamma=0.0078125, total=  45.1s\n",
      "[CV] C=15, degree=5, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=15, degree=5, gamma=0.0078125, total=  45.1s\n",
      "[CV] C=15, degree=5, gamma=0.0078125 .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=15, degree=5, gamma=0.0078125, total=  45.6s\n",
      "[CV] C=15, degree=10, gamma=0.5 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=15, degree=10, gamma=0.5, total=   7.4s\n",
      "[CV] C=15, degree=10, gamma=0.5 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=15, degree=10, gamma=0.5, total=   8.0s\n",
      "[CV] C=15, degree=10, gamma=0.5 ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=15, degree=10, gamma=0.5, total=   6.7s\n",
      "[CV] C=15, degree=10, gamma=0.125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=15, degree=10, gamma=0.125, total=   7.3s\n",
      "[CV] C=15, degree=10, gamma=0.125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=15, degree=10, gamma=0.125, total=   8.2s\n",
      "[CV] C=15, degree=10, gamma=0.125 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=15, degree=10, gamma=0.125, total=   8.7s\n",
      "[CV] C=15, degree=10, gamma=0.03125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=15, degree=10, gamma=0.03125, total=  29.8s\n",
      "[CV] C=15, degree=10, gamma=0.03125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=15, degree=10, gamma=0.03125, total=  29.2s\n",
      "[CV] C=15, degree=10, gamma=0.03125 ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=15, degree=10, gamma=0.03125, total=  29.4s\n",
      "[CV] C=15, degree=10, gamma=0.0078125 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=15, degree=10, gamma=0.0078125, total=  48.1s\n",
      "[CV] C=15, degree=10, gamma=0.0078125 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=15, degree=10, gamma=0.0078125, total=  49.1s\n",
      "[CV] C=15, degree=10, gamma=0.0078125 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=15, degree=10, gamma=0.0078125, total=  48.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 144 out of 144 | elapsed: 79.8min finished\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=40000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training ROC AUC Score is 0.5755363078755019\n",
      "The validation ROC AUC Score is 0.5810086837578496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight={0: 1.0, 1: 25.0}, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=1, gamma=0.0078125, kernel='poly',\n",
       "  max_iter=40000, probability=False, random_state=23, shrinking=True,\n",
       "  tol=0.01, verbose=False)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Warning:\n",
    "## This code block may take very long to run,\n",
    "## Results are included in report appendix to save time.\n",
    "\n",
    "parameters = {'C':[1,5,10,15], 'degree':[1,5,10], 'gamma':[0.5,2**-3,2**-5,2**-7]}\n",
    "\n",
    "class_weight = {0: 1.0, 1: 25.0}\n",
    "model =svm.SVC(random_state = 23, class_weight=class_weight, kernel='poly', max_iter = 40000, tol=0.01)\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "class_weight = {0: 1.0, 1: 25.0}\n",
    "grid_obj = GridSearchCV(model, parameters, scoring=scorer, verbose=2, cv=3)\n",
    "\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator.\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Fit the new model.\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the new model.\n",
    "best_train_predictions = best_clf.predict(X_train)\n",
    "best_val_predictions = best_clf.predict(X_val)\n",
    "\n",
    "print('The training ROC AUC Score is', roc_auc_score(best_train_predictions, y_train))\n",
    "print('The validation ROC AUC Score is', roc_auc_score(best_val_predictions, y_val))\n",
    "\n",
    "# Let's also explore what parameters ended up being used in the new model.\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial kernel model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVM_models/poly_model.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 1.0, 1: 25.0}\n",
    "clf = svm.SVC(C=10, class_weight = class_weight, kernel = 'poly', gamma = 2**-5, degree = 1, random_state = 23)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Store model\n",
    "joblib.dump(clf, 'SVM_models/poly_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.8048452243568605\n",
      "The validation accuracy is 0.8131916243654823\n",
      "Training Set Confusion Mat.:\n",
      "[[18593  5088]\n",
      " [  203   954]]\n",
      "Precision:  0.15789473684210525\n",
      "Recall:  0.8245462402765773\n",
      "Valid Set Confusion Mat.:\n",
      "[[6177 1703]\n",
      " [  63  337]]\n",
      "Precision:  0.16519607843137254\n",
      "Recall:  0.8425\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "clf = joblib.load('SVM_models/poly_model.pkl')\n",
    "#Predict\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_val_pred = clf.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF Kernel parameter tuning (output included in report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Warning:\n",
    "## This code block may take very long to run,\n",
    "## Results are included in report appendix to save time.\n",
    "\n",
    "parameters = {'C':[0.5,2,8,32,128,512,2048,4096], 'gamma':[0.5,2**-3,2**-5,2**-7, 2**-9, 2**-11, 2**-13, 2**-15]}\n",
    "\n",
    "class_weight = {0: 1.0, 1: 25.0}\n",
    "model =svm.SVC(random_state = 23, class_weight=class_weight, kernel='poly', max_iter = 100000, tol=0.01)\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "class_weight = {0: 1.0, 1: 25.0}\n",
    "grid_obj = GridSearchCV(model, parameters, scoring=scorer, verbose=3, cv=3)\n",
    "\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator.\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Fit the new model.\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the new model.\n",
    "best_train_predictions = best_clf.predict(X_train)\n",
    "best_val_predictions = best_clf.predict(X_val)\n",
    "\n",
    "print('The training ROC AUC Score is', roc_auc_score(best_train_predictions, y_train))\n",
    "print('The validation ROC AUC Score is', roc_auc_score(best_val_predictions, y_val))\n",
    "\n",
    "# Let's also explore what parameters ended up being used in the new model.\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF kernel model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVM_models/rbf_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = {0: 1.0, 1: 25.0}\n",
    "clf = svm.SVC(C=2048, class_weight = class_weight, kernel = 'rbf', gamma = 2**-11 , random_state = 23)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Store model\n",
    "joblib.dump(clf, 'SVM_models/rbf_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9343442479861521\n",
      "The validation accuracy is 0.8582296954314721\n",
      "Training Set Confusion Mat.:\n",
      "[[21779  1902]\n",
      " [   59  1098]]\n",
      "Precision:  0.366\n",
      "Recall:  0.9490060501296457\n",
      "Valid Set Confusion Mat.:\n",
      "[[7202  678]\n",
      " [  79  321]]\n",
      "Precision:  0.3213213213213213\n",
      "Recall:  0.8025\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "clf = joblib.load('SVM_models/rbf_model.pkl')\n",
    "#Predictions\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_val_pred = clf.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode target\n",
    "y_train_NN = np.array(keras.utils.to_categorical(y_train, 2))\n",
    "y_val_NN = np.array(keras.utils.to_categorical(y_val, 2))\n",
    "y_test_NN = np.array(keras.utils.to_categorical(y_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Layer Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 8,578\n",
      "Trainable params: 8,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation = 'relu', input_shape=(64,)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24838 samples, validate on 8280 samples\n",
      "Epoch 1/100\n",
      "24838/24838 [==============================] - 7s 295us/step - loss: 1.2555 - acc: 0.6131 - val_loss: 0.6377 - val_acc: 0.6762\n",
      "Epoch 2/100\n",
      "24838/24838 [==============================] - 2s 75us/step - loss: 1.1153 - acc: 0.6986 - val_loss: 0.6078 - val_acc: 0.6812\n",
      "Epoch 3/100\n",
      "24838/24838 [==============================] - 2s 70us/step - loss: 1.0543 - acc: 0.7198 - val_loss: 0.5866 - val_acc: 0.7178\n",
      "Epoch 4/100\n",
      "24838/24838 [==============================] - 2s 69us/step - loss: 0.9899 - acc: 0.7478 - val_loss: 0.4840 - val_acc: 0.7806\n",
      "Epoch 5/100\n",
      "24838/24838 [==============================] - 2s 73us/step - loss: 0.9464 - acc: 0.7679 - val_loss: 0.4546 - val_acc: 0.8036\n",
      "Epoch 6/100\n",
      "24838/24838 [==============================] - 2s 69us/step - loss: 0.8980 - acc: 0.7897 - val_loss: 0.4909 - val_acc: 0.7911\n",
      "Epoch 7/100\n",
      "24838/24838 [==============================] - 2s 73us/step - loss: 0.8433 - acc: 0.8014 - val_loss: 0.4892 - val_acc: 0.7940\n",
      "Epoch 8/100\n",
      "24838/24838 [==============================] - 2s 96us/step - loss: 0.7931 - acc: 0.8177 - val_loss: 0.4203 - val_acc: 0.8308\n",
      "Epoch 9/100\n",
      "24838/24838 [==============================] - 2s 93us/step - loss: 0.7409 - acc: 0.8280 - val_loss: 0.3541 - val_acc: 0.8571\n",
      "Epoch 10/100\n",
      "24838/24838 [==============================] - 2s 79us/step - loss: 0.7046 - acc: 0.8407 - val_loss: 0.4288 - val_acc: 0.8272\n",
      "Epoch 11/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.6784 - acc: 0.8466 - val_loss: 0.3931 - val_acc: 0.8306\n",
      "Epoch 12/100\n",
      "24838/24838 [==============================] - 2s 91us/step - loss: 0.6517 - acc: 0.8512 - val_loss: 0.3924 - val_acc: 0.8430\n",
      "Epoch 13/100\n",
      "24838/24838 [==============================] - 4s 171us/step - loss: 0.6281 - acc: 0.8584 - val_loss: 0.3656 - val_acc: 0.8469\n",
      "Epoch 14/100\n",
      "24838/24838 [==============================] - 3s 116us/step - loss: 0.6252 - acc: 0.8584 - val_loss: 0.3451 - val_acc: 0.8652\n",
      "Epoch 15/100\n",
      "24838/24838 [==============================] - 3s 125us/step - loss: 0.5835 - acc: 0.8685 - val_loss: 0.4045 - val_acc: 0.8426\n",
      "Epoch 16/100\n",
      "24838/24838 [==============================] - 3s 120us/step - loss: 0.5656 - acc: 0.8662 - val_loss: 0.3103 - val_acc: 0.8775\n",
      "Epoch 17/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.5434 - acc: 0.8751 - val_loss: 0.3064 - val_acc: 0.8849\n",
      "Epoch 18/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.5433 - acc: 0.8773 - val_loss: 0.4052 - val_acc: 0.8414\n",
      "Epoch 19/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.5338 - acc: 0.8768 - val_loss: 0.3641 - val_acc: 0.8540\n",
      "Epoch 20/100\n",
      "24838/24838 [==============================] - 3s 132us/step - loss: 0.5346 - acc: 0.8784 - val_loss: 0.3205 - val_acc: 0.8748\n",
      "Epoch 21/100\n",
      "24838/24838 [==============================] - 2s 91us/step - loss: 0.5175 - acc: 0.8826 - val_loss: 0.2879 - val_acc: 0.8905\n",
      "Epoch 22/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.4882 - acc: 0.8882 - val_loss: 0.3016 - val_acc: 0.8845\n",
      "Epoch 23/100\n",
      "24838/24838 [==============================] - 2s 80us/step - loss: 0.4879 - acc: 0.8901 - val_loss: 0.3299 - val_acc: 0.8664\n",
      "Epoch 24/100\n",
      "24838/24838 [==============================] - 2s 75us/step - loss: 0.4651 - acc: 0.8918 - val_loss: 0.3123 - val_acc: 0.8736\n",
      "Epoch 25/100\n",
      "24838/24838 [==============================] - 2s 77us/step - loss: 0.4643 - acc: 0.8934 - val_loss: 0.2667 - val_acc: 0.9071\n",
      "Epoch 26/100\n",
      "24838/24838 [==============================] - 2s 78us/step - loss: 0.4552 - acc: 0.8978 - val_loss: 0.2819 - val_acc: 0.8930\n",
      "Epoch 27/100\n",
      "24838/24838 [==============================] - 2s 82us/step - loss: 0.4411 - acc: 0.8974 - val_loss: 0.2543 - val_acc: 0.9059\n",
      "Epoch 28/100\n",
      "24838/24838 [==============================] - 2s 83us/step - loss: 0.4145 - acc: 0.9056 - val_loss: 0.2786 - val_acc: 0.8930\n",
      "Epoch 29/100\n",
      "24838/24838 [==============================] - 2s 74us/step - loss: 0.4039 - acc: 0.9080 - val_loss: 0.2543 - val_acc: 0.9045\n",
      "Epoch 30/100\n",
      "24838/24838 [==============================] - 2s 90us/step - loss: 0.4039 - acc: 0.9034 - val_loss: 0.3102 - val_acc: 0.8909\n",
      "Epoch 31/100\n",
      "24838/24838 [==============================] - 2s 83us/step - loss: 0.4028 - acc: 0.9091 - val_loss: 0.2329 - val_acc: 0.9123\n",
      "Epoch 32/100\n",
      "24838/24838 [==============================] - 2s 88us/step - loss: 0.3955 - acc: 0.9097 - val_loss: 0.2258 - val_acc: 0.9169\n",
      "Epoch 33/100\n",
      "24838/24838 [==============================] - 3s 101us/step - loss: 0.3817 - acc: 0.9134 - val_loss: 0.3033 - val_acc: 0.8982\n",
      "Epoch 34/100\n",
      "24838/24838 [==============================] - 2s 98us/step - loss: 0.4014 - acc: 0.9115 - val_loss: 0.2589 - val_acc: 0.9076\n",
      "Epoch 35/100\n",
      "24838/24838 [==============================] - 3s 108us/step - loss: 0.3785 - acc: 0.9146 - val_loss: 0.2603 - val_acc: 0.9040\n",
      "Epoch 36/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.3755 - acc: 0.9141 - val_loss: 0.2172 - val_acc: 0.9187\n",
      "Epoch 37/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.3395 - acc: 0.9252 - val_loss: 0.3060 - val_acc: 0.8909\n",
      "Epoch 38/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.3700 - acc: 0.9150 - val_loss: 0.2557 - val_acc: 0.9091\n",
      "Epoch 39/100\n",
      "24838/24838 [==============================] - 3s 105us/step - loss: 0.3626 - acc: 0.9211 - val_loss: 0.2019 - val_acc: 0.9273\n",
      "Epoch 40/100\n",
      "24838/24838 [==============================] - 2s 93us/step - loss: 0.3444 - acc: 0.9262 - val_loss: 0.2638 - val_acc: 0.9029\n",
      "Epoch 41/100\n",
      "24838/24838 [==============================] - 2s 96us/step - loss: 0.3590 - acc: 0.9215 - val_loss: 0.2758 - val_acc: 0.9006\n",
      "Epoch 42/100\n",
      "24838/24838 [==============================] - 2s 84us/step - loss: 0.3218 - acc: 0.9270 - val_loss: 0.2272 - val_acc: 0.9174\n",
      "Epoch 43/100\n",
      "24838/24838 [==============================] - 2s 83us/step - loss: 0.3301 - acc: 0.9286 - val_loss: 0.2002 - val_acc: 0.9244\n",
      "Epoch 44/100\n",
      "24838/24838 [==============================] - 2s 78us/step - loss: 0.3193 - acc: 0.9291 - val_loss: 0.2063 - val_acc: 0.9264\n",
      "Epoch 45/100\n",
      "24838/24838 [==============================] - 2s 85us/step - loss: 0.3112 - acc: 0.9299 - val_loss: 0.2367 - val_acc: 0.9193\n",
      "Epoch 46/100\n",
      "24838/24838 [==============================] - 2s 81us/step - loss: 0.3010 - acc: 0.9331 - val_loss: 0.2111 - val_acc: 0.9255\n",
      "Epoch 47/100\n",
      "24838/24838 [==============================] - 2s 89us/step - loss: 0.2996 - acc: 0.9347 - val_loss: 0.2013 - val_acc: 0.9283\n",
      "Epoch 48/100\n",
      "24838/24838 [==============================] - 3s 120us/step - loss: 0.2922 - acc: 0.9337 - val_loss: 0.2084 - val_acc: 0.9221\n",
      "Epoch 49/100\n",
      "24838/24838 [==============================] - 3s 111us/step - loss: 0.2939 - acc: 0.9352 - val_loss: 0.1923 - val_acc: 0.9325\n",
      "Epoch 50/100\n",
      "24838/24838 [==============================] - 3s 111us/step - loss: 0.2930 - acc: 0.9359 - val_loss: 0.2694 - val_acc: 0.9043\n",
      "Epoch 51/100\n",
      "24838/24838 [==============================] - 3s 121us/step - loss: 0.3270 - acc: 0.9308 - val_loss: 0.2443 - val_acc: 0.9116\n",
      "Epoch 52/100\n",
      "24838/24838 [==============================] - 3s 121us/step - loss: 0.3139 - acc: 0.9324 - val_loss: 0.2862 - val_acc: 0.8987\n",
      "Epoch 53/100\n",
      "24838/24838 [==============================] - 3s 111us/step - loss: 0.2729 - acc: 0.9413 - val_loss: 0.2322 - val_acc: 0.9167\n",
      "Epoch 54/100\n",
      "24838/24838 [==============================] - 7s 269us/step - loss: 0.2684 - acc: 0.9411 - val_loss: 0.2371 - val_acc: 0.9205\n",
      "Epoch 55/100\n",
      "24838/24838 [==============================] - 3s 126us/step - loss: 0.2676 - acc: 0.9433 - val_loss: 0.2232 - val_acc: 0.9257\n",
      "Epoch 56/100\n",
      "24838/24838 [==============================] - 3s 130us/step - loss: 0.2801 - acc: 0.9403 - val_loss: 0.2085 - val_acc: 0.9243\n",
      "Epoch 57/100\n",
      "24838/24838 [==============================] - 3s 137us/step - loss: 0.2452 - acc: 0.9461 - val_loss: 0.2082 - val_acc: 0.9308\n",
      "Epoch 58/100\n",
      "24838/24838 [==============================] - 4s 176us/step - loss: 0.2604 - acc: 0.9438 - val_loss: 0.2108 - val_acc: 0.9209\n",
      "Epoch 59/100\n",
      "24838/24838 [==============================] - 3s 119us/step - loss: 0.2445 - acc: 0.9483 - val_loss: 0.3340 - val_acc: 0.8959\n",
      "Epoch 60/100\n",
      "24838/24838 [==============================] - 4s 146us/step - loss: 0.3274 - acc: 0.9340 - val_loss: 0.2514 - val_acc: 0.9136\n",
      "Epoch 61/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.2521 - acc: 0.9467 - val_loss: 0.2195 - val_acc: 0.9216\n",
      "Epoch 62/100\n",
      "24838/24838 [==============================] - 2s 97us/step - loss: 0.2404 - acc: 0.9483 - val_loss: 0.1989 - val_acc: 0.9310\n",
      "Epoch 63/100\n",
      "24838/24838 [==============================] - 3s 106us/step - loss: 0.2217 - acc: 0.9525 - val_loss: 0.2599 - val_acc: 0.9053\n",
      "Epoch 64/100\n",
      "24838/24838 [==============================] - 2s 96us/step - loss: 0.2144 - acc: 0.9562 - val_loss: 0.2365 - val_acc: 0.9150\n",
      "Epoch 65/100\n",
      "24838/24838 [==============================] - 3s 114us/step - loss: 0.2188 - acc: 0.9530 - val_loss: 0.1765 - val_acc: 0.9426\n",
      "Epoch 66/100\n",
      "24838/24838 [==============================] - 3s 106us/step - loss: 0.2176 - acc: 0.9541 - val_loss: 0.1799 - val_acc: 0.9390\n",
      "Epoch 67/100\n",
      "24838/24838 [==============================] - 2s 100us/step - loss: 0.2187 - acc: 0.9543 - val_loss: 0.2523 - val_acc: 0.9117\n",
      "Epoch 68/100\n",
      "24838/24838 [==============================] - 3s 116us/step - loss: 0.2512 - acc: 0.9475 - val_loss: 0.1997 - val_acc: 0.9380\n",
      "Epoch 69/100\n",
      "24838/24838 [==============================] - 2s 94us/step - loss: 0.2371 - acc: 0.9488 - val_loss: 0.2186 - val_acc: 0.9226\n",
      "Epoch 70/100\n",
      "24838/24838 [==============================] - 2s 94us/step - loss: 0.2578 - acc: 0.9474 - val_loss: 0.2938 - val_acc: 0.9069\n",
      "Epoch 71/100\n",
      "24838/24838 [==============================] - 3s 123us/step - loss: 0.2386 - acc: 0.9510 - val_loss: 0.1848 - val_acc: 0.9420\n",
      "Epoch 72/100\n",
      "24838/24838 [==============================] - 3s 114us/step - loss: 0.2715 - acc: 0.9492 - val_loss: 0.1880 - val_acc: 0.9362\n",
      "Epoch 73/100\n",
      "24838/24838 [==============================] - 2s 83us/step - loss: 0.2088 - acc: 0.9546 - val_loss: 0.1859 - val_acc: 0.9385\n",
      "Epoch 74/100\n",
      "24838/24838 [==============================] - 2s 86us/step - loss: 0.1945 - acc: 0.9591 - val_loss: 0.1582 - val_acc: 0.9490\n",
      "Epoch 75/100\n",
      "24838/24838 [==============================] - 2s 82us/step - loss: 0.1976 - acc: 0.9607 - val_loss: 0.2326 - val_acc: 0.9281\n",
      "Epoch 76/100\n",
      "24838/24838 [==============================] - 2s 83us/step - loss: 0.1862 - acc: 0.9618 - val_loss: 0.1695 - val_acc: 0.9417\n",
      "Epoch 77/100\n",
      "24838/24838 [==============================] - 2s 84us/step - loss: 0.1750 - acc: 0.9638 - val_loss: 0.1970 - val_acc: 0.9315\n",
      "Epoch 78/100\n",
      "24838/24838 [==============================] - 2s 83us/step - loss: 0.1727 - acc: 0.9651 - val_loss: 0.1608 - val_acc: 0.9489\n",
      "Epoch 79/100\n",
      "24838/24838 [==============================] - 2s 81us/step - loss: 0.2017 - acc: 0.9595 - val_loss: 0.2558 - val_acc: 0.9171\n",
      "Epoch 80/100\n",
      "24838/24838 [==============================] - 3s 114us/step - loss: 0.2662 - acc: 0.9469 - val_loss: 0.2894 - val_acc: 0.9168\n",
      "Epoch 81/100\n",
      "24838/24838 [==============================] - 3s 123us/step - loss: 0.2183 - acc: 0.9552 - val_loss: 0.2238 - val_acc: 0.9320\n",
      "Epoch 82/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.1908 - acc: 0.9591 - val_loss: 0.2039 - val_acc: 0.9388\n",
      "Epoch 83/100\n",
      "24838/24838 [==============================] - 2s 82us/step - loss: 0.2154 - acc: 0.9577 - val_loss: 0.2479 - val_acc: 0.9267\n",
      "Epoch 84/100\n",
      "24838/24838 [==============================] - 2s 78us/step - loss: 0.2102 - acc: 0.9575 - val_loss: 0.2274 - val_acc: 0.9319\n",
      "Epoch 85/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.1923 - acc: 0.9611 - val_loss: 0.1583 - val_acc: 0.9501\n",
      "Epoch 86/100\n",
      "24838/24838 [==============================] - 2s 90us/step - loss: 0.1652 - acc: 0.9663 - val_loss: 0.1685 - val_acc: 0.9476\n",
      "Epoch 87/100\n",
      "24838/24838 [==============================] - 3s 112us/step - loss: 0.1865 - acc: 0.9647 - val_loss: 0.1812 - val_acc: 0.9495\n",
      "Epoch 88/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.1874 - acc: 0.9622 - val_loss: 0.1718 - val_acc: 0.9449\n",
      "Epoch 89/100\n",
      "24838/24838 [==============================] - 2s 87us/step - loss: 0.1606 - acc: 0.9679 - val_loss: 0.2114 - val_acc: 0.9303\n",
      "Epoch 90/100\n",
      "24838/24838 [==============================] - 2s 94us/step - loss: 0.2211 - acc: 0.9561 - val_loss: 0.2718 - val_acc: 0.9262\n",
      "Epoch 91/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.1781 - acc: 0.9649 - val_loss: 0.1786 - val_acc: 0.9435\n",
      "Epoch 92/100\n",
      "24838/24838 [==============================] - 2s 89us/step - loss: 0.1558 - acc: 0.9688 - val_loss: 0.1865 - val_acc: 0.9437\n",
      "Epoch 93/100\n",
      "24838/24838 [==============================] - 3s 105us/step - loss: 0.2004 - acc: 0.9621 - val_loss: 0.1939 - val_acc: 0.9487\n",
      "Epoch 94/100\n",
      "24838/24838 [==============================] - 3s 101us/step - loss: 0.2157 - acc: 0.9581 - val_loss: 0.1920 - val_acc: 0.9405\n",
      "Epoch 95/100\n",
      "24838/24838 [==============================] - 2s 100us/step - loss: 0.1639 - acc: 0.9656 - val_loss: 0.2453 - val_acc: 0.9307\n",
      "Epoch 96/100\n",
      "24838/24838 [==============================] - 2s 97us/step - loss: 0.1531 - acc: 0.9690 - val_loss: 0.1466 - val_acc: 0.9577\n",
      "Epoch 97/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.1384 - acc: 0.9737 - val_loss: 0.1867 - val_acc: 0.9425\n",
      "Epoch 98/100\n",
      "24838/24838 [==============================] - 2s 88us/step - loss: 0.1666 - acc: 0.9680 - val_loss: 0.1806 - val_acc: 0.9524\n",
      "Epoch 99/100\n",
      "24838/24838 [==============================] - 2s 83us/step - loss: 0.1536 - acc: 0.9701 - val_loss: 0.2650 - val_acc: 0.9203\n",
      "Epoch 100/100\n",
      "24838/24838 [==============================] - 2s 89us/step - loss: 0.1792 - acc: 0.9641 - val_loss: 0.1755 - val_acc: 0.9473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_weight = {0: 1., 1: 25.}\n",
    "model.fit(X_train, y_train_NN, validation_data=(X_val, y_val_NN), epochs=100,  batch_size=100, class_weight = class_weight, verbose=1)\n",
    "model.save('NN_models/single_layer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9970641175342806\n",
      "The validation accuracy is 0.93127538071066\n",
      "Training Set Confusion Mat.:\n",
      "[[23000   681]\n",
      " [    3  1154]]\n",
      "Precision:  0.6288828337874659\n",
      "Recall:  0.9974070872947277\n",
      "Valid Set Confusion Mat.:\n",
      "[[7539  341]\n",
      " [  95  305]]\n",
      "Precision:  0.47213622291021673\n",
      "Recall:  0.7625\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model = load_model('NN_models/single_layer.h5')\n",
    "#Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 41,666\n",
      "Trainable params: 41,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(64,)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24838 samples, validate on 8280 samples\n",
      "Epoch 1/100\n",
      "24838/24838 [==============================] - 4s 155us/step - loss: 1.2658 - acc: 0.5914 - val_loss: 0.6041 - val_acc: 0.6739\n",
      "Epoch 2/100\n",
      "24838/24838 [==============================] - 3s 117us/step - loss: 1.1127 - acc: 0.6718 - val_loss: 0.5311 - val_acc: 0.7734\n",
      "Epoch 3/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.9597 - acc: 0.7432 - val_loss: 0.5027 - val_acc: 0.7502\n",
      "Epoch 4/100\n",
      "24838/24838 [==============================] - 3s 114us/step - loss: 0.8698 - acc: 0.7646 - val_loss: 0.3142 - val_acc: 0.8396\n",
      "Epoch 5/100\n",
      "24838/24838 [==============================] - 3s 108us/step - loss: 0.8270 - acc: 0.7781 - val_loss: 0.4121 - val_acc: 0.7946\n",
      "Epoch 6/100\n",
      "24838/24838 [==============================] - 4s 160us/step - loss: 0.7606 - acc: 0.7877 - val_loss: 0.3655 - val_acc: 0.8122\n",
      "Epoch 7/100\n",
      "24838/24838 [==============================] - 3s 111us/step - loss: 0.7547 - acc: 0.8005 - val_loss: 0.3275 - val_acc: 0.8373\n",
      "Epoch 8/100\n",
      "24838/24838 [==============================] - 3s 115us/step - loss: 0.7314 - acc: 0.8106 - val_loss: 0.4121 - val_acc: 0.7831\n",
      "Epoch 9/100\n",
      "24838/24838 [==============================] - 3s 122us/step - loss: 0.6875 - acc: 0.8147 - val_loss: 0.3350 - val_acc: 0.8293\n",
      "Epoch 10/100\n",
      "24838/24838 [==============================] - 3s 125us/step - loss: 0.6486 - acc: 0.8270 - val_loss: 0.3444 - val_acc: 0.8275\n",
      "Epoch 11/100\n",
      "24838/24838 [==============================] - 3s 130us/step - loss: 0.6637 - acc: 0.8218 - val_loss: 0.3004 - val_acc: 0.8508\n",
      "Epoch 12/100\n",
      "24838/24838 [==============================] - 3s 109us/step - loss: 0.6383 - acc: 0.8210 - val_loss: 0.2722 - val_acc: 0.8582\n",
      "Epoch 13/100\n",
      "24838/24838 [==============================] - 3s 112us/step - loss: 0.6033 - acc: 0.8343 - val_loss: 0.2377 - val_acc: 0.8792\n",
      "Epoch 14/100\n",
      "24838/24838 [==============================] - 3s 112us/step - loss: 0.6203 - acc: 0.8373 - val_loss: 0.2755 - val_acc: 0.8545\n",
      "Epoch 15/100\n",
      "24838/24838 [==============================] - 3s 107us/step - loss: 0.5924 - acc: 0.8408 - val_loss: 0.3188 - val_acc: 0.8511\n",
      "Epoch 16/100\n",
      "24838/24838 [==============================] - 3s 104us/step - loss: 0.5872 - acc: 0.8513 - val_loss: 0.2720 - val_acc: 0.8732\n",
      "Epoch 17/100\n",
      "24838/24838 [==============================] - 3s 109us/step - loss: 0.5587 - acc: 0.8526 - val_loss: 0.2966 - val_acc: 0.8470\n",
      "Epoch 18/100\n",
      "24838/24838 [==============================] - 3s 113us/step - loss: 0.5536 - acc: 0.8474 - val_loss: 0.2576 - val_acc: 0.8729\n",
      "Epoch 19/100\n",
      "24838/24838 [==============================] - 3s 107us/step - loss: 0.5273 - acc: 0.8603 - val_loss: 0.4043 - val_acc: 0.7760\n",
      "Epoch 20/100\n",
      "24838/24838 [==============================] - 3s 106us/step - loss: 0.5229 - acc: 0.8504 - val_loss: 0.2783 - val_acc: 0.8599\n",
      "Epoch 21/100\n",
      "24838/24838 [==============================] - 3s 107us/step - loss: 0.5186 - acc: 0.8512 - val_loss: 0.2298 - val_acc: 0.8832\n",
      "Epoch 22/100\n",
      "24838/24838 [==============================] - 3s 107us/step - loss: 0.5162 - acc: 0.8540 - val_loss: 0.2806 - val_acc: 0.8640\n",
      "Epoch 23/100\n",
      "24838/24838 [==============================] - 3s 105us/step - loss: 0.4915 - acc: 0.8657 - val_loss: 0.2962 - val_acc: 0.8566\n",
      "Epoch 24/100\n",
      "24838/24838 [==============================] - 3s 104us/step - loss: 0.4799 - acc: 0.8666 - val_loss: 0.2595 - val_acc: 0.8771\n",
      "Epoch 25/100\n",
      "24838/24838 [==============================] - 3s 108us/step - loss: 0.4778 - acc: 0.8686 - val_loss: 0.2375 - val_acc: 0.8847\n",
      "Epoch 26/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.4652 - acc: 0.8726 - val_loss: 0.2492 - val_acc: 0.8772\n",
      "Epoch 27/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.4728 - acc: 0.8665 - val_loss: 0.2238 - val_acc: 0.8876\n",
      "Epoch 28/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.4763 - acc: 0.8616 - val_loss: 0.2341 - val_acc: 0.8738\n",
      "Epoch 29/100\n",
      "24838/24838 [==============================] - 3s 106us/step - loss: 0.4517 - acc: 0.8812 - val_loss: 0.2147 - val_acc: 0.8957\n",
      "Epoch 30/100\n",
      "24838/24838 [==============================] - 3s 107us/step - loss: 0.4330 - acc: 0.8797 - val_loss: 0.2391 - val_acc: 0.8778\n",
      "Epoch 31/100\n",
      "24838/24838 [==============================] - 3s 113us/step - loss: 0.4285 - acc: 0.8876 - val_loss: 0.2211 - val_acc: 0.9045\n",
      "Epoch 32/100\n",
      "24838/24838 [==============================] - 3s 126us/step - loss: 0.4215 - acc: 0.8849 - val_loss: 0.2445 - val_acc: 0.8822\n",
      "Epoch 33/100\n",
      "24838/24838 [==============================] - 3s 120us/step - loss: 0.4153 - acc: 0.8871 - val_loss: 0.2452 - val_acc: 0.8807\n",
      "Epoch 34/100\n",
      "24838/24838 [==============================] - 3s 117us/step - loss: 0.4220 - acc: 0.8887 - val_loss: 0.2531 - val_acc: 0.8841\n",
      "Epoch 35/100\n",
      "24838/24838 [==============================] - 3s 114us/step - loss: 0.4332 - acc: 0.8875 - val_loss: 0.2268 - val_acc: 0.8907\n",
      "Epoch 36/100\n",
      "24838/24838 [==============================] - 3s 125us/step - loss: 0.3951 - acc: 0.8943 - val_loss: 0.1909 - val_acc: 0.9143\n",
      "Epoch 37/100\n",
      "24838/24838 [==============================] - 3s 113us/step - loss: 0.3871 - acc: 0.8948 - val_loss: 0.2312 - val_acc: 0.8934\n",
      "Epoch 38/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.3824 - acc: 0.8967 - val_loss: 0.1946 - val_acc: 0.9080\n",
      "Epoch 39/100\n",
      "24838/24838 [==============================] - 3s 114us/step - loss: 0.3833 - acc: 0.8945 - val_loss: 0.1665 - val_acc: 0.9244\n",
      "Epoch 40/100\n",
      "24838/24838 [==============================] - 3s 110us/step - loss: 0.3702 - acc: 0.9065 - val_loss: 0.2195 - val_acc: 0.8999\n",
      "Epoch 41/100\n",
      "24838/24838 [==============================] - 3s 116us/step - loss: 0.3797 - acc: 0.9023 - val_loss: 0.1657 - val_acc: 0.9260\n",
      "Epoch 42/100\n",
      "24838/24838 [==============================] - 3s 108us/step - loss: 0.3496 - acc: 0.9143 - val_loss: 0.2067 - val_acc: 0.9103\n",
      "Epoch 43/100\n",
      "24838/24838 [==============================] - 3s 106us/step - loss: 0.3622 - acc: 0.9043 - val_loss: 0.2132 - val_acc: 0.8928\n",
      "Epoch 44/100\n",
      "24838/24838 [==============================] - 4s 151us/step - loss: 0.3644 - acc: 0.8990 - val_loss: 0.1884 - val_acc: 0.9087\n",
      "Epoch 45/100\n",
      "24838/24838 [==============================] - 3s 112us/step - loss: 0.3502 - acc: 0.9076 - val_loss: 0.2098 - val_acc: 0.9097\n",
      "Epoch 46/100\n",
      "24838/24838 [==============================] - 3s 121us/step - loss: 0.3608 - acc: 0.9058 - val_loss: 0.1829 - val_acc: 0.9193\n",
      "Epoch 47/100\n",
      "24838/24838 [==============================] - 3s 108us/step - loss: 0.3567 - acc: 0.9100 - val_loss: 0.1659 - val_acc: 0.9260\n",
      "Epoch 48/100\n",
      "24838/24838 [==============================] - 3s 109us/step - loss: 0.3439 - acc: 0.9163 - val_loss: 0.2125 - val_acc: 0.9035\n",
      "Epoch 49/100\n",
      "24838/24838 [==============================] - 3s 111us/step - loss: 0.3660 - acc: 0.9079 - val_loss: 0.2035 - val_acc: 0.9060\n",
      "Epoch 50/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.3409 - acc: 0.9144 - val_loss: 0.1874 - val_acc: 0.9202\n",
      "Epoch 51/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.3210 - acc: 0.9177 - val_loss: 0.2491 - val_acc: 0.8897\n",
      "Epoch 52/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.3317 - acc: 0.9159 - val_loss: 0.1780 - val_acc: 0.9257\n",
      "Epoch 53/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.3293 - acc: 0.9193 - val_loss: 0.2029 - val_acc: 0.9150\n",
      "Epoch 54/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.3447 - acc: 0.9117 - val_loss: 0.1880 - val_acc: 0.9150\n",
      "Epoch 55/100\n",
      "24838/24838 [==============================] - 3s 104us/step - loss: 0.3431 - acc: 0.9151 - val_loss: 0.2039 - val_acc: 0.9127\n",
      "Epoch 56/100\n",
      "24838/24838 [==============================] - 2s 96us/step - loss: 0.3330 - acc: 0.9126 - val_loss: 0.2372 - val_acc: 0.8815\n",
      "Epoch 57/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.2971 - acc: 0.9181 - val_loss: 0.1490 - val_acc: 0.9356\n",
      "Epoch 58/100\n",
      "24838/24838 [==============================] - 3s 114us/step - loss: 0.3071 - acc: 0.9256 - val_loss: 0.1607 - val_acc: 0.9289\n",
      "Epoch 59/100\n",
      "24838/24838 [==============================] - 3s 105us/step - loss: 0.3112 - acc: 0.9189 - val_loss: 0.2025 - val_acc: 0.9103\n",
      "Epoch 60/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.3354 - acc: 0.9165 - val_loss: 0.1780 - val_acc: 0.9250\n",
      "Epoch 61/100\n",
      "24838/24838 [==============================] - 2s 94us/step - loss: 0.3131 - acc: 0.9249 - val_loss: 0.1643 - val_acc: 0.9287\n",
      "Epoch 62/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.3040 - acc: 0.9262 - val_loss: 0.1635 - val_acc: 0.9281\n",
      "Epoch 63/100\n",
      "24838/24838 [==============================] - 2s 98us/step - loss: 0.2882 - acc: 0.9318 - val_loss: 0.2025 - val_acc: 0.9122\n",
      "Epoch 64/100\n",
      "24838/24838 [==============================] - 2s 93us/step - loss: 0.2960 - acc: 0.9246 - val_loss: 0.1962 - val_acc: 0.9135\n",
      "Epoch 65/100\n",
      "24838/24838 [==============================] - 2s 96us/step - loss: 0.2939 - acc: 0.9237 - val_loss: 0.1667 - val_acc: 0.9240\n",
      "Epoch 66/100\n",
      "24838/24838 [==============================] - 2s 96us/step - loss: 0.2850 - acc: 0.9250 - val_loss: 0.1932 - val_acc: 0.9153\n",
      "Epoch 67/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.3060 - acc: 0.9236 - val_loss: 0.1769 - val_acc: 0.9237\n",
      "Epoch 68/100\n",
      "24838/24838 [==============================] - 2s 98us/step - loss: 0.2963 - acc: 0.9274 - val_loss: 0.1605 - val_acc: 0.9300\n",
      "Epoch 69/100\n",
      "24838/24838 [==============================] - 2s 94us/step - loss: 0.2797 - acc: 0.9299 - val_loss: 0.1747 - val_acc: 0.9231\n",
      "Epoch 70/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.2907 - acc: 0.9285 - val_loss: 0.1573 - val_acc: 0.9338\n",
      "Epoch 71/100\n",
      "24838/24838 [==============================] - 3s 120us/step - loss: 0.3051 - acc: 0.9188 - val_loss: 0.1936 - val_acc: 0.9156\n",
      "Epoch 72/100\n",
      "24838/24838 [==============================] - 4s 154us/step - loss: 0.2873 - acc: 0.9290 - val_loss: 0.1512 - val_acc: 0.9336\n",
      "Epoch 73/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.2810 - acc: 0.9301 - val_loss: 0.1606 - val_acc: 0.9301\n",
      "Epoch 74/100\n",
      "24838/24838 [==============================] - 3s 109us/step - loss: 0.2640 - acc: 0.9371 - val_loss: 0.1385 - val_acc: 0.9444\n",
      "Epoch 75/100\n",
      "24838/24838 [==============================] - 3s 104us/step - loss: 0.2665 - acc: 0.9367 - val_loss: 0.1527 - val_acc: 0.9389\n",
      "Epoch 76/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.2510 - acc: 0.9360 - val_loss: 0.1438 - val_acc: 0.9383\n",
      "Epoch 77/100\n",
      "24838/24838 [==============================] - 2s 100us/step - loss: 0.2595 - acc: 0.9352 - val_loss: 0.1590 - val_acc: 0.9339\n",
      "Epoch 78/100\n",
      "24838/24838 [==============================] - 2s 97us/step - loss: 0.2848 - acc: 0.9299 - val_loss: 0.1746 - val_acc: 0.9249\n",
      "Epoch 79/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.2609 - acc: 0.9369 - val_loss: 0.1676 - val_acc: 0.9321\n",
      "Epoch 80/100\n",
      "24838/24838 [==============================] - 3s 101us/step - loss: 0.2926 - acc: 0.9271 - val_loss: 0.1591 - val_acc: 0.9326\n",
      "Epoch 81/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.2785 - acc: 0.9326 - val_loss: 0.1867 - val_acc: 0.9191\n",
      "Epoch 82/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.2847 - acc: 0.9312 - val_loss: 0.1464 - val_acc: 0.9377\n",
      "Epoch 83/100\n",
      "24838/24838 [==============================] - 2s 100us/step - loss: 0.2721 - acc: 0.9299 - val_loss: 0.1624 - val_acc: 0.9298\n",
      "Epoch 84/100\n",
      "24838/24838 [==============================] - 3s 107us/step - loss: 0.2623 - acc: 0.9361 - val_loss: 0.1476 - val_acc: 0.9449\n",
      "Epoch 85/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.2557 - acc: 0.9389 - val_loss: 0.1563 - val_acc: 0.9347\n",
      "Epoch 86/100\n",
      "24838/24838 [==============================] - 2s 100us/step - loss: 0.2594 - acc: 0.9368 - val_loss: 0.1393 - val_acc: 0.9476\n",
      "Epoch 87/100\n",
      "24838/24838 [==============================] - 3s 104us/step - loss: 0.2523 - acc: 0.9433 - val_loss: 0.1404 - val_acc: 0.9452\n",
      "Epoch 88/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.2383 - acc: 0.9415 - val_loss: 0.1346 - val_acc: 0.9455\n",
      "Epoch 89/100\n",
      "24838/24838 [==============================] - 2s 95us/step - loss: 0.2539 - acc: 0.9372 - val_loss: 0.1310 - val_acc: 0.9460\n",
      "Epoch 90/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.2325 - acc: 0.9456 - val_loss: 0.1512 - val_acc: 0.9414\n",
      "Epoch 91/100\n",
      "24838/24838 [==============================] - 2s 100us/step - loss: 0.2448 - acc: 0.9392 - val_loss: 0.1604 - val_acc: 0.9342\n",
      "Epoch 92/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.2534 - acc: 0.9365 - val_loss: 0.1520 - val_acc: 0.9388\n",
      "Epoch 93/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.2660 - acc: 0.9379 - val_loss: 0.1993 - val_acc: 0.9231\n",
      "Epoch 94/100\n",
      "24838/24838 [==============================] - 2s 96us/step - loss: 0.2554 - acc: 0.9439 - val_loss: 0.1511 - val_acc: 0.9405\n",
      "Epoch 95/100\n",
      "24838/24838 [==============================] - 2s 98us/step - loss: 0.2597 - acc: 0.9435 - val_loss: 0.1611 - val_acc: 0.9426\n",
      "Epoch 96/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.2423 - acc: 0.9476 - val_loss: 0.1441 - val_acc: 0.9450\n",
      "Epoch 97/100\n",
      "24838/24838 [==============================] - 2s 99us/step - loss: 0.2372 - acc: 0.9452 - val_loss: 0.1298 - val_acc: 0.9492\n",
      "Epoch 98/100\n",
      "24838/24838 [==============================] - 3s 103us/step - loss: 0.2209 - acc: 0.9464 - val_loss: 0.1330 - val_acc: 0.9512\n",
      "Epoch 99/100\n",
      "24838/24838 [==============================] - 3s 113us/step - loss: 0.2378 - acc: 0.9454 - val_loss: 0.1407 - val_acc: 0.9444\n",
      "Epoch 100/100\n",
      "24838/24838 [==============================] - 3s 102us/step - loss: 0.2367 - acc: 0.9446 - val_loss: 0.1261 - val_acc: 0.9506\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 1., 1: 25.}\n",
    "model.fit(X_train, y_train_NN, validation_data=(X_val, y_val_NN), epochs=100,  batch_size=100, class_weight = class_weight, verbose=1)\n",
    "model.save('NN_models/deep_layer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9973145653895736\n",
      "The validation accuracy is 0.9583550126903551\n",
      "Training Set Confusion Mat.:\n",
      "[[22942   739]\n",
      " [    3  1154]]\n",
      "Precision:  0.6096143687268886\n",
      "Recall:  0.9974070872947277\n",
      "Valid Set Confusion Mat.:\n",
      "[[7549  331]\n",
      " [  78  322]]\n",
      "Precision:  0.49310872894333846\n",
      "Recall:  0.805\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model = load_model('NN_models/deep_layer.h5')\n",
    "#Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagged Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model # 1 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 1.2814 - acc: 0.5901 - val_loss: 0.7447 - val_acc: 0.5847\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 1.1574 - acc: 0.6635 - val_loss: 0.6746 - val_acc: 0.6386\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 1.0874 - acc: 0.7044 - val_loss: 0.6524 - val_acc: 0.6478\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 1.0510 - acc: 0.7190 - val_loss: 0.5278 - val_acc: 0.7647\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 1.0087 - acc: 0.7447 - val_loss: 0.6471 - val_acc: 0.6910\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.9774 - acc: 0.7575 - val_loss: 0.5526 - val_acc: 0.7308\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.9352 - acc: 0.7725 - val_loss: 0.5604 - val_acc: 0.7448\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.8975 - acc: 0.7840 - val_loss: 0.4611 - val_acc: 0.8076\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.8753 - acc: 0.7896 - val_loss: 0.4118 - val_acc: 0.8391\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.8327 - acc: 0.8086 - val_loss: 0.4435 - val_acc: 0.8211\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.7997 - acc: 0.8205 - val_loss: 0.5525 - val_acc: 0.7686\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.7818 - acc: 0.8269 - val_loss: 0.4225 - val_acc: 0.8208\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.7279 - acc: 0.8342 - val_loss: 0.4367 - val_acc: 0.8232\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.7015 - acc: 0.8451 - val_loss: 0.4886 - val_acc: 0.7994\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.6863 - acc: 0.8485 - val_loss: 0.3590 - val_acc: 0.8652\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 0.6708 - acc: 0.8557 - val_loss: 0.3657 - val_acc: 0.8538\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.6287 - acc: 0.8608 - val_loss: 0.4134 - val_acc: 0.8357\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.6048 - acc: 0.8623 - val_loss: 0.3418 - val_acc: 0.8733\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.5724 - acc: 0.8697 - val_loss: 0.3171 - val_acc: 0.8763\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5745 - acc: 0.8699 - val_loss: 0.3296 - val_acc: 0.8738\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.5605 - acc: 0.8755 - val_loss: 0.3088 - val_acc: 0.8919\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.5312 - acc: 0.8828 - val_loss: 0.3860 - val_acc: 0.8496\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.5488 - acc: 0.8779 - val_loss: 0.3209 - val_acc: 0.8805\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5183 - acc: 0.8820 - val_loss: 0.2923 - val_acc: 0.8882\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4872 - acc: 0.8879 - val_loss: 0.3236 - val_acc: 0.8783\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.4716 - acc: 0.8928 - val_loss: 0.2633 - val_acc: 0.9063\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.4988 - acc: 0.8913 - val_loss: 0.3044 - val_acc: 0.8960\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.4598 - acc: 0.8988 - val_loss: 0.3121 - val_acc: 0.8829\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.4588 - acc: 0.8962 - val_loss: 0.2612 - val_acc: 0.9066\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.4451 - acc: 0.9027 - val_loss: 0.3595 - val_acc: 0.8604\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.4296 - acc: 0.9014 - val_loss: 0.2665 - val_acc: 0.9058\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.4246 - acc: 0.9048 - val_loss: 0.2580 - val_acc: 0.9034\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.4217 - acc: 0.9070 - val_loss: 0.2845 - val_acc: 0.9003\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.4114 - acc: 0.9115 - val_loss: 0.3028 - val_acc: 0.8932\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.4021 - acc: 0.9127 - val_loss: 0.3526 - val_acc: 0.8773\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.4412 - acc: 0.9050 - val_loss: 0.2644 - val_acc: 0.9053\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3947 - acc: 0.9133 - val_loss: 0.2798 - val_acc: 0.8990\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 68us/step - loss: 0.3819 - acc: 0.9149 - val_loss: 0.2928 - val_acc: 0.8931\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.3878 - acc: 0.9127 - val_loss: 0.3136 - val_acc: 0.8865\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.3687 - acc: 0.9194 - val_loss: 0.2437 - val_acc: 0.9126\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.3856 - acc: 0.9154 - val_loss: 0.2540 - val_acc: 0.9087\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.3488 - acc: 0.9237 - val_loss: 0.2532 - val_acc: 0.9092\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3429 - acc: 0.9238 - val_loss: 0.2551 - val_acc: 0.9050\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.3424 - acc: 0.9229 - val_loss: 0.3123 - val_acc: 0.9068\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.3672 - acc: 0.9210 - val_loss: 0.2525 - val_acc: 0.9103\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3443 - acc: 0.9262 - val_loss: 0.3294 - val_acc: 0.8986\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3343 - acc: 0.9274 - val_loss: 0.3577 - val_acc: 0.8874\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.3314 - acc: 0.9288 - val_loss: 0.2481 - val_acc: 0.9118\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3114 - acc: 0.9312 - val_loss: 0.2167 - val_acc: 0.9295\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.3091 - acc: 0.9338 - val_loss: 0.2427 - val_acc: 0.9156\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3016 - acc: 0.9328 - val_loss: 0.2423 - val_acc: 0.9174\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3057 - acc: 0.9348 - val_loss: 0.2762 - val_acc: 0.9085\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.3066 - acc: 0.9316 - val_loss: 0.2135 - val_acc: 0.9256\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2946 - acc: 0.9355 - val_loss: 0.2599 - val_acc: 0.9093\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.3192 - acc: 0.9320 - val_loss: 0.2487 - val_acc: 0.9158\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2864 - acc: 0.9379 - val_loss: 0.1804 - val_acc: 0.9409\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2871 - acc: 0.9397 - val_loss: 0.2722 - val_acc: 0.9138\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2974 - acc: 0.9366 - val_loss: 0.3216 - val_acc: 0.8976\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.3069 - acc: 0.9373 - val_loss: 0.2330 - val_acc: 0.9188\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.2654 - acc: 0.9412 - val_loss: 0.2187 - val_acc: 0.9227\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2477 - acc: 0.9459 - val_loss: 0.1841 - val_acc: 0.9388\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.2670 - acc: 0.9437 - val_loss: 0.2350 - val_acc: 0.9179\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2658 - acc: 0.9431 - val_loss: 0.2160 - val_acc: 0.9242\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2593 - acc: 0.9452 - val_loss: 0.2594 - val_acc: 0.9169\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.2539 - acc: 0.9448 - val_loss: 0.2798 - val_acc: 0.9159\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2378 - acc: 0.9487 - val_loss: 0.2008 - val_acc: 0.9343\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2328 - acc: 0.9513 - val_loss: 0.1784 - val_acc: 0.9444\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2498 - acc: 0.9494 - val_loss: 0.2220 - val_acc: 0.9261\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2207 - acc: 0.9529 - val_loss: 0.1813 - val_acc: 0.9433\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2557 - acc: 0.9463 - val_loss: 0.1979 - val_acc: 0.9374\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.2447 - acc: 0.9479 - val_loss: 0.2035 - val_acc: 0.9335\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2280 - acc: 0.9525 - val_loss: 0.1904 - val_acc: 0.9391\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2650 - acc: 0.9456 - val_loss: 0.2142 - val_acc: 0.9366\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.3316 - acc: 0.9350 - val_loss: 0.2074 - val_acc: 0.9388\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2386 - acc: 0.9514 - val_loss: 0.1992 - val_acc: 0.9354\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.1999 - acc: 0.9586 - val_loss: 0.1936 - val_acc: 0.9366\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1963 - acc: 0.9598 - val_loss: 0.1838 - val_acc: 0.9406\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1890 - acc: 0.9620 - val_loss: 0.2121 - val_acc: 0.9324\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 69us/step - loss: 0.1997 - acc: 0.9581 - val_loss: 0.1972 - val_acc: 0.9366\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2082 - acc: 0.9554 - val_loss: 0.2042 - val_acc: 0.9319\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.3042 - acc: 0.9403 - val_loss: 0.2159 - val_acc: 0.9369\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2006 - acc: 0.9602 - val_loss: 0.1910 - val_acc: 0.9375\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1855 - acc: 0.9613 - val_loss: 0.1794 - val_acc: 0.9465\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1887 - acc: 0.9607 - val_loss: 0.1689 - val_acc: 0.9475\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1789 - acc: 0.9637 - val_loss: 0.1895 - val_acc: 0.9370\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1753 - acc: 0.9634 - val_loss: 0.1636 - val_acc: 0.9525\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1881 - acc: 0.9618 - val_loss: 0.2147 - val_acc: 0.9361\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2382 - acc: 0.9539 - val_loss: 0.2086 - val_acc: 0.9382\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2294 - acc: 0.9532 - val_loss: 0.1758 - val_acc: 0.9425\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1831 - acc: 0.9627 - val_loss: 0.1903 - val_acc: 0.9412\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1843 - acc: 0.9619 - val_loss: 0.1623 - val_acc: 0.9560\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2401 - acc: 0.9552 - val_loss: 0.1829 - val_acc: 0.9469\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1990 - acc: 0.9602 - val_loss: 0.1834 - val_acc: 0.9502\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1668 - acc: 0.9657 - val_loss: 0.1971 - val_acc: 0.9382\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1565 - acc: 0.9691 - val_loss: 0.1703 - val_acc: 0.9494\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1587 - acc: 0.9687 - val_loss: 0.1874 - val_acc: 0.9461\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1658 - acc: 0.9665 - val_loss: 0.1950 - val_acc: 0.9420\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1890 - acc: 0.9620 - val_loss: 0.2321 - val_acc: 0.9467\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1989 - acc: 0.9632 - val_loss: 0.1961 - val_acc: 0.9443\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1832 - acc: 0.9655 - val_loss: 0.3132 - val_acc: 0.9269\n",
      "Saving model...\n",
      "Fitting model # 2 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 99us/step - loss: 1.2325 - acc: 0.6126 - val_loss: 0.6397 - val_acc: 0.6639\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 1.0974 - acc: 0.6788 - val_loss: 0.5660 - val_acc: 0.6998\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 1.0392 - acc: 0.7086 - val_loss: 0.5409 - val_acc: 0.7130\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.9941 - acc: 0.7309 - val_loss: 0.5249 - val_acc: 0.7432\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.9598 - acc: 0.7452 - val_loss: 0.5091 - val_acc: 0.7531\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.9093 - acc: 0.7640 - val_loss: 0.5169 - val_acc: 0.7531\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.8865 - acc: 0.7780 - val_loss: 0.5169 - val_acc: 0.7560\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.8427 - acc: 0.7877 - val_loss: 0.4805 - val_acc: 0.7913\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.8200 - acc: 0.8089 - val_loss: 0.4825 - val_acc: 0.7729\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.7870 - acc: 0.8100 - val_loss: 0.5062 - val_acc: 0.7596\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.7439 - acc: 0.8200 - val_loss: 0.4172 - val_acc: 0.8179\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.7116 - acc: 0.8396 - val_loss: 0.3749 - val_acc: 0.8420\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.6983 - acc: 0.8445 - val_loss: 0.3512 - val_acc: 0.8588\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.6709 - acc: 0.8518 - val_loss: 0.3661 - val_acc: 0.8520\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.6358 - acc: 0.8585 - val_loss: 0.4290 - val_acc: 0.8208\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.6095 - acc: 0.8639 - val_loss: 0.4166 - val_acc: 0.8179\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.5944 - acc: 0.8639 - val_loss: 0.3201 - val_acc: 0.8731\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.5759 - acc: 0.8739 - val_loss: 0.3464 - val_acc: 0.8559\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.5506 - acc: 0.8786 - val_loss: 0.3213 - val_acc: 0.8726\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5513 - acc: 0.8832 - val_loss: 0.2814 - val_acc: 0.8907\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.5140 - acc: 0.8910 - val_loss: 0.3599 - val_acc: 0.8554\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.5111 - acc: 0.8833 - val_loss: 0.2857 - val_acc: 0.8800\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.4832 - acc: 0.8944 - val_loss: 0.3906 - val_acc: 0.8401\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.4722 - acc: 0.8968 - val_loss: 0.3205 - val_acc: 0.8744\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.4699 - acc: 0.8980 - val_loss: 0.2658 - val_acc: 0.8952\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.4480 - acc: 0.9039 - val_loss: 0.3826 - val_acc: 0.8509\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.4540 - acc: 0.9035 - val_loss: 0.3444 - val_acc: 0.8543\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 2s 111us/step - loss: 0.4287 - acc: 0.9080 - val_loss: 0.2912 - val_acc: 0.8804\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.4126 - acc: 0.9097 - val_loss: 0.2709 - val_acc: 0.8847\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.4291 - acc: 0.9095 - val_loss: 0.3256 - val_acc: 0.8731\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.4308 - acc: 0.9064 - val_loss: 0.3103 - val_acc: 0.8707\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.4007 - acc: 0.9126 - val_loss: 0.3156 - val_acc: 0.8675\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4098 - acc: 0.9121 - val_loss: 0.3116 - val_acc: 0.8699\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3850 - acc: 0.9173 - val_loss: 0.3084 - val_acc: 0.8720\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 2s 108us/step - loss: 0.3655 - acc: 0.9208 - val_loss: 0.2940 - val_acc: 0.8865\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 2s 112us/step - loss: 0.3582 - acc: 0.9228 - val_loss: 0.3038 - val_acc: 0.8741\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.3552 - acc: 0.9225 - val_loss: 0.2773 - val_acc: 0.8878\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.3425 - acc: 0.9225 - val_loss: 0.2704 - val_acc: 0.8977\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3595 - acc: 0.9240 - val_loss: 0.2863 - val_acc: 0.8931\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.3367 - acc: 0.9307 - val_loss: 0.3198 - val_acc: 0.8784\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3258 - acc: 0.9260 - val_loss: 0.2643 - val_acc: 0.8981\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3175 - acc: 0.9306 - val_loss: 0.2778 - val_acc: 0.8940\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3463 - acc: 0.9268 - val_loss: 0.2516 - val_acc: 0.9018\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3242 - acc: 0.9312 - val_loss: 0.2266 - val_acc: 0.9138\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3011 - acc: 0.9345 - val_loss: 0.2507 - val_acc: 0.9081\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3031 - acc: 0.9349 - val_loss: 0.2118 - val_acc: 0.9250\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.3014 - acc: 0.9374 - val_loss: 0.3275 - val_acc: 0.8892\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.3019 - acc: 0.9356 - val_loss: 0.2221 - val_acc: 0.9214\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2989 - acc: 0.9362 - val_loss: 0.2753 - val_acc: 0.9056\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.2932 - acc: 0.9362 - val_loss: 0.2515 - val_acc: 0.9079\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2970 - acc: 0.9363 - val_loss: 0.2720 - val_acc: 0.8982\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2662 - acc: 0.9432 - val_loss: 0.2455 - val_acc: 0.9066\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 0.2925 - acc: 0.9391 - val_loss: 0.2332 - val_acc: 0.9193\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.2881 - acc: 0.9397 - val_loss: 0.2489 - val_acc: 0.9081\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2989 - acc: 0.9385 - val_loss: 0.2435 - val_acc: 0.9114\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.2696 - acc: 0.9425 - val_loss: 0.1844 - val_acc: 0.9444\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2489 - acc: 0.9482 - val_loss: 0.1948 - val_acc: 0.9398\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2585 - acc: 0.9467 - val_loss: 0.2376 - val_acc: 0.9142\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2770 - acc: 0.9449 - val_loss: 0.2246 - val_acc: 0.9229\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2492 - acc: 0.9472 - val_loss: 0.2069 - val_acc: 0.9309\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2615 - acc: 0.9473 - val_loss: 0.2549 - val_acc: 0.9087\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2332 - acc: 0.9506 - val_loss: 0.1988 - val_acc: 0.9353\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2315 - acc: 0.9521 - val_loss: 0.2343 - val_acc: 0.9238\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2194 - acc: 0.9541 - val_loss: 0.1891 - val_acc: 0.9353\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2462 - acc: 0.9495 - val_loss: 0.2689 - val_acc: 0.9103\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2488 - acc: 0.9502 - val_loss: 0.2338 - val_acc: 0.9156\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2311 - acc: 0.9530 - val_loss: 0.2340 - val_acc: 0.9156\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2107 - acc: 0.9559 - val_loss: 0.1997 - val_acc: 0.9393\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.2087 - acc: 0.9582 - val_loss: 0.2133 - val_acc: 0.9279\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.2314 - acc: 0.9523 - val_loss: 0.2332 - val_acc: 0.9225\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2302 - acc: 0.9522 - val_loss: 0.2289 - val_acc: 0.9219\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.2079 - acc: 0.9566 - val_loss: 0.2026 - val_acc: 0.9383\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.1926 - acc: 0.9610 - val_loss: 0.2059 - val_acc: 0.9359\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.2102 - acc: 0.9585 - val_loss: 0.3768 - val_acc: 0.8879\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 2s 99us/step - loss: 0.2221 - acc: 0.9569 - val_loss: 0.2645 - val_acc: 0.9213\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.1903 - acc: 0.9625 - val_loss: 0.1839 - val_acc: 0.9464\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2365 - acc: 0.9549 - val_loss: 0.2347 - val_acc: 0.9246\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 0.2308 - acc: 0.9540 - val_loss: 0.2222 - val_acc: 0.9454\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2095 - acc: 0.9604 - val_loss: 0.2163 - val_acc: 0.9335\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2040 - acc: 0.9622 - val_loss: 0.3270 - val_acc: 0.9122\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2332 - acc: 0.9575 - val_loss: 0.2837 - val_acc: 0.9225\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2230 - acc: 0.9594 - val_loss: 0.2240 - val_acc: 0.9293\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 2s 121us/step - loss: 0.1860 - acc: 0.9624 - val_loss: 0.2287 - val_acc: 0.9311\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 2s 113us/step - loss: 0.1769 - acc: 0.9666 - val_loss: 0.2053 - val_acc: 0.9378\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 2s 110us/step - loss: 0.1629 - acc: 0.9685 - val_loss: 0.2105 - val_acc: 0.9351\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 2s 119us/step - loss: 0.1790 - acc: 0.9655 - val_loss: 0.2813 - val_acc: 0.9076\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 2s 134us/step - loss: 0.1656 - acc: 0.9677 - val_loss: 0.2208 - val_acc: 0.9349\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 2s 119us/step - loss: 0.1623 - acc: 0.9685 - val_loss: 0.1966 - val_acc: 0.9409\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.1543 - acc: 0.9710 - val_loss: 0.2368 - val_acc: 0.9196\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1697 - acc: 0.9664 - val_loss: 0.2129 - val_acc: 0.9396\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 3s 140us/step - loss: 0.1865 - acc: 0.9641 - val_loss: 0.2088 - val_acc: 0.9432\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 3s 143us/step - loss: 0.2469 - acc: 0.9579 - val_loss: 0.3229 - val_acc: 0.9114\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 2s 121us/step - loss: 0.2243 - acc: 0.9587 - val_loss: 0.1980 - val_acc: 0.9480\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 2s 108us/step - loss: 0.1640 - acc: 0.9693 - val_loss: 0.2057 - val_acc: 0.9401\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1483 - acc: 0.9714 - val_loss: 0.2065 - val_acc: 0.9415\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1465 - acc: 0.9728 - val_loss: 0.2314 - val_acc: 0.9317\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1471 - acc: 0.9712 - val_loss: 0.2160 - val_acc: 0.9386\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.1430 - acc: 0.9740 - val_loss: 0.1929 - val_acc: 0.9496\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1320 - acc: 0.9782 - val_loss: 0.1963 - val_acc: 0.9480\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.1449 - acc: 0.9728 - val_loss: 0.2293 - val_acc: 0.9349\n",
      "Saving model...\n",
      "Fitting model # 3 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 1.3350 - acc: 0.5612 - val_loss: 0.6437 - val_acc: 0.6507\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 69us/step - loss: 1.1260 - acc: 0.6834 - val_loss: 0.6140 - val_acc: 0.6915\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 1.0386 - acc: 0.7249 - val_loss: 0.5321 - val_acc: 0.7626\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.9701 - acc: 0.7598 - val_loss: 0.4708 - val_acc: 0.8018\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.9243 - acc: 0.7721 - val_loss: 0.5392 - val_acc: 0.7531\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.8526 - acc: 0.8004 - val_loss: 0.4868 - val_acc: 0.7916\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.8284 - acc: 0.8055 - val_loss: 0.4429 - val_acc: 0.8169\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.7863 - acc: 0.8174 - val_loss: 0.4115 - val_acc: 0.8440\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 0.7479 - acc: 0.8306 - val_loss: 0.4179 - val_acc: 0.8275\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 2s 103us/step - loss: 0.7171 - acc: 0.8359 - val_loss: 0.4733 - val_acc: 0.8145\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 3s 138us/step - loss: 0.6756 - acc: 0.8426 - val_loss: 0.4232 - val_acc: 0.8229\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 3s 138us/step - loss: 0.6480 - acc: 0.8501 - val_loss: 0.3910 - val_acc: 0.8383\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 2s 109us/step - loss: 0.6079 - acc: 0.8572 - val_loss: 0.3724 - val_acc: 0.8549\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 2s 104us/step - loss: 0.5976 - acc: 0.8611 - val_loss: 0.4730 - val_acc: 0.8116\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 3s 134us/step - loss: 0.5776 - acc: 0.8660 - val_loss: 0.3911 - val_acc: 0.8432\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 2s 104us/step - loss: 0.5883 - acc: 0.8654 - val_loss: 0.4054 - val_acc: 0.8367\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 0.5325 - acc: 0.8755 - val_loss: 0.3166 - val_acc: 0.8868\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 3s 145us/step - loss: 0.5293 - acc: 0.8803 - val_loss: 0.3663 - val_acc: 0.8573\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.5050 - acc: 0.8826 - val_loss: 0.3045 - val_acc: 0.8831\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5064 - acc: 0.8861 - val_loss: 0.3210 - val_acc: 0.8749\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.5035 - acc: 0.8862 - val_loss: 0.3649 - val_acc: 0.8604\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.4731 - acc: 0.8886 - val_loss: 0.3356 - val_acc: 0.8799\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4469 - acc: 0.8948 - val_loss: 0.2912 - val_acc: 0.8868\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.4413 - acc: 0.8968 - val_loss: 0.4064 - val_acc: 0.8470\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.4559 - acc: 0.8920 - val_loss: 0.2952 - val_acc: 0.8866\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.4217 - acc: 0.9008 - val_loss: 0.3177 - val_acc: 0.8768\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 69us/step - loss: 0.4030 - acc: 0.9060 - val_loss: 0.2771 - val_acc: 0.8950\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.4001 - acc: 0.9083 - val_loss: 0.2621 - val_acc: 0.9085\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3973 - acc: 0.9061 - val_loss: 0.3608 - val_acc: 0.8610\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.3766 - acc: 0.9121 - val_loss: 0.3548 - val_acc: 0.8576\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.3716 - acc: 0.9112 - val_loss: 0.3538 - val_acc: 0.8715\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 69us/step - loss: 0.3891 - acc: 0.9108 - val_loss: 0.2909 - val_acc: 0.8984\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3931 - acc: 0.9141 - val_loss: 0.3099 - val_acc: 0.8900\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.3458 - acc: 0.9195 - val_loss: 0.2477 - val_acc: 0.9130\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3685 - acc: 0.9163 - val_loss: 0.3274 - val_acc: 0.8752\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.3320 - acc: 0.9206 - val_loss: 0.2274 - val_acc: 0.9300\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.3271 - acc: 0.9226 - val_loss: 0.2597 - val_acc: 0.9103\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3440 - acc: 0.9206 - val_loss: 0.2768 - val_acc: 0.9076\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.3360 - acc: 0.9225 - val_loss: 0.2802 - val_acc: 0.9085\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.3222 - acc: 0.9263 - val_loss: 0.2911 - val_acc: 0.9008\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.3014 - acc: 0.9301 - val_loss: 0.3291 - val_acc: 0.8833\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3091 - acc: 0.9257 - val_loss: 0.3089 - val_acc: 0.9029\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.2932 - acc: 0.9326 - val_loss: 0.2334 - val_acc: 0.9238\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 2s 108us/step - loss: 0.2800 - acc: 0.9345 - val_loss: 0.2263 - val_acc: 0.9279\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 2s 107us/step - loss: 0.2647 - acc: 0.9396 - val_loss: 0.2631 - val_acc: 0.9106\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 2s 127us/step - loss: 0.2894 - acc: 0.9323 - val_loss: 0.2946 - val_acc: 0.8979\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 105us/step - loss: 0.2712 - acc: 0.9372 - val_loss: 0.3078 - val_acc: 0.8950\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2582 - acc: 0.9393 - val_loss: 0.2592 - val_acc: 0.9090\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2676 - acc: 0.9374 - val_loss: 0.2771 - val_acc: 0.9066\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2931 - acc: 0.9361 - val_loss: 0.4665 - val_acc: 0.8519\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3484 - acc: 0.9233 - val_loss: 0.2443 - val_acc: 0.9222\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2571 - acc: 0.9422 - val_loss: 0.2257 - val_acc: 0.9287\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.2713 - acc: 0.9390 - val_loss: 0.3020 - val_acc: 0.8931\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2375 - acc: 0.9422 - val_loss: 0.2459 - val_acc: 0.9253\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.2355 - acc: 0.9450 - val_loss: 0.2225 - val_acc: 0.9311\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.2113 - acc: 0.9513 - val_loss: 0.2127 - val_acc: 0.9335\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2106 - acc: 0.9516 - val_loss: 0.2205 - val_acc: 0.9319\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2292 - acc: 0.9465 - val_loss: 0.3473 - val_acc: 0.8948\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2208 - acc: 0.9471 - val_loss: 0.2067 - val_acc: 0.9382\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2032 - acc: 0.9529 - val_loss: 0.2591 - val_acc: 0.9182\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2106 - acc: 0.9517 - val_loss: 0.2439 - val_acc: 0.9259\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2205 - acc: 0.9510 - val_loss: 0.2390 - val_acc: 0.9279\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2084 - acc: 0.9512 - val_loss: 0.2436 - val_acc: 0.9230\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2649 - acc: 0.9448 - val_loss: 0.3511 - val_acc: 0.9021\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2523 - acc: 0.9437 - val_loss: 0.2921 - val_acc: 0.8977\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2374 - acc: 0.9479 - val_loss: 0.2612 - val_acc: 0.9188\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2167 - acc: 0.9472 - val_loss: 0.2208 - val_acc: 0.9325\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1803 - acc: 0.9573 - val_loss: 0.2062 - val_acc: 0.9411\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1896 - acc: 0.9572 - val_loss: 0.2443 - val_acc: 0.9320\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.1838 - acc: 0.9603 - val_loss: 0.2572 - val_acc: 0.9238\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1899 - acc: 0.9550 - val_loss: 0.2558 - val_acc: 0.9271\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.1659 - acc: 0.9633 - val_loss: 0.2348 - val_acc: 0.9306\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.1932 - acc: 0.9578 - val_loss: 0.2706 - val_acc: 0.9163\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1815 - acc: 0.9582 - val_loss: 0.2181 - val_acc: 0.9372\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2055 - acc: 0.9558 - val_loss: 0.1914 - val_acc: 0.9472\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 2s 112us/step - loss: 0.1764 - acc: 0.9605 - val_loss: 0.2159 - val_acc: 0.9375\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.1483 - acc: 0.9665 - val_loss: 0.2201 - val_acc: 0.9361\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1546 - acc: 0.9646 - val_loss: 0.1853 - val_acc: 0.9535\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1880 - acc: 0.9596 - val_loss: 0.3710 - val_acc: 0.9060\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2307 - acc: 0.9524 - val_loss: 0.2234 - val_acc: 0.9395\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.1727 - acc: 0.9612 - val_loss: 0.2693 - val_acc: 0.9209\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.1771 - acc: 0.9604 - val_loss: 0.2380 - val_acc: 0.9353\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.1527 - acc: 0.9659 - val_loss: 0.2485 - val_acc: 0.9277\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 70us/step - loss: 0.1522 - acc: 0.9655 - val_loss: 0.2184 - val_acc: 0.9464\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1700 - acc: 0.9626 - val_loss: 0.2634 - val_acc: 0.9329\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1526 - acc: 0.9657 - val_loss: 0.2486 - val_acc: 0.9290\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1357 - acc: 0.9700 - val_loss: 0.2733 - val_acc: 0.9201\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1585 - acc: 0.9647 - val_loss: 0.2510 - val_acc: 0.9348\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.1913 - acc: 0.9591 - val_loss: 0.2304 - val_acc: 0.9419\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 0.2034 - acc: 0.9567 - val_loss: 0.2678 - val_acc: 0.9406\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 3s 154us/step - loss: 0.1622 - acc: 0.9650 - val_loss: 0.2365 - val_acc: 0.9386\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.1296 - acc: 0.9712 - val_loss: 0.2172 - val_acc: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.1412 - acc: 0.9687 - val_loss: 0.3067 - val_acc: 0.9163\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 2s 116us/step - loss: 0.1541 - acc: 0.9663 - val_loss: 0.2045 - val_acc: 0.9475\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1286 - acc: 0.9707 - val_loss: 0.2204 - val_acc: 0.9414\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.1250 - acc: 0.9728 - val_loss: 0.2248 - val_acc: 0.9470\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1310 - acc: 0.9729 - val_loss: 0.2465 - val_acc: 0.9366\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.1207 - acc: 0.9736 - val_loss: 0.2106 - val_acc: 0.9440\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 0.1165 - acc: 0.9733 - val_loss: 0.2583 - val_acc: 0.9372\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1201 - acc: 0.9748 - val_loss: 0.2097 - val_acc: 0.9464\n",
      "Saving model...\n",
      "Fitting model # 4 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 106us/step - loss: 1.2536 - acc: 0.6081 - val_loss: 0.6022 - val_acc: 0.6919\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 1.1217 - acc: 0.6722 - val_loss: 0.5376 - val_acc: 0.7480\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 1.0729 - acc: 0.7123 - val_loss: 0.5388 - val_acc: 0.7391\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 1.0259 - acc: 0.7207 - val_loss: 0.6563 - val_acc: 0.6729\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 0.9895 - acc: 0.7426 - val_loss: 0.5033 - val_acc: 0.7783\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 2s 122us/step - loss: 0.9428 - acc: 0.7662 - val_loss: 0.4587 - val_acc: 0.8093\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 2s 126us/step - loss: 0.8925 - acc: 0.7885 - val_loss: 0.4787 - val_acc: 0.7955\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 2s 114us/step - loss: 0.8260 - acc: 0.8078 - val_loss: 0.4322 - val_acc: 0.8111\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.8136 - acc: 0.8101 - val_loss: 0.4793 - val_acc: 0.7969\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.7619 - acc: 0.8261 - val_loss: 0.5597 - val_acc: 0.7646\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.7231 - acc: 0.8355 - val_loss: 0.3946 - val_acc: 0.8351\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.6766 - acc: 0.8468 - val_loss: 0.4094 - val_acc: 0.8272\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 2s 109us/step - loss: 0.6536 - acc: 0.8512 - val_loss: 0.3445 - val_acc: 0.8659\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 2s 119us/step - loss: 0.6136 - acc: 0.8624 - val_loss: 0.3852 - val_acc: 0.8346\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.5897 - acc: 0.8674 - val_loss: 0.3027 - val_acc: 0.8797\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.5879 - acc: 0.8654 - val_loss: 0.3211 - val_acc: 0.8660\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.5651 - acc: 0.8703 - val_loss: 0.4020 - val_acc: 0.8354\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 0.5348 - acc: 0.8772 - val_loss: 0.2659 - val_acc: 0.8990\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 0.5298 - acc: 0.8778 - val_loss: 0.3295 - val_acc: 0.8723\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 2s 110us/step - loss: 0.5084 - acc: 0.8855 - val_loss: 0.3653 - val_acc: 0.8443\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.5147 - acc: 0.8806 - val_loss: 0.3700 - val_acc: 0.8541\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.5201 - acc: 0.8857 - val_loss: 0.3240 - val_acc: 0.8773\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.4846 - acc: 0.8912 - val_loss: 0.3441 - val_acc: 0.8676\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.4550 - acc: 0.8948 - val_loss: 0.3135 - val_acc: 0.8688\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.4613 - acc: 0.8962 - val_loss: 0.3098 - val_acc: 0.8874\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.4413 - acc: 0.8955 - val_loss: 0.3185 - val_acc: 0.8837\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4377 - acc: 0.9020 - val_loss: 0.3164 - val_acc: 0.8731\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.4213 - acc: 0.9025 - val_loss: 0.2679 - val_acc: 0.8942\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 81us/step - loss: 0.3926 - acc: 0.9109 - val_loss: 0.2663 - val_acc: 0.8937\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.4138 - acc: 0.9050 - val_loss: 0.3318 - val_acc: 0.8755\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 2s 112us/step - loss: 0.3818 - acc: 0.9114 - val_loss: 0.2232 - val_acc: 0.9167\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3840 - acc: 0.9145 - val_loss: 0.3683 - val_acc: 0.8477\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 0.3749 - acc: 0.9131 - val_loss: 0.3394 - val_acc: 0.8662\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3721 - acc: 0.9118 - val_loss: 0.2277 - val_acc: 0.9203\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3926 - acc: 0.9142 - val_loss: 0.2652 - val_acc: 0.9002\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.3980 - acc: 0.9099 - val_loss: 0.2826 - val_acc: 0.9002\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 2s 127us/step - loss: 0.3703 - acc: 0.9160 - val_loss: 0.3177 - val_acc: 0.8860\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 4s 191us/step - loss: 0.3522 - acc: 0.9238 - val_loss: 0.2863 - val_acc: 0.8860\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 99us/step - loss: 0.3385 - acc: 0.9238 - val_loss: 0.2407 - val_acc: 0.9035\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.3378 - acc: 0.9240 - val_loss: 0.2830 - val_acc: 0.8905\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.3356 - acc: 0.9245 - val_loss: 0.2928 - val_acc: 0.8907\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3294 - acc: 0.9240 - val_loss: 0.2658 - val_acc: 0.9074\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.3091 - acc: 0.9289 - val_loss: 0.2270 - val_acc: 0.9187\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.3049 - acc: 0.9316 - val_loss: 0.2048 - val_acc: 0.9262\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 0.3064 - acc: 0.9309 - val_loss: 0.2239 - val_acc: 0.9200\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.3356 - acc: 0.9265 - val_loss: 0.3000 - val_acc: 0.8957\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.3035 - acc: 0.9331 - val_loss: 0.2520 - val_acc: 0.9047\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.2775 - acc: 0.9379 - val_loss: 0.2525 - val_acc: 0.9008\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.2842 - acc: 0.9365 - val_loss: 0.2377 - val_acc: 0.9092\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2782 - acc: 0.9389 - val_loss: 0.4104 - val_acc: 0.8749\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2884 - acc: 0.9386 - val_loss: 0.2283 - val_acc: 0.9143\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2803 - acc: 0.9361 - val_loss: 0.2302 - val_acc: 0.9184\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.2863 - acc: 0.9377 - val_loss: 0.2120 - val_acc: 0.9356\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.2584 - acc: 0.9448 - val_loss: 0.1956 - val_acc: 0.9312\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.2442 - acc: 0.9455 - val_loss: 0.2218 - val_acc: 0.9221\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.2287 - acc: 0.9502 - val_loss: 0.2199 - val_acc: 0.9205\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2789 - acc: 0.9400 - val_loss: 0.2071 - val_acc: 0.9319\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2419 - acc: 0.9482 - val_loss: 0.1999 - val_acc: 0.9298\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.2233 - acc: 0.9508 - val_loss: 0.1808 - val_acc: 0.9422\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 0.2293 - acc: 0.9512 - val_loss: 0.2021 - val_acc: 0.9280\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.2243 - acc: 0.9503 - val_loss: 0.2055 - val_acc: 0.9283\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 2s 105us/step - loss: 0.2529 - acc: 0.9488 - val_loss: 0.1905 - val_acc: 0.9374\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2279 - acc: 0.9513 - val_loss: 0.2188 - val_acc: 0.9203\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 2s 104us/step - loss: 0.2390 - acc: 0.9506 - val_loss: 0.2048 - val_acc: 0.9383\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.2555 - acc: 0.9470 - val_loss: 0.1780 - val_acc: 0.9461\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.2235 - acc: 0.9530 - val_loss: 0.2011 - val_acc: 0.9296\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.1938 - acc: 0.9595 - val_loss: 0.1894 - val_acc: 0.9386\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.3087 - acc: 0.9361 - val_loss: 0.2576 - val_acc: 0.9198\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2230 - acc: 0.9540 - val_loss: 0.2215 - val_acc: 0.9213\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2262 - acc: 0.9548 - val_loss: 0.2398 - val_acc: 0.9217\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2009 - acc: 0.9574 - val_loss: 0.2113 - val_acc: 0.9309\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1915 - acc: 0.9610 - val_loss: 0.1915 - val_acc: 0.9346\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1786 - acc: 0.9645 - val_loss: 0.1880 - val_acc: 0.9366\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1828 - acc: 0.9634 - val_loss: 0.2087 - val_acc: 0.9398\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1794 - acc: 0.9631 - val_loss: 0.1940 - val_acc: 0.9385\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1756 - acc: 0.9632 - val_loss: 0.1822 - val_acc: 0.9425\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.1746 - acc: 0.9647 - val_loss: 0.1927 - val_acc: 0.9391\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1937 - acc: 0.9609 - val_loss: 0.3337 - val_acc: 0.9180\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.2837 - acc: 0.9484 - val_loss: 0.2260 - val_acc: 0.9282\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 3s 182us/step - loss: 0.2476 - acc: 0.9543 - val_loss: 0.2347 - val_acc: 0.9279\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 2s 108us/step - loss: 0.1923 - acc: 0.9632 - val_loss: 0.2429 - val_acc: 0.9211\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.1612 - acc: 0.9677 - val_loss: 0.1859 - val_acc: 0.9451\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.1560 - acc: 0.9709 - val_loss: 0.1601 - val_acc: 0.9567\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1520 - acc: 0.9705 - val_loss: 0.1815 - val_acc: 0.9425\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.1565 - acc: 0.9691 - val_loss: 0.1790 - val_acc: 0.9481\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1645 - acc: 0.9666 - val_loss: 0.2283 - val_acc: 0.9407\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1643 - acc: 0.9666 - val_loss: 0.1904 - val_acc: 0.9403\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.1637 - acc: 0.9688 - val_loss: 0.2318 - val_acc: 0.9275\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.1939 - acc: 0.9608 - val_loss: 0.2015 - val_acc: 0.9364\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.1643 - acc: 0.9666 - val_loss: 0.1987 - val_acc: 0.9459\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1588 - acc: 0.9675 - val_loss: 0.1930 - val_acc: 0.9436\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.1600 - acc: 0.9688 - val_loss: 0.1970 - val_acc: 0.9393\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.1492 - acc: 0.9713 - val_loss: 0.1578 - val_acc: 0.9585\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1770 - acc: 0.9648 - val_loss: 0.1978 - val_acc: 0.9469\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1477 - acc: 0.9722 - val_loss: 0.1843 - val_acc: 0.9456\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 64us/step - loss: 0.1335 - acc: 0.9751 - val_loss: 0.1953 - val_acc: 0.9448\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 63us/step - loss: 0.1524 - acc: 0.9719 - val_loss: 0.1807 - val_acc: 0.9465\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 65us/step - loss: 0.1368 - acc: 0.9746 - val_loss: 0.1765 - val_acc: 0.9493\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 62us/step - loss: 0.1383 - acc: 0.9746 - val_loss: 0.1832 - val_acc: 0.9462\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.1242 - acc: 0.9768 - val_loss: 0.1575 - val_acc: 0.9589\n",
      "Saving model...\n",
      "Fitting model # 5 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 131us/step - loss: 1.2774 - acc: 0.5946 - val_loss: 0.6537 - val_acc: 0.6324\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 1.1363 - acc: 0.6596 - val_loss: 0.5971 - val_acc: 0.6689\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 1.0777 - acc: 0.6874 - val_loss: 0.6414 - val_acc: 0.6696\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 1.0191 - acc: 0.7248 - val_loss: 0.6657 - val_acc: 0.6525\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.9861 - acc: 0.7323 - val_loss: 0.5599 - val_acc: 0.7232\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.9416 - acc: 0.7481 - val_loss: 0.4989 - val_acc: 0.7900\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.8945 - acc: 0.7756 - val_loss: 0.5401 - val_acc: 0.7457\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 64us/step - loss: 0.8644 - acc: 0.7814 - val_loss: 0.5215 - val_acc: 0.7694\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.8406 - acc: 0.7923 - val_loss: 0.4466 - val_acc: 0.8169\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.7934 - acc: 0.8093 - val_loss: 0.4377 - val_acc: 0.8151\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.7638 - acc: 0.8144 - val_loss: 0.3947 - val_acc: 0.8501\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 63us/step - loss: 0.7269 - acc: 0.8327 - val_loss: 0.4607 - val_acc: 0.8093\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 63us/step - loss: 0.7032 - acc: 0.8359 - val_loss: 0.3843 - val_acc: 0.8515\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 66us/step - loss: 0.6706 - acc: 0.8468 - val_loss: 0.4605 - val_acc: 0.7979\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.6668 - acc: 0.8470 - val_loss: 0.5203 - val_acc: 0.7797\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.6309 - acc: 0.8544 - val_loss: 0.3610 - val_acc: 0.8560\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.5949 - acc: 0.8625 - val_loss: 0.3698 - val_acc: 0.8602\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.5692 - acc: 0.8752 - val_loss: 0.3543 - val_acc: 0.8605\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.5453 - acc: 0.8730 - val_loss: 0.3400 - val_acc: 0.8692\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.5274 - acc: 0.8774 - val_loss: 0.3459 - val_acc: 0.8760\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.5099 - acc: 0.8791 - val_loss: 0.3139 - val_acc: 0.8828\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.4964 - acc: 0.8864 - val_loss: 0.3332 - val_acc: 0.8728\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.5393 - acc: 0.8809 - val_loss: 0.6689 - val_acc: 0.7626\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5235 - acc: 0.8804 - val_loss: 0.3117 - val_acc: 0.8750\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4735 - acc: 0.8915 - val_loss: 0.2803 - val_acc: 0.8968\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4406 - acc: 0.9019 - val_loss: 0.2950 - val_acc: 0.8897\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4426 - acc: 0.8995 - val_loss: 0.3787 - val_acc: 0.8470\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4146 - acc: 0.9031 - val_loss: 0.2663 - val_acc: 0.8998\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4074 - acc: 0.9068 - val_loss: 0.3039 - val_acc: 0.8857\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4023 - acc: 0.9058 - val_loss: 0.3045 - val_acc: 0.8792\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3987 - acc: 0.9080 - val_loss: 0.3271 - val_acc: 0.8876\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4026 - acc: 0.9086 - val_loss: 0.3104 - val_acc: 0.8839\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3823 - acc: 0.9104 - val_loss: 0.2974 - val_acc: 0.8889\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3637 - acc: 0.9146 - val_loss: 0.2307 - val_acc: 0.9176\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3673 - acc: 0.9162 - val_loss: 0.3224 - val_acc: 0.8810\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3416 - acc: 0.9201 - val_loss: 0.2985 - val_acc: 0.8932\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3368 - acc: 0.9219 - val_loss: 0.2838 - val_acc: 0.8971\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3605 - acc: 0.9179 - val_loss: 0.3837 - val_acc: 0.8787\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3411 - acc: 0.9208 - val_loss: 0.2615 - val_acc: 0.9106\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3338 - acc: 0.9241 - val_loss: 0.2569 - val_acc: 0.9113\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3135 - acc: 0.9267 - val_loss: 0.2512 - val_acc: 0.9124\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3068 - acc: 0.9302 - val_loss: 0.2914 - val_acc: 0.9118\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3127 - acc: 0.9281 - val_loss: 0.2125 - val_acc: 0.9380\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2998 - acc: 0.9311 - val_loss: 0.3167 - val_acc: 0.8918\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3377 - acc: 0.9284 - val_loss: 0.3283 - val_acc: 0.9056\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.3422 - acc: 0.9210 - val_loss: 0.2322 - val_acc: 0.9258\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 99us/step - loss: 0.2779 - acc: 0.9364 - val_loss: 0.2531 - val_acc: 0.9167\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 0.2709 - acc: 0.9398 - val_loss: 0.2400 - val_acc: 0.9176\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 2s 112us/step - loss: 0.2768 - acc: 0.9356 - val_loss: 0.2612 - val_acc: 0.9169\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 2s 103us/step - loss: 0.2830 - acc: 0.9333 - val_loss: 0.2295 - val_acc: 0.9264\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2665 - acc: 0.9392 - val_loss: 0.2440 - val_acc: 0.9187\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.2486 - acc: 0.9408 - val_loss: 0.2234 - val_acc: 0.9271\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2548 - acc: 0.9424 - val_loss: 0.2371 - val_acc: 0.9303\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.3111 - acc: 0.9323 - val_loss: 0.2435 - val_acc: 0.9285\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2493 - acc: 0.9432 - val_loss: 0.2566 - val_acc: 0.9196\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2631 - acc: 0.9444 - val_loss: 0.2564 - val_acc: 0.9158\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2743 - acc: 0.9376 - val_loss: 0.3410 - val_acc: 0.8905\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.2551 - acc: 0.9433 - val_loss: 0.2670 - val_acc: 0.9143\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2610 - acc: 0.9404 - val_loss: 0.2829 - val_acc: 0.9116\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2303 - acc: 0.9473 - val_loss: 0.2351 - val_acc: 0.9314\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2204 - acc: 0.9507 - val_loss: 0.2983 - val_acc: 0.9137\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2297 - acc: 0.9494 - val_loss: 0.3260 - val_acc: 0.8982\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2276 - acc: 0.9498 - val_loss: 0.2395 - val_acc: 0.9222\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2260 - acc: 0.9492 - val_loss: 0.2633 - val_acc: 0.9151\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2064 - acc: 0.9530 - val_loss: 0.2366 - val_acc: 0.9271\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2046 - acc: 0.9545 - val_loss: 0.2650 - val_acc: 0.9182\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2183 - acc: 0.9505 - val_loss: 0.3552 - val_acc: 0.8952\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2142 - acc: 0.9510 - val_loss: 0.2079 - val_acc: 0.9380\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2173 - acc: 0.9518 - val_loss: 0.3808 - val_acc: 0.8882\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2102 - acc: 0.9522 - val_loss: 0.2378 - val_acc: 0.9293\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1847 - acc: 0.9583 - val_loss: 0.2442 - val_acc: 0.9235\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1776 - acc: 0.9602 - val_loss: 0.2702 - val_acc: 0.9134\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1794 - acc: 0.9589 - val_loss: 0.2164 - val_acc: 0.9377\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1714 - acc: 0.9613 - val_loss: 0.2381 - val_acc: 0.9254\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2227 - acc: 0.9529 - val_loss: 0.2532 - val_acc: 0.9259\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2429 - acc: 0.9474 - val_loss: 0.4189 - val_acc: 0.8921\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2377 - acc: 0.9491 - val_loss: 0.2265 - val_acc: 0.9417\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1787 - acc: 0.9621 - val_loss: 0.2200 - val_acc: 0.9390\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.1935 - acc: 0.9582 - val_loss: 0.2154 - val_acc: 0.9367\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1631 - acc: 0.9646 - val_loss: 0.2435 - val_acc: 0.9256\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.1608 - acc: 0.9631 - val_loss: 0.2064 - val_acc: 0.9481\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1590 - acc: 0.9642 - val_loss: 0.2171 - val_acc: 0.9465\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1767 - acc: 0.9631 - val_loss: 0.1991 - val_acc: 0.9501\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.1508 - acc: 0.9670 - val_loss: 0.2184 - val_acc: 0.9396\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1492 - acc: 0.9670 - val_loss: 0.2671 - val_acc: 0.9240\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1523 - acc: 0.9653 - val_loss: 0.2431 - val_acc: 0.9306\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1431 - acc: 0.9675 - val_loss: 0.2130 - val_acc: 0.9459\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.1700 - acc: 0.9658 - val_loss: 0.3485 - val_acc: 0.9185\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.2366 - acc: 0.9512 - val_loss: 0.2177 - val_acc: 0.9494\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1644 - acc: 0.9644 - val_loss: 0.2343 - val_acc: 0.9337\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 0.1443 - acc: 0.9678 - val_loss: 0.2423 - val_acc: 0.9340\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1600 - acc: 0.9663 - val_loss: 0.1982 - val_acc: 0.9506\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 0.1743 - acc: 0.9658 - val_loss: 0.2761 - val_acc: 0.9174\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 2s 101us/step - loss: 0.1644 - acc: 0.9628 - val_loss: 0.3000 - val_acc: 0.9221\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 2s 125us/step - loss: 0.1833 - acc: 0.9603 - val_loss: 0.2270 - val_acc: 0.9430\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.1485 - acc: 0.9686 - val_loss: 0.2606 - val_acc: 0.9303\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 2s 108us/step - loss: 0.1305 - acc: 0.9709 - val_loss: 0.2172 - val_acc: 0.9469\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 2s 119us/step - loss: 0.1213 - acc: 0.9744 - val_loss: 0.2341 - val_acc: 0.9456\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1216 - acc: 0.9739 - val_loss: 0.2677 - val_acc: 0.9298\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.1380 - acc: 0.9703 - val_loss: 0.2258 - val_acc: 0.9436\n",
      "Saving model...\n",
      "Fitting model # 6 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 132us/step - loss: 1.2588 - acc: 0.5800 - val_loss: 0.5968 - val_acc: 0.6971\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 2s 97us/step - loss: 1.1092 - acc: 0.6900 - val_loss: 0.5504 - val_acc: 0.7444\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 2s 118us/step - loss: 1.0182 - acc: 0.7388 - val_loss: 0.5988 - val_acc: 0.6915\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 2s 101us/step - loss: 0.9442 - acc: 0.7609 - val_loss: 0.4914 - val_acc: 0.7828\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 2s 106us/step - loss: 0.8656 - acc: 0.7891 - val_loss: 0.4530 - val_acc: 0.8134\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 2s 103us/step - loss: 0.8066 - acc: 0.8110 - val_loss: 0.4362 - val_acc: 0.8143\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.7553 - acc: 0.8145 - val_loss: 0.4523 - val_acc: 0.8224\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.7098 - acc: 0.8330 - val_loss: 0.4513 - val_acc: 0.8014\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.6797 - acc: 0.8362 - val_loss: 0.3749 - val_acc: 0.8440\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.6515 - acc: 0.8410 - val_loss: 0.4373 - val_acc: 0.8118\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.6344 - acc: 0.8435 - val_loss: 0.3422 - val_acc: 0.8662\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 2s 111us/step - loss: 0.6334 - acc: 0.8459 - val_loss: 0.2975 - val_acc: 0.8831\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.5860 - acc: 0.8578 - val_loss: 0.3370 - val_acc: 0.8639\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 2s 114us/step - loss: 0.5749 - acc: 0.8591 - val_loss: 0.4060 - val_acc: 0.8312\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 2s 133us/step - loss: 0.5603 - acc: 0.8635 - val_loss: 0.3743 - val_acc: 0.8478\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 2s 118us/step - loss: 0.5317 - acc: 0.8665 - val_loss: 0.3207 - val_acc: 0.8684\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 2s 114us/step - loss: 0.5225 - acc: 0.8751 - val_loss: 0.2955 - val_acc: 0.8771\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 3s 161us/step - loss: 0.4967 - acc: 0.8798 - val_loss: 0.3104 - val_acc: 0.8699\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 2s 123us/step - loss: 0.5128 - acc: 0.8753 - val_loss: 0.2821 - val_acc: 0.8923\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 3s 152us/step - loss: 0.4902 - acc: 0.8839 - val_loss: 0.3287 - val_acc: 0.8623\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 3s 138us/step - loss: 0.4652 - acc: 0.8834 - val_loss: 0.2969 - val_acc: 0.8858\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 3s 144us/step - loss: 0.4745 - acc: 0.8853 - val_loss: 0.3000 - val_acc: 0.8800\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.4604 - acc: 0.8847 - val_loss: 0.3442 - val_acc: 0.8588\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 2s 115us/step - loss: 0.4361 - acc: 0.8933 - val_loss: 0.2784 - val_acc: 0.8897\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 2s 114us/step - loss: 0.4282 - acc: 0.8971 - val_loss: 0.2850 - val_acc: 0.8850\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 2s 101us/step - loss: 0.4179 - acc: 0.8964 - val_loss: 0.2919 - val_acc: 0.8948\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 2s 110us/step - loss: 0.4342 - acc: 0.9000 - val_loss: 0.3092 - val_acc: 0.8850\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 2s 99us/step - loss: 0.4140 - acc: 0.8992 - val_loss: 0.2943 - val_acc: 0.8878\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.3922 - acc: 0.9063 - val_loss: 0.2584 - val_acc: 0.9010\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.3951 - acc: 0.9040 - val_loss: 0.2995 - val_acc: 0.8853\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.3679 - acc: 0.9102 - val_loss: 0.2593 - val_acc: 0.8937\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3847 - acc: 0.9056 - val_loss: 0.2447 - val_acc: 0.9085\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3579 - acc: 0.9124 - val_loss: 0.2912 - val_acc: 0.8852\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 2s 101us/step - loss: 0.3400 - acc: 0.9174 - val_loss: 0.3184 - val_acc: 0.8707\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.3468 - acc: 0.9124 - val_loss: 0.2441 - val_acc: 0.9098\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.3259 - acc: 0.9204 - val_loss: 0.2487 - val_acc: 0.9048\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.3389 - acc: 0.9170 - val_loss: 0.2622 - val_acc: 0.9058\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.3476 - acc: 0.9182 - val_loss: 0.2573 - val_acc: 0.9081\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.3317 - acc: 0.9225 - val_loss: 0.2745 - val_acc: 0.8960\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 2s 131us/step - loss: 0.3487 - acc: 0.9178 - val_loss: 0.2489 - val_acc: 0.9071\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.3243 - acc: 0.9221 - val_loss: 0.2247 - val_acc: 0.9193\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 2s 99us/step - loss: 0.3196 - acc: 0.9218 - val_loss: 0.3425 - val_acc: 0.8807\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.3041 - acc: 0.9253 - val_loss: 0.2454 - val_acc: 0.9164\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.3087 - acc: 0.9284 - val_loss: 0.2866 - val_acc: 0.8910\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.2997 - acc: 0.9277 - val_loss: 0.2113 - val_acc: 0.9250\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 2s 125us/step - loss: 0.2699 - acc: 0.9345 - val_loss: 0.3028 - val_acc: 0.8870\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2654 - acc: 0.9340 - val_loss: 0.2351 - val_acc: 0.9124\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.2582 - acc: 0.9376 - val_loss: 0.2446 - val_acc: 0.9142\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2885 - acc: 0.9309 - val_loss: 0.2316 - val_acc: 0.9159\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.2658 - acc: 0.9382 - val_loss: 0.2447 - val_acc: 0.9130\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3308 - acc: 0.9283 - val_loss: 0.2109 - val_acc: 0.9283\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 2s 134us/step - loss: 0.2745 - acc: 0.9344 - val_loss: 0.2360 - val_acc: 0.9203\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 0.2424 - acc: 0.9413 - val_loss: 0.2117 - val_acc: 0.9262\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 2s 122us/step - loss: 0.2454 - acc: 0.9421 - val_loss: 0.2513 - val_acc: 0.9106\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 2s 131us/step - loss: 0.2293 - acc: 0.9456 - val_loss: 0.2888 - val_acc: 0.8940\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 2s 101us/step - loss: 0.2455 - acc: 0.9405 - val_loss: 0.1867 - val_acc: 0.9367\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 2s 103us/step - loss: 0.2329 - acc: 0.9458 - val_loss: 0.2772 - val_acc: 0.9014\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2309 - acc: 0.9441 - val_loss: 0.1699 - val_acc: 0.9456\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.2264 - acc: 0.9466 - val_loss: 0.2061 - val_acc: 0.9306\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.2284 - acc: 0.9472 - val_loss: 0.2458 - val_acc: 0.9156\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2265 - acc: 0.9467 - val_loss: 0.2422 - val_acc: 0.9240\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2140 - acc: 0.9490 - val_loss: 0.2572 - val_acc: 0.9103\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.1977 - acc: 0.9519 - val_loss: 0.1901 - val_acc: 0.9369\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1947 - acc: 0.9523 - val_loss: 0.2171 - val_acc: 0.9258\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2358 - acc: 0.9421 - val_loss: 0.2188 - val_acc: 0.9217\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1949 - acc: 0.9548 - val_loss: 0.2282 - val_acc: 0.9250\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.1931 - acc: 0.9545 - val_loss: 0.1958 - val_acc: 0.9343\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.1768 - acc: 0.9586 - val_loss: 0.2006 - val_acc: 0.9324\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2343 - acc: 0.9470 - val_loss: 0.2209 - val_acc: 0.9256\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1778 - acc: 0.9583 - val_loss: 0.2224 - val_acc: 0.9320\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1739 - acc: 0.9589 - val_loss: 0.1838 - val_acc: 0.9377\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2320 - acc: 0.9496 - val_loss: 0.3015 - val_acc: 0.8965\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.1914 - acc: 0.9549 - val_loss: 0.1874 - val_acc: 0.9415\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.1921 - acc: 0.9550 - val_loss: 0.2082 - val_acc: 0.9345\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1671 - acc: 0.9605 - val_loss: 0.1760 - val_acc: 0.9424\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.1688 - acc: 0.9607 - val_loss: 0.2189 - val_acc: 0.9287\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.1910 - acc: 0.9566 - val_loss: 0.1781 - val_acc: 0.9436\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1632 - acc: 0.9632 - val_loss: 0.1842 - val_acc: 0.9446\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1874 - acc: 0.9549 - val_loss: 0.1891 - val_acc: 0.9407\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1582 - acc: 0.9623 - val_loss: 0.1703 - val_acc: 0.9499\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1487 - acc: 0.9669 - val_loss: 0.2351 - val_acc: 0.9359\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1558 - acc: 0.9642 - val_loss: 0.1871 - val_acc: 0.9420\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1577 - acc: 0.9635 - val_loss: 0.1719 - val_acc: 0.9467\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1453 - acc: 0.9654 - val_loss: 0.2238 - val_acc: 0.9436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1477 - acc: 0.9651 - val_loss: 0.1684 - val_acc: 0.9480\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1294 - acc: 0.9702 - val_loss: 0.1817 - val_acc: 0.9440\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1558 - acc: 0.9621 - val_loss: 0.1803 - val_acc: 0.9432\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1460 - acc: 0.9668 - val_loss: 0.1626 - val_acc: 0.9490\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1939 - acc: 0.9573 - val_loss: 0.2117 - val_acc: 0.9395\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1517 - acc: 0.9645 - val_loss: 0.1885 - val_acc: 0.9432\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1293 - acc: 0.9715 - val_loss: 0.1703 - val_acc: 0.9519\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1259 - acc: 0.9706 - val_loss: 0.2291 - val_acc: 0.9238\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1376 - acc: 0.9684 - val_loss: 0.2047 - val_acc: 0.9356\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1776 - acc: 0.9582 - val_loss: 0.2475 - val_acc: 0.9219\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1533 - acc: 0.9661 - val_loss: 0.1894 - val_acc: 0.9467\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1166 - acc: 0.9752 - val_loss: 0.2035 - val_acc: 0.9351\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1163 - acc: 0.9729 - val_loss: 0.1716 - val_acc: 0.9459\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1061 - acc: 0.9758 - val_loss: 0.1728 - val_acc: 0.9504\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1069 - acc: 0.9766 - val_loss: 0.1875 - val_acc: 0.9454\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1176 - acc: 0.9731 - val_loss: 0.2055 - val_acc: 0.9441\n",
      "Saving model...\n",
      "Fitting model # 7 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 108us/step - loss: 1.2772 - acc: 0.6151 - val_loss: 0.6264 - val_acc: 0.6752\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 1.1096 - acc: 0.6976 - val_loss: 0.6997 - val_acc: 0.6061\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 1.0495 - acc: 0.7240 - val_loss: 0.6099 - val_acc: 0.6945\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 1.0085 - acc: 0.7407 - val_loss: 0.5024 - val_acc: 0.7794\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.9505 - acc: 0.7597 - val_loss: 0.5253 - val_acc: 0.7649\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.9259 - acc: 0.7686 - val_loss: 0.4749 - val_acc: 0.8060\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.8895 - acc: 0.7843 - val_loss: 0.4581 - val_acc: 0.7924\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.8518 - acc: 0.8013 - val_loss: 0.4022 - val_acc: 0.8461\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.7958 - acc: 0.8196 - val_loss: 0.5514 - val_acc: 0.7420\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.7873 - acc: 0.8177 - val_loss: 0.4227 - val_acc: 0.8419\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.7651 - acc: 0.8365 - val_loss: 0.4314 - val_acc: 0.8240\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.6998 - acc: 0.8444 - val_loss: 0.4041 - val_acc: 0.8262\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.6776 - acc: 0.8457 - val_loss: 0.3189 - val_acc: 0.8815\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.6401 - acc: 0.8622 - val_loss: 0.3572 - val_acc: 0.8519\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.6216 - acc: 0.8636 - val_loss: 0.3364 - val_acc: 0.8638\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.5932 - acc: 0.8719 - val_loss: 0.3307 - val_acc: 0.8647\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.5656 - acc: 0.8737 - val_loss: 0.2883 - val_acc: 0.8847\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.6059 - acc: 0.8702 - val_loss: 0.3224 - val_acc: 0.8808\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.5495 - acc: 0.8819 - val_loss: 0.3039 - val_acc: 0.8763\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.5237 - acc: 0.8836 - val_loss: 0.2967 - val_acc: 0.8783\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5172 - acc: 0.8857 - val_loss: 0.3878 - val_acc: 0.8459\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.5097 - acc: 0.8898 - val_loss: 0.2965 - val_acc: 0.8742\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4894 - acc: 0.8927 - val_loss: 0.2832 - val_acc: 0.8860\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.4586 - acc: 0.9002 - val_loss: 0.4513 - val_acc: 0.8454\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 2s 97us/step - loss: 0.4673 - acc: 0.8955 - val_loss: 0.2848 - val_acc: 0.8855\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.4761 - acc: 0.8975 - val_loss: 0.3561 - val_acc: 0.8538\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4631 - acc: 0.8978 - val_loss: 0.3474 - val_acc: 0.8692\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.4380 - acc: 0.9036 - val_loss: 0.3396 - val_acc: 0.8778\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4243 - acc: 0.9095 - val_loss: 0.3312 - val_acc: 0.8683\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.4106 - acc: 0.9100 - val_loss: 0.2871 - val_acc: 0.8852\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.4266 - acc: 0.9073 - val_loss: 0.2703 - val_acc: 0.8989\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3993 - acc: 0.9125 - val_loss: 0.2612 - val_acc: 0.8969\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.3858 - acc: 0.9171 - val_loss: 0.2848 - val_acc: 0.8882\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3775 - acc: 0.9156 - val_loss: 0.2664 - val_acc: 0.8976\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.3792 - acc: 0.9183 - val_loss: 0.2594 - val_acc: 0.9031\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3716 - acc: 0.9180 - val_loss: 0.3032 - val_acc: 0.8771\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 2s 116us/step - loss: 0.3633 - acc: 0.9200 - val_loss: 0.2686 - val_acc: 0.9021\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3591 - acc: 0.9206 - val_loss: 0.2236 - val_acc: 0.9211\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.3618 - acc: 0.9260 - val_loss: 0.2479 - val_acc: 0.9087\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3600 - acc: 0.9244 - val_loss: 0.2676 - val_acc: 0.9000\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3441 - acc: 0.9267 - val_loss: 0.2498 - val_acc: 0.9106\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3254 - acc: 0.9295 - val_loss: 0.2364 - val_acc: 0.9143\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.3167 - acc: 0.9318 - val_loss: 0.2678 - val_acc: 0.9035\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 81us/step - loss: 0.3419 - acc: 0.9248 - val_loss: 0.2578 - val_acc: 0.9069\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3662 - acc: 0.9222 - val_loss: 0.2417 - val_acc: 0.9113\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3433 - acc: 0.9257 - val_loss: 0.2322 - val_acc: 0.9182\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.3450 - acc: 0.9296 - val_loss: 0.2535 - val_acc: 0.9098\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3168 - acc: 0.9326 - val_loss: 0.2682 - val_acc: 0.9021\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3028 - acc: 0.9354 - val_loss: 0.3360 - val_acc: 0.8897\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.3189 - acc: 0.9333 - val_loss: 0.2626 - val_acc: 0.9053\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.2791 - acc: 0.9407 - val_loss: 0.2256 - val_acc: 0.9190\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2682 - acc: 0.9395 - val_loss: 0.2149 - val_acc: 0.9275\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2754 - acc: 0.9418 - val_loss: 0.2359 - val_acc: 0.9214\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2732 - acc: 0.9405 - val_loss: 0.2113 - val_acc: 0.9285\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2874 - acc: 0.9385 - val_loss: 0.1980 - val_acc: 0.9380\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2656 - acc: 0.9448 - val_loss: 0.3042 - val_acc: 0.9003\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2674 - acc: 0.9458 - val_loss: 0.2348 - val_acc: 0.9193\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2516 - acc: 0.9470 - val_loss: 0.2529 - val_acc: 0.9100\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2383 - acc: 0.9499 - val_loss: 0.2160 - val_acc: 0.9251\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2256 - acc: 0.9521 - val_loss: 0.2108 - val_acc: 0.9327\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.2402 - acc: 0.9507 - val_loss: 0.2228 - val_acc: 0.9266\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 81us/step - loss: 0.2844 - acc: 0.9403 - val_loss: 0.2114 - val_acc: 0.9314\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2637 - acc: 0.9469 - val_loss: 0.3975 - val_acc: 0.8729\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2547 - acc: 0.9477 - val_loss: 0.2317 - val_acc: 0.9222\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2252 - acc: 0.9520 - val_loss: 0.2031 - val_acc: 0.9324\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2190 - acc: 0.9543 - val_loss: 0.2415 - val_acc: 0.9176\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2230 - acc: 0.9552 - val_loss: 0.2438 - val_acc: 0.9200\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2643 - acc: 0.9482 - val_loss: 0.2203 - val_acc: 0.9291\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2713 - acc: 0.9460 - val_loss: 0.2703 - val_acc: 0.9071\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.2613 - acc: 0.9479 - val_loss: 0.2486 - val_acc: 0.9230\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2210 - acc: 0.9539 - val_loss: 0.2068 - val_acc: 0.9366\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.1972 - acc: 0.9598 - val_loss: 0.2397 - val_acc: 0.9192\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2086 - acc: 0.9571 - val_loss: 0.2366 - val_acc: 0.9211\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2135 - acc: 0.9547 - val_loss: 0.2029 - val_acc: 0.9338\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.2004 - acc: 0.9588 - val_loss: 0.2153 - val_acc: 0.9325\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 81us/step - loss: 0.2002 - acc: 0.9587 - val_loss: 0.1908 - val_acc: 0.9412\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1928 - acc: 0.9602 - val_loss: 0.1974 - val_acc: 0.9382\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.1839 - acc: 0.9633 - val_loss: 0.1989 - val_acc: 0.9383\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.1912 - acc: 0.9617 - val_loss: 0.1966 - val_acc: 0.9427\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.1804 - acc: 0.9635 - val_loss: 0.2349 - val_acc: 0.9221\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1719 - acc: 0.9655 - val_loss: 0.2006 - val_acc: 0.9374\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1781 - acc: 0.9636 - val_loss: 0.2255 - val_acc: 0.9271\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1685 - acc: 0.9665 - val_loss: 0.1934 - val_acc: 0.9427\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.1673 - acc: 0.9671 - val_loss: 0.1791 - val_acc: 0.9498\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1700 - acc: 0.9673 - val_loss: 0.2498 - val_acc: 0.9235\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1744 - acc: 0.9647 - val_loss: 0.2202 - val_acc: 0.9401\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2625 - acc: 0.9516 - val_loss: 0.2462 - val_acc: 0.9393\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2809 - acc: 0.9483 - val_loss: 0.5189 - val_acc: 0.8660\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.3027 - acc: 0.9454 - val_loss: 0.2719 - val_acc: 0.9190\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.1972 - acc: 0.9610 - val_loss: 0.2077 - val_acc: 0.9393\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2013 - acc: 0.9615 - val_loss: 0.2442 - val_acc: 0.9301\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1714 - acc: 0.9669 - val_loss: 0.2143 - val_acc: 0.9382\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.1533 - acc: 0.9702 - val_loss: 0.1844 - val_acc: 0.9477\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1530 - acc: 0.9709 - val_loss: 0.1935 - val_acc: 0.9522\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1534 - acc: 0.9706 - val_loss: 0.1809 - val_acc: 0.9530\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.1569 - acc: 0.9696 - val_loss: 0.2202 - val_acc: 0.9415\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1499 - acc: 0.9725 - val_loss: 0.1992 - val_acc: 0.9420\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 2s 102us/step - loss: 0.1361 - acc: 0.9756 - val_loss: 0.1997 - val_acc: 0.9396\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.1352 - acc: 0.9753 - val_loss: 0.2144 - val_acc: 0.9333\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.1370 - acc: 0.9750 - val_loss: 0.1904 - val_acc: 0.9459\n",
      "Saving model...\n",
      "Fitting model # 8 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 2s 116us/step - loss: 1.2751 - acc: 0.5747 - val_loss: 0.6669 - val_acc: 0.6179\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 1.1450 - acc: 0.6599 - val_loss: 0.6605 - val_acc: 0.6370\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 1.0756 - acc: 0.6894 - val_loss: 0.5607 - val_acc: 0.7113\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 1.0317 - acc: 0.7151 - val_loss: 0.5812 - val_acc: 0.7097\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.9875 - acc: 0.7407 - val_loss: 0.5121 - val_acc: 0.7636\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.9330 - acc: 0.7628 - val_loss: 0.4829 - val_acc: 0.7779\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.9246 - acc: 0.7691 - val_loss: 0.4590 - val_acc: 0.8052\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.8622 - acc: 0.7920 - val_loss: 0.5729 - val_acc: 0.7554\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.8207 - acc: 0.8111 - val_loss: 0.4748 - val_acc: 0.8047\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.7860 - acc: 0.8190 - val_loss: 0.4088 - val_acc: 0.8343\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.7489 - acc: 0.8309 - val_loss: 0.4646 - val_acc: 0.8245\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.7348 - acc: 0.8342 - val_loss: 0.3750 - val_acc: 0.8538\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.6935 - acc: 0.8445 - val_loss: 0.4512 - val_acc: 0.8151\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.6582 - acc: 0.8562 - val_loss: 0.3567 - val_acc: 0.8605\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.6407 - acc: 0.8578 - val_loss: 0.4217 - val_acc: 0.8329\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.6139 - acc: 0.8651 - val_loss: 0.3728 - val_acc: 0.8525\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.5833 - acc: 0.8709 - val_loss: 0.3675 - val_acc: 0.8488\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.5717 - acc: 0.8740 - val_loss: 0.3465 - val_acc: 0.8596\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.5589 - acc: 0.8765 - val_loss: 0.3210 - val_acc: 0.8747\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.5583 - acc: 0.8784 - val_loss: 0.3395 - val_acc: 0.8662\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.5431 - acc: 0.8797 - val_loss: 0.3305 - val_acc: 0.8768\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5121 - acc: 0.8859 - val_loss: 0.3610 - val_acc: 0.8504\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.5385 - acc: 0.8834 - val_loss: 0.4124 - val_acc: 0.8319\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.5021 - acc: 0.8906 - val_loss: 0.4683 - val_acc: 0.8190\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4904 - acc: 0.8948 - val_loss: 0.3006 - val_acc: 0.8834\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4667 - acc: 0.8985 - val_loss: 0.3095 - val_acc: 0.8820\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4597 - acc: 0.8968 - val_loss: 0.2926 - val_acc: 0.8892\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4545 - acc: 0.9019 - val_loss: 0.3206 - val_acc: 0.8773\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.4733 - acc: 0.8972 - val_loss: 0.3127 - val_acc: 0.8771\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4324 - acc: 0.9058 - val_loss: 0.2624 - val_acc: 0.9006\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4341 - acc: 0.9059 - val_loss: 0.2928 - val_acc: 0.8870\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4112 - acc: 0.9087 - val_loss: 0.3214 - val_acc: 0.8894\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4116 - acc: 0.9077 - val_loss: 0.2352 - val_acc: 0.9105\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.4113 - acc: 0.9125 - val_loss: 0.2928 - val_acc: 0.8868\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4142 - acc: 0.9122 - val_loss: 0.2472 - val_acc: 0.9126\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.4034 - acc: 0.9123 - val_loss: 0.3299 - val_acc: 0.8810\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3771 - acc: 0.9186 - val_loss: 0.2644 - val_acc: 0.8994\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3628 - acc: 0.9209 - val_loss: 0.2960 - val_acc: 0.8918\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3610 - acc: 0.9226 - val_loss: 0.2634 - val_acc: 0.8971\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3465 - acc: 0.9249 - val_loss: 0.2495 - val_acc: 0.9063\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3523 - acc: 0.9221 - val_loss: 0.2612 - val_acc: 0.9027\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3699 - acc: 0.9234 - val_loss: 0.3030 - val_acc: 0.8934\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3917 - acc: 0.9180 - val_loss: 0.2909 - val_acc: 0.8974\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4066 - acc: 0.9144 - val_loss: 0.4082 - val_acc: 0.8633\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3594 - acc: 0.9246 - val_loss: 0.2380 - val_acc: 0.9132\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.3345 - acc: 0.9291 - val_loss: 0.2303 - val_acc: 0.9156\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3128 - acc: 0.9320 - val_loss: 0.2414 - val_acc: 0.9103\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3142 - acc: 0.9335 - val_loss: 0.2021 - val_acc: 0.9246\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.3304 - acc: 0.9303 - val_loss: 0.2903 - val_acc: 0.8857\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3151 - acc: 0.9314 - val_loss: 0.2670 - val_acc: 0.8997\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2927 - acc: 0.9358 - val_loss: 0.1956 - val_acc: 0.9322\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2837 - acc: 0.9403 - val_loss: 0.2057 - val_acc: 0.9245\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2933 - acc: 0.9374 - val_loss: 0.2553 - val_acc: 0.9040\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2864 - acc: 0.9392 - val_loss: 0.2729 - val_acc: 0.8994\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.3357 - acc: 0.9340 - val_loss: 0.2577 - val_acc: 0.9138\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3277 - acc: 0.9305 - val_loss: 0.2831 - val_acc: 0.8990\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3125 - acc: 0.9352 - val_loss: 0.2921 - val_acc: 0.8952\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2789 - acc: 0.9394 - val_loss: 0.1962 - val_acc: 0.9329\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2576 - acc: 0.9466 - val_loss: 0.2243 - val_acc: 0.9179\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2668 - acc: 0.9416 - val_loss: 0.2058 - val_acc: 0.9290\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2686 - acc: 0.9438 - val_loss: 0.1879 - val_acc: 0.9356\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2667 - acc: 0.9437 - val_loss: 0.2025 - val_acc: 0.9325\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2498 - acc: 0.9484 - val_loss: 0.1855 - val_acc: 0.9354\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2690 - acc: 0.9427 - val_loss: 0.2138 - val_acc: 0.9242\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2458 - acc: 0.9482 - val_loss: 0.1712 - val_acc: 0.9417\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2329 - acc: 0.9528 - val_loss: 0.2322 - val_acc: 0.9135\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2658 - acc: 0.9452 - val_loss: 0.2532 - val_acc: 0.9179\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2451 - acc: 0.9486 - val_loss: 0.1922 - val_acc: 0.9327\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2413 - acc: 0.9505 - val_loss: 0.2084 - val_acc: 0.9329\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2387 - acc: 0.9514 - val_loss: 0.2939 - val_acc: 0.9187\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3122 - acc: 0.9374 - val_loss: 0.2052 - val_acc: 0.9337\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2875 - acc: 0.9432 - val_loss: 0.3063 - val_acc: 0.9087\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2840 - acc: 0.9431 - val_loss: 0.2199 - val_acc: 0.9348\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2346 - acc: 0.9527 - val_loss: 0.2064 - val_acc: 0.9343\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2256 - acc: 0.9546 - val_loss: 0.2124 - val_acc: 0.9283\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2330 - acc: 0.9537 - val_loss: 0.2016 - val_acc: 0.9349\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2166 - acc: 0.9566 - val_loss: 0.2156 - val_acc: 0.9235\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2017 - acc: 0.9581 - val_loss: 0.1728 - val_acc: 0.9425\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1904 - acc: 0.9630 - val_loss: 0.2022 - val_acc: 0.9317\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2024 - acc: 0.9592 - val_loss: 0.2037 - val_acc: 0.9333\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.1918 - acc: 0.9617 - val_loss: 0.1913 - val_acc: 0.9362\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2083 - acc: 0.9590 - val_loss: 0.1846 - val_acc: 0.9448\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2057 - acc: 0.9587 - val_loss: 0.2932 - val_acc: 0.9100\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2082 - acc: 0.9598 - val_loss: 0.2108 - val_acc: 0.9303\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2188 - acc: 0.9559 - val_loss: 0.2052 - val_acc: 0.9340\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2084 - acc: 0.9586 - val_loss: 0.2062 - val_acc: 0.9304\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1805 - acc: 0.9645 - val_loss: 0.1732 - val_acc: 0.9422\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1793 - acc: 0.9658 - val_loss: 0.1816 - val_acc: 0.9446\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.1878 - acc: 0.9623 - val_loss: 0.1768 - val_acc: 0.9470\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2470 - acc: 0.9535 - val_loss: 0.2176 - val_acc: 0.9312\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2711 - acc: 0.9482 - val_loss: 0.1949 - val_acc: 0.9428\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2221 - acc: 0.9565 - val_loss: 0.1698 - val_acc: 0.9510\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1887 - acc: 0.9628 - val_loss: 0.1923 - val_acc: 0.9395\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1677 - acc: 0.9674 - val_loss: 0.1782 - val_acc: 0.9433\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1615 - acc: 0.9697 - val_loss: 0.1964 - val_acc: 0.9372\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 2s 103us/step - loss: 0.1549 - acc: 0.9705 - val_loss: 0.1820 - val_acc: 0.9422\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.1626 - acc: 0.9694 - val_loss: 0.2012 - val_acc: 0.9348\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.1514 - acc: 0.9699 - val_loss: 0.1585 - val_acc: 0.9502\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1745 - acc: 0.9679 - val_loss: 0.2264 - val_acc: 0.9354\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1920 - acc: 0.9629 - val_loss: 0.2816 - val_acc: 0.9118\n",
      "Saving model...\n",
      "Fitting model # 9 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 116us/step - loss: 1.2273 - acc: 0.6039 - val_loss: 0.5681 - val_acc: 0.7377\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 1.0803 - acc: 0.6944 - val_loss: 0.4879 - val_acc: 0.7871\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.9978 - acc: 0.7412 - val_loss: 0.5523 - val_acc: 0.7475\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.9121 - acc: 0.7814 - val_loss: 0.4863 - val_acc: 0.7952\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.8394 - acc: 0.8009 - val_loss: 0.4404 - val_acc: 0.8145\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.7863 - acc: 0.8167 - val_loss: 0.4599 - val_acc: 0.8060\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 2s 97us/step - loss: 0.7615 - acc: 0.8242 - val_loss: 0.5996 - val_acc: 0.7490\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.7185 - acc: 0.8305 - val_loss: 0.4561 - val_acc: 0.8106\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.6832 - acc: 0.8378 - val_loss: 0.4285 - val_acc: 0.8188\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.6525 - acc: 0.8449 - val_loss: 0.3623 - val_acc: 0.8457\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.6249 - acc: 0.8540 - val_loss: 0.4461 - val_acc: 0.8148\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.6030 - acc: 0.8566 - val_loss: 0.3702 - val_acc: 0.8337\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.5755 - acc: 0.8604 - val_loss: 0.3280 - val_acc: 0.8643\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 2s 94us/step - loss: 0.5569 - acc: 0.8645 - val_loss: 0.3473 - val_acc: 0.8618\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.5464 - acc: 0.8735 - val_loss: 0.4362 - val_acc: 0.8214\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.5582 - acc: 0.8624 - val_loss: 0.3484 - val_acc: 0.8731\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.5178 - acc: 0.8791 - val_loss: 0.3421 - val_acc: 0.8510\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.4938 - acc: 0.8785 - val_loss: 0.3098 - val_acc: 0.8739\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.4772 - acc: 0.8863 - val_loss: 0.3006 - val_acc: 0.8810\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4644 - acc: 0.8858 - val_loss: 0.3064 - val_acc: 0.8710\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4539 - acc: 0.8898 - val_loss: 0.3185 - val_acc: 0.8681\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 2s 87us/step - loss: 0.4496 - acc: 0.8933 - val_loss: 0.2685 - val_acc: 0.8921\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4291 - acc: 0.8945 - val_loss: 0.2764 - val_acc: 0.8839\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.4481 - acc: 0.8956 - val_loss: 0.3481 - val_acc: 0.8552\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4292 - acc: 0.8940 - val_loss: 0.2358 - val_acc: 0.9068\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.4065 - acc: 0.9014 - val_loss: 0.2393 - val_acc: 0.9089\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3952 - acc: 0.9029 - val_loss: 0.2734 - val_acc: 0.8899\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3923 - acc: 0.9037 - val_loss: 0.2706 - val_acc: 0.8913\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3810 - acc: 0.9115 - val_loss: 0.2801 - val_acc: 0.8911\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3791 - acc: 0.9082 - val_loss: 0.2552 - val_acc: 0.9034\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3608 - acc: 0.9145 - val_loss: 0.2409 - val_acc: 0.9084\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3489 - acc: 0.9186 - val_loss: 0.2708 - val_acc: 0.8936\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3477 - acc: 0.9145 - val_loss: 0.2446 - val_acc: 0.9063\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3302 - acc: 0.9216 - val_loss: 0.2633 - val_acc: 0.9043\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3409 - acc: 0.9206 - val_loss: 0.2536 - val_acc: 0.9058\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.3334 - acc: 0.9221 - val_loss: 0.2785 - val_acc: 0.8890\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3114 - acc: 0.9246 - val_loss: 0.2215 - val_acc: 0.9153\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3164 - acc: 0.9247 - val_loss: 0.2291 - val_acc: 0.9159\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3094 - acc: 0.9272 - val_loss: 0.2064 - val_acc: 0.9248\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2870 - acc: 0.9316 - val_loss: 0.1887 - val_acc: 0.9324\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2895 - acc: 0.9310 - val_loss: 0.1819 - val_acc: 0.9346\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3262 - acc: 0.9269 - val_loss: 0.2870 - val_acc: 0.8887\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2832 - acc: 0.9331 - val_loss: 0.2601 - val_acc: 0.9066\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2863 - acc: 0.9340 - val_loss: 0.2141 - val_acc: 0.9225\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2681 - acc: 0.9364 - val_loss: 0.2070 - val_acc: 0.9230\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2638 - acc: 0.9412 - val_loss: 0.2713 - val_acc: 0.8928\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2531 - acc: 0.9397 - val_loss: 0.2083 - val_acc: 0.9258\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2766 - acc: 0.9332 - val_loss: 0.2605 - val_acc: 0.9151\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2585 - acc: 0.9398 - val_loss: 0.2297 - val_acc: 0.9217\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2536 - acc: 0.9410 - val_loss: 0.2219 - val_acc: 0.9161\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2548 - acc: 0.9406 - val_loss: 0.2239 - val_acc: 0.9187\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2453 - acc: 0.9426 - val_loss: 0.2107 - val_acc: 0.9279\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2360 - acc: 0.9452 - val_loss: 0.2305 - val_acc: 0.9122\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2137 - acc: 0.9496 - val_loss: 0.1729 - val_acc: 0.9404\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2140 - acc: 0.9508 - val_loss: 0.4663 - val_acc: 0.8812\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2876 - acc: 0.9382 - val_loss: 0.2085 - val_acc: 0.9317\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2239 - acc: 0.9498 - val_loss: 0.1882 - val_acc: 0.9291\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1995 - acc: 0.9549 - val_loss: 0.2104 - val_acc: 0.9238\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1932 - acc: 0.9568 - val_loss: 0.2074 - val_acc: 0.9245\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.1966 - acc: 0.9549 - val_loss: 0.2127 - val_acc: 0.9296\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2010 - acc: 0.9543 - val_loss: 0.1894 - val_acc: 0.9324\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1939 - acc: 0.9566 - val_loss: 0.2021 - val_acc: 0.9312\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1953 - acc: 0.9552 - val_loss: 0.2176 - val_acc: 0.9280\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1900 - acc: 0.9558 - val_loss: 0.1942 - val_acc: 0.9343\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1813 - acc: 0.9584 - val_loss: 0.2073 - val_acc: 0.9300\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1795 - acc: 0.9604 - val_loss: 0.2027 - val_acc: 0.9251\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1860 - acc: 0.9571 - val_loss: 0.2017 - val_acc: 0.9275\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1744 - acc: 0.9610 - val_loss: 0.2306 - val_acc: 0.9222\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1737 - acc: 0.9616 - val_loss: 0.1990 - val_acc: 0.9320\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1594 - acc: 0.9647 - val_loss: 0.1997 - val_acc: 0.9322\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1640 - acc: 0.9616 - val_loss: 0.1850 - val_acc: 0.9380\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1681 - acc: 0.9645 - val_loss: 0.2493 - val_acc: 0.9225\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1944 - acc: 0.9571 - val_loss: 0.2206 - val_acc: 0.9256\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.1625 - acc: 0.9624 - val_loss: 0.1894 - val_acc: 0.9388\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1522 - acc: 0.9657 - val_loss: 0.1595 - val_acc: 0.9512\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2097 - acc: 0.9569 - val_loss: 0.3500 - val_acc: 0.9221\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2917 - acc: 0.9455 - val_loss: 0.2500 - val_acc: 0.9214\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.1650 - acc: 0.9640 - val_loss: 0.1906 - val_acc: 0.9377\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1493 - acc: 0.9659 - val_loss: 0.1714 - val_acc: 0.9506\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1433 - acc: 0.9686 - val_loss: 0.1741 - val_acc: 0.9469\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1555 - acc: 0.9659 - val_loss: 0.1859 - val_acc: 0.9449\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1728 - acc: 0.9640 - val_loss: 0.2522 - val_acc: 0.9127\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1526 - acc: 0.9656 - val_loss: 0.2010 - val_acc: 0.9357\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1341 - acc: 0.9697 - val_loss: 0.1777 - val_acc: 0.9424\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1227 - acc: 0.9740 - val_loss: 0.1848 - val_acc: 0.9377\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1218 - acc: 0.9742 - val_loss: 0.3773 - val_acc: 0.9132\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1491 - acc: 0.9709 - val_loss: 0.2014 - val_acc: 0.9382\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1554 - acc: 0.9657 - val_loss: 0.2196 - val_acc: 0.9337\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1245 - acc: 0.9730 - val_loss: 0.1913 - val_acc: 0.9378\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1121 - acc: 0.9753 - val_loss: 0.1756 - val_acc: 0.9424\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1084 - acc: 0.9771 - val_loss: 0.2016 - val_acc: 0.9338\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1537 - acc: 0.9669 - val_loss: 0.1853 - val_acc: 0.9438\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1363 - acc: 0.9703 - val_loss: 0.2450 - val_acc: 0.9219\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1248 - acc: 0.9728 - val_loss: 0.1816 - val_acc: 0.9483\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1134 - acc: 0.9755 - val_loss: 0.2831 - val_acc: 0.9217\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1127 - acc: 0.9760 - val_loss: 0.2075 - val_acc: 0.9369\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1037 - acc: 0.9784 - val_loss: 0.2034 - val_acc: 0.9403\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1122 - acc: 0.9771 - val_loss: 0.2178 - val_acc: 0.9332\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1067 - acc: 0.9772 - val_loss: 0.1833 - val_acc: 0.9488\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.1640 - acc: 0.9673 - val_loss: 0.2447 - val_acc: 0.9367\n",
      "Saving model...\n",
      "Fitting model # 10 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 129us/step - loss: 1.2599 - acc: 0.5885 - val_loss: 0.6234 - val_acc: 0.6519\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 1.1318 - acc: 0.6677 - val_loss: 0.6104 - val_acc: 0.6599\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 1.0514 - acc: 0.7053 - val_loss: 0.5287 - val_acc: 0.7364\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.9994 - acc: 0.7334 - val_loss: 0.4362 - val_acc: 0.8114\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.9669 - acc: 0.7462 - val_loss: 0.5343 - val_acc: 0.7308\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.9177 - acc: 0.7603 - val_loss: 0.4983 - val_acc: 0.7810\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.8698 - acc: 0.7841 - val_loss: 0.4966 - val_acc: 0.7818\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.8226 - acc: 0.8036 - val_loss: 0.5184 - val_acc: 0.7728\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.7745 - acc: 0.8196 - val_loss: 0.4039 - val_acc: 0.8357\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 0.7422 - acc: 0.8284 - val_loss: 0.3368 - val_acc: 0.8728\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.6935 - acc: 0.8417 - val_loss: 0.4144 - val_acc: 0.8153\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.6447 - acc: 0.8497 - val_loss: 0.3823 - val_acc: 0.8459\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.6475 - acc: 0.8503 - val_loss: 0.3640 - val_acc: 0.8601\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.6117 - acc: 0.8647 - val_loss: 0.3937 - val_acc: 0.8325\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5828 - acc: 0.8648 - val_loss: 0.4052 - val_acc: 0.8258\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5680 - acc: 0.8719 - val_loss: 0.3477 - val_acc: 0.8556\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.5593 - acc: 0.8698 - val_loss: 0.2893 - val_acc: 0.8858\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5259 - acc: 0.8767 - val_loss: 0.3580 - val_acc: 0.8610\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.5200 - acc: 0.8802 - val_loss: 0.3254 - val_acc: 0.8709\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.5031 - acc: 0.8871 - val_loss: 0.3303 - val_acc: 0.8681\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4776 - acc: 0.8867 - val_loss: 0.3055 - val_acc: 0.8829\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4749 - acc: 0.8928 - val_loss: 0.2208 - val_acc: 0.9230\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4668 - acc: 0.8961 - val_loss: 0.3781 - val_acc: 0.8491\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.4576 - acc: 0.8946 - val_loss: 0.3578 - val_acc: 0.8548\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4409 - acc: 0.8998 - val_loss: 0.3540 - val_acc: 0.8519\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4208 - acc: 0.9037 - val_loss: 0.2891 - val_acc: 0.8913\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.4452 - acc: 0.8985 - val_loss: 0.2613 - val_acc: 0.8958\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4104 - acc: 0.9083 - val_loss: 0.2849 - val_acc: 0.8826\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3938 - acc: 0.9130 - val_loss: 0.2731 - val_acc: 0.8940\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3771 - acc: 0.9175 - val_loss: 0.2726 - val_acc: 0.8924\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3765 - acc: 0.9148 - val_loss: 0.3028 - val_acc: 0.8821\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3751 - acc: 0.9179 - val_loss: 0.2651 - val_acc: 0.9003\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3655 - acc: 0.9190 - val_loss: 0.2443 - val_acc: 0.9077\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3592 - acc: 0.9196 - val_loss: 0.2769 - val_acc: 0.8899\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3988 - acc: 0.9117 - val_loss: 0.2740 - val_acc: 0.9039\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3603 - acc: 0.9213 - val_loss: 0.2559 - val_acc: 0.9043\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.3387 - acc: 0.9273 - val_loss: 0.2632 - val_acc: 0.8998\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.3390 - acc: 0.9247 - val_loss: 0.2826 - val_acc: 0.8876\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3310 - acc: 0.9238 - val_loss: 0.1857 - val_acc: 0.9383\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 2s 91us/step - loss: 0.3581 - acc: 0.9239 - val_loss: 0.2186 - val_acc: 0.9180\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.3070 - acc: 0.9330 - val_loss: 0.2375 - val_acc: 0.9101\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3006 - acc: 0.9337 - val_loss: 0.2346 - val_acc: 0.9147\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.3111 - acc: 0.9310 - val_loss: 0.2200 - val_acc: 0.9169\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2966 - acc: 0.9360 - val_loss: 0.2605 - val_acc: 0.9018\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2899 - acc: 0.9364 - val_loss: 0.2446 - val_acc: 0.9098\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2847 - acc: 0.9359 - val_loss: 0.2011 - val_acc: 0.9338\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2707 - acc: 0.9441 - val_loss: 0.2608 - val_acc: 0.9047\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2886 - acc: 0.9384 - val_loss: 0.2458 - val_acc: 0.9114\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2613 - acc: 0.9442 - val_loss: 0.2305 - val_acc: 0.9192\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2715 - acc: 0.9424 - val_loss: 0.2536 - val_acc: 0.9085\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2734 - acc: 0.9403 - val_loss: 0.2276 - val_acc: 0.9279\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2758 - acc: 0.9415 - val_loss: 0.3119 - val_acc: 0.8889\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2662 - acc: 0.9407 - val_loss: 0.2041 - val_acc: 0.9320\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2652 - acc: 0.9425 - val_loss: 0.2107 - val_acc: 0.9293\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2678 - acc: 0.9470 - val_loss: 0.2838 - val_acc: 0.9019\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3932 - acc: 0.9256 - val_loss: 0.2689 - val_acc: 0.9177\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2678 - acc: 0.9453 - val_loss: 0.2656 - val_acc: 0.9063\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2368 - acc: 0.9501 - val_loss: 0.1903 - val_acc: 0.9361\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2232 - acc: 0.9530 - val_loss: 0.1801 - val_acc: 0.9432\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2398 - acc: 0.9518 - val_loss: 0.2192 - val_acc: 0.9229\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2311 - acc: 0.9516 - val_loss: 0.2096 - val_acc: 0.9271\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2120 - acc: 0.9560 - val_loss: 0.1927 - val_acc: 0.9378\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2051 - acc: 0.9573 - val_loss: 0.1899 - val_acc: 0.9409\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2087 - acc: 0.9570 - val_loss: 0.1895 - val_acc: 0.9422\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2054 - acc: 0.9572 - val_loss: 0.1810 - val_acc: 0.9457\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2001 - acc: 0.9596 - val_loss: 0.1876 - val_acc: 0.9449\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.2422 - acc: 0.9500 - val_loss: 0.2377 - val_acc: 0.9216\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3460 - acc: 0.9351 - val_loss: 0.3077 - val_acc: 0.9008\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2668 - acc: 0.9472 - val_loss: 0.2325 - val_acc: 0.9219\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2392 - acc: 0.9528 - val_loss: 0.2269 - val_acc: 0.9316\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2082 - acc: 0.9562 - val_loss: 0.2190 - val_acc: 0.9311\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1952 - acc: 0.9615 - val_loss: 0.1944 - val_acc: 0.9349\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1830 - acc: 0.9640 - val_loss: 0.2017 - val_acc: 0.9359\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1778 - acc: 0.9644 - val_loss: 0.1852 - val_acc: 0.9412\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1939 - acc: 0.9622 - val_loss: 0.1979 - val_acc: 0.9364\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1875 - acc: 0.9620 - val_loss: 0.1924 - val_acc: 0.9443\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1934 - acc: 0.9624 - val_loss: 0.2193 - val_acc: 0.9312\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1790 - acc: 0.9624 - val_loss: 0.1856 - val_acc: 0.9452\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1665 - acc: 0.9674 - val_loss: 0.1855 - val_acc: 0.9449\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2585 - acc: 0.9507 - val_loss: 0.1908 - val_acc: 0.9399\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1793 - acc: 0.9629 - val_loss: 0.1811 - val_acc: 0.9459\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1918 - acc: 0.9633 - val_loss: 0.1782 - val_acc: 0.9522\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1669 - acc: 0.9677 - val_loss: 0.1752 - val_acc: 0.9485\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1535 - acc: 0.9719 - val_loss: 0.1954 - val_acc: 0.9385\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1588 - acc: 0.9680 - val_loss: 0.1831 - val_acc: 0.9498\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2074 - acc: 0.9599 - val_loss: 0.2294 - val_acc: 0.9385\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1856 - acc: 0.9654 - val_loss: 0.2011 - val_acc: 0.9362\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1709 - acc: 0.9674 - val_loss: 0.2021 - val_acc: 0.9415\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.1598 - acc: 0.9692 - val_loss: 0.1677 - val_acc: 0.9567\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1697 - acc: 0.9681 - val_loss: 0.3025 - val_acc: 0.9184\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1915 - acc: 0.9631 - val_loss: 0.2261 - val_acc: 0.9320\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1594 - acc: 0.9698 - val_loss: 0.2012 - val_acc: 0.9380\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1910 - acc: 0.9633 - val_loss: 0.2578 - val_acc: 0.9250\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1825 - acc: 0.9626 - val_loss: 0.1828 - val_acc: 0.9522\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1569 - acc: 0.9703 - val_loss: 0.1880 - val_acc: 0.9486\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 0.1552 - acc: 0.9703 - val_loss: 0.1842 - val_acc: 0.9499\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.1402 - acc: 0.9740 - val_loss: 0.2061 - val_acc: 0.9436\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1584 - acc: 0.9706 - val_loss: 0.2139 - val_acc: 0.9456\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1484 - acc: 0.9727 - val_loss: 0.1900 - val_acc: 0.9446\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1361 - acc: 0.9750 - val_loss: 0.1747 - val_acc: 0.9597\n",
      "Saving model...\n",
      "Fitting model # 11 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 117us/step - loss: 1.2899 - acc: 0.5868 - val_loss: 0.6632 - val_acc: 0.6359\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 2s 86us/step - loss: 1.1349 - acc: 0.6972 - val_loss: 0.7122 - val_acc: 0.6266\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 2s 92us/step - loss: 1.0642 - acc: 0.7222 - val_loss: 0.5104 - val_acc: 0.7800\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 1.0261 - acc: 0.7477 - val_loss: 0.4882 - val_acc: 0.7778\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.9704 - acc: 0.7674 - val_loss: 0.4709 - val_acc: 0.7936\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.9275 - acc: 0.7818 - val_loss: 0.4996 - val_acc: 0.7886\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.8743 - acc: 0.7983 - val_loss: 0.5318 - val_acc: 0.7823\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.8384 - acc: 0.8153 - val_loss: 0.6025 - val_acc: 0.7473\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.8013 - acc: 0.8164 - val_loss: 0.4606 - val_acc: 0.8140\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.7670 - acc: 0.8306 - val_loss: 0.5080 - val_acc: 0.7916\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.7381 - acc: 0.8333 - val_loss: 0.4093 - val_acc: 0.8293\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.7184 - acc: 0.8411 - val_loss: 0.4888 - val_acc: 0.8113\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.6900 - acc: 0.8493 - val_loss: 0.3399 - val_acc: 0.8686\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.6581 - acc: 0.8541 - val_loss: 0.3735 - val_acc: 0.8498\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.6432 - acc: 0.8516 - val_loss: 0.3534 - val_acc: 0.8655\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.6076 - acc: 0.8658 - val_loss: 0.3109 - val_acc: 0.8752\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.5860 - acc: 0.8704 - val_loss: 0.2944 - val_acc: 0.8794\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5655 - acc: 0.8722 - val_loss: 0.3252 - val_acc: 0.8651\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5461 - acc: 0.8741 - val_loss: 0.2885 - val_acc: 0.8873\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.5559 - acc: 0.8735 - val_loss: 0.2734 - val_acc: 0.9018\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5331 - acc: 0.8811 - val_loss: 0.2754 - val_acc: 0.8931\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5298 - acc: 0.8857 - val_loss: 0.4061 - val_acc: 0.8491\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.5064 - acc: 0.8896 - val_loss: 0.4295 - val_acc: 0.8288\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.5046 - acc: 0.8856 - val_loss: 0.3165 - val_acc: 0.8778\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4689 - acc: 0.8953 - val_loss: 0.2727 - val_acc: 0.8950\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4648 - acc: 0.8964 - val_loss: 0.3406 - val_acc: 0.8670\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.4675 - acc: 0.8947 - val_loss: 0.2740 - val_acc: 0.8881\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4575 - acc: 0.8986 - val_loss: 0.2706 - val_acc: 0.8982\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.4468 - acc: 0.8998 - val_loss: 0.2568 - val_acc: 0.9037\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4429 - acc: 0.8984 - val_loss: 0.2347 - val_acc: 0.9095\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4159 - acc: 0.9071 - val_loss: 0.3019 - val_acc: 0.8862\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4038 - acc: 0.9116 - val_loss: 0.2521 - val_acc: 0.9034\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3929 - acc: 0.9144 - val_loss: 0.3328 - val_acc: 0.8799\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4079 - acc: 0.9114 - val_loss: 0.2541 - val_acc: 0.9014\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3987 - acc: 0.9127 - val_loss: 0.3320 - val_acc: 0.8823\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3884 - acc: 0.9141 - val_loss: 0.2648 - val_acc: 0.9081\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3889 - acc: 0.9145 - val_loss: 0.2863 - val_acc: 0.8986\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.3564 - acc: 0.9217 - val_loss: 0.2683 - val_acc: 0.9050\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.3572 - acc: 0.9226 - val_loss: 0.2669 - val_acc: 0.9024\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3458 - acc: 0.9247 - val_loss: 0.2437 - val_acc: 0.9122\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3403 - acc: 0.9260 - val_loss: 0.2831 - val_acc: 0.8929\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3321 - acc: 0.9267 - val_loss: 0.2646 - val_acc: 0.9003\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3232 - acc: 0.9289 - val_loss: 0.3049 - val_acc: 0.8824\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.3277 - acc: 0.9277 - val_loss: 0.2922 - val_acc: 0.8942\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3179 - acc: 0.9305 - val_loss: 0.2792 - val_acc: 0.9021\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3076 - acc: 0.9331 - val_loss: 0.2246 - val_acc: 0.9192\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3115 - acc: 0.9332 - val_loss: 0.2286 - val_acc: 0.9240\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3412 - acc: 0.9283 - val_loss: 0.2963 - val_acc: 0.8921\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2896 - acc: 0.9363 - val_loss: 0.2098 - val_acc: 0.9251\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3100 - acc: 0.9338 - val_loss: 0.2207 - val_acc: 0.9254\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2799 - acc: 0.9411 - val_loss: 0.1976 - val_acc: 0.9325\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.2670 - acc: 0.9410 - val_loss: 0.2508 - val_acc: 0.9124\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3128 - acc: 0.9338 - val_loss: 0.2271 - val_acc: 0.9221\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2787 - acc: 0.9394 - val_loss: 0.1850 - val_acc: 0.9356\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 73us/step - loss: 0.2773 - acc: 0.9398 - val_loss: 0.2551 - val_acc: 0.9192\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2805 - acc: 0.9407 - val_loss: 0.3700 - val_acc: 0.8736\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2631 - acc: 0.9448 - val_loss: 0.2536 - val_acc: 0.9127\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2428 - acc: 0.9473 - val_loss: 0.2317 - val_acc: 0.9190\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2319 - acc: 0.9505 - val_loss: 0.1815 - val_acc: 0.9383\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2313 - acc: 0.9523 - val_loss: 0.2241 - val_acc: 0.9243\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2426 - acc: 0.9496 - val_loss: 0.2207 - val_acc: 0.9229\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2301 - acc: 0.9517 - val_loss: 0.2265 - val_acc: 0.9345\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2975 - acc: 0.9375 - val_loss: 0.2195 - val_acc: 0.9258\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2962 - acc: 0.9407 - val_loss: 0.3221 - val_acc: 0.9140\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2714 - acc: 0.9454 - val_loss: 0.2341 - val_acc: 0.9192\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2219 - acc: 0.9530 - val_loss: 0.3207 - val_acc: 0.8882\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2149 - acc: 0.9557 - val_loss: 0.2170 - val_acc: 0.9320\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2092 - acc: 0.9566 - val_loss: 0.2036 - val_acc: 0.9266\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2061 - acc: 0.9575 - val_loss: 0.3050 - val_acc: 0.9032\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2382 - acc: 0.9513 - val_loss: 0.1671 - val_acc: 0.9494\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2289 - acc: 0.9545 - val_loss: 0.2067 - val_acc: 0.9354\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2054 - acc: 0.9576 - val_loss: 0.2590 - val_acc: 0.9158\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2318 - acc: 0.9530 - val_loss: 0.2374 - val_acc: 0.9293\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2751 - acc: 0.9454 - val_loss: 0.1893 - val_acc: 0.9490\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2023 - acc: 0.9582 - val_loss: 0.1936 - val_acc: 0.9369\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.2168 - acc: 0.9576 - val_loss: 0.1952 - val_acc: 0.9364\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2215 - acc: 0.9564 - val_loss: 0.2234 - val_acc: 0.9285\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.1919 - acc: 0.9606 - val_loss: 0.2225 - val_acc: 0.9246\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1803 - acc: 0.9632 - val_loss: 0.2118 - val_acc: 0.9320\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2157 - acc: 0.9569 - val_loss: 0.2365 - val_acc: 0.9329\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1926 - acc: 0.9603 - val_loss: 0.1704 - val_acc: 0.9494\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1816 - acc: 0.9645 - val_loss: 0.2199 - val_acc: 0.9300\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2033 - acc: 0.9578 - val_loss: 0.2163 - val_acc: 0.9325\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.1908 - acc: 0.9626 - val_loss: 0.2497 - val_acc: 0.9320\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1844 - acc: 0.9642 - val_loss: 0.2084 - val_acc: 0.9378\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1662 - acc: 0.9664 - val_loss: 0.1835 - val_acc: 0.9438\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2208 - acc: 0.9598 - val_loss: 0.3764 - val_acc: 0.8966\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2401 - acc: 0.9535 - val_loss: 0.2294 - val_acc: 0.9301\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1771 - acc: 0.9654 - val_loss: 0.2130 - val_acc: 0.9314\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.1640 - acc: 0.9676 - val_loss: 0.1974 - val_acc: 0.9419\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.1503 - acc: 0.9737 - val_loss: 0.2064 - val_acc: 0.9353\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.1537 - acc: 0.9723 - val_loss: 0.2335 - val_acc: 0.9246\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1726 - acc: 0.9681 - val_loss: 0.2738 - val_acc: 0.9275\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.1651 - acc: 0.9692 - val_loss: 0.1860 - val_acc: 0.9457\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.1787 - acc: 0.9663 - val_loss: 0.2152 - val_acc: 0.9357\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2181 - acc: 0.9605 - val_loss: 0.2365 - val_acc: 0.9259\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.1537 - acc: 0.9700 - val_loss: 0.1704 - val_acc: 0.9483\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2288 - acc: 0.9599 - val_loss: 0.4008 - val_acc: 0.9110\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2097 - acc: 0.9614 - val_loss: 0.2192 - val_acc: 0.9362\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.1451 - acc: 0.9738 - val_loss: 0.1886 - val_acc: 0.9428\n",
      "Saving model...\n",
      "Fitting model # 12 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 115us/step - loss: 1.2714 - acc: 0.5878 - val_loss: 0.7213 - val_acc: 0.5612\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 1.1391 - acc: 0.6525 - val_loss: 0.5095 - val_acc: 0.7501\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 1.0765 - acc: 0.6940 - val_loss: 0.6232 - val_acc: 0.6820\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 1.0270 - acc: 0.7275 - val_loss: 0.6136 - val_acc: 0.6907\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.9864 - acc: 0.7435 - val_loss: 0.6505 - val_acc: 0.6767\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.9432 - acc: 0.7555 - val_loss: 0.5169 - val_acc: 0.7506\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.9085 - acc: 0.7724 - val_loss: 0.5246 - val_acc: 0.7671\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.8735 - acc: 0.7935 - val_loss: 0.4871 - val_acc: 0.7945\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.8318 - acc: 0.8085 - val_loss: 0.4510 - val_acc: 0.8135\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.7891 - acc: 0.8236 - val_loss: 0.5648 - val_acc: 0.7348\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.7511 - acc: 0.8213 - val_loss: 0.3608 - val_acc: 0.8612\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.7349 - acc: 0.8394 - val_loss: 0.4589 - val_acc: 0.8034\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.6865 - acc: 0.8405 - val_loss: 0.4301 - val_acc: 0.8206\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.6466 - acc: 0.8568 - val_loss: 0.3990 - val_acc: 0.8288\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.6724 - acc: 0.8497 - val_loss: 0.3935 - val_acc: 0.8457\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.6022 - acc: 0.8696 - val_loss: 0.3869 - val_acc: 0.8386\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 2s 93us/step - loss: 0.5886 - acc: 0.8662 - val_loss: 0.3081 - val_acc: 0.8770\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 2s 102us/step - loss: 0.5591 - acc: 0.8755 - val_loss: 0.3553 - val_acc: 0.8517\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 2s 85us/step - loss: 0.5508 - acc: 0.8766 - val_loss: 0.3877 - val_acc: 0.8407\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.5313 - acc: 0.8789 - val_loss: 0.3132 - val_acc: 0.8826\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.5297 - acc: 0.8823 - val_loss: 0.3104 - val_acc: 0.8841\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.5221 - acc: 0.8804 - val_loss: 0.3326 - val_acc: 0.8712\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4931 - acc: 0.8918 - val_loss: 0.3173 - val_acc: 0.8694\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4911 - acc: 0.8925 - val_loss: 0.3749 - val_acc: 0.8490\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4714 - acc: 0.8957 - val_loss: 0.3445 - val_acc: 0.8655\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 78us/step - loss: 0.4643 - acc: 0.8952 - val_loss: 0.2765 - val_acc: 0.8850\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4665 - acc: 0.8990 - val_loss: 0.3235 - val_acc: 0.8771\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.4588 - acc: 0.9006 - val_loss: 0.4130 - val_acc: 0.8462\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4598 - acc: 0.8972 - val_loss: 0.2709 - val_acc: 0.8969\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.4269 - acc: 0.9051 - val_loss: 0.2807 - val_acc: 0.8926\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.4094 - acc: 0.9088 - val_loss: 0.3486 - val_acc: 0.8720\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 2s 90us/step - loss: 0.4062 - acc: 0.9109 - val_loss: 0.2721 - val_acc: 0.9082\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 2s 107us/step - loss: 0.4152 - acc: 0.9087 - val_loss: 0.2460 - val_acc: 0.9023\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.3862 - acc: 0.9156 - val_loss: 0.2947 - val_acc: 0.8913\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3639 - acc: 0.9200 - val_loss: 0.2720 - val_acc: 0.8939\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 2s 89us/step - loss: 0.3580 - acc: 0.9210 - val_loss: 0.2865 - val_acc: 0.8897\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 2s 88us/step - loss: 0.3590 - acc: 0.9204 - val_loss: 0.2761 - val_acc: 0.8939\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 2s 82us/step - loss: 0.3541 - acc: 0.9225 - val_loss: 0.2532 - val_acc: 0.9037\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.3450 - acc: 0.9222 - val_loss: 0.2294 - val_acc: 0.9238\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3552 - acc: 0.9206 - val_loss: 0.2576 - val_acc: 0.9097\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3616 - acc: 0.9239 - val_loss: 0.2506 - val_acc: 0.9143\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3416 - acc: 0.9254 - val_loss: 0.2553 - val_acc: 0.9071\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3148 - acc: 0.9321 - val_loss: 0.2680 - val_acc: 0.8894\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.3015 - acc: 0.9337 - val_loss: 0.2448 - val_acc: 0.9093\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.3400 - acc: 0.9255 - val_loss: 0.3125 - val_acc: 0.8908\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.3155 - acc: 0.9309 - val_loss: 0.2731 - val_acc: 0.8957\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2902 - acc: 0.9371 - val_loss: 0.2736 - val_acc: 0.9011\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.3158 - acc: 0.9296 - val_loss: 0.3687 - val_acc: 0.8791\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.3278 - acc: 0.9289 - val_loss: 0.2832 - val_acc: 0.8994\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 2s 83us/step - loss: 0.2983 - acc: 0.9351 - val_loss: 0.2836 - val_acc: 0.9087\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.3345 - acc: 0.9311 - val_loss: 0.2445 - val_acc: 0.9201\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2962 - acc: 0.9369 - val_loss: 0.2067 - val_acc: 0.9287\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2664 - acc: 0.9435 - val_loss: 0.2431 - val_acc: 0.9130\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2622 - acc: 0.9418 - val_loss: 0.2247 - val_acc: 0.9213\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2580 - acc: 0.9450 - val_loss: 0.2081 - val_acc: 0.9293\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2709 - acc: 0.9433 - val_loss: 0.2264 - val_acc: 0.9256\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 75us/step - loss: 0.2523 - acc: 0.9465 - val_loss: 0.2470 - val_acc: 0.9155\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2511 - acc: 0.9487 - val_loss: 0.2827 - val_acc: 0.8957\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2744 - acc: 0.9443 - val_loss: 0.2569 - val_acc: 0.9068\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2351 - acc: 0.9496 - val_loss: 0.2137 - val_acc: 0.9235\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2745 - acc: 0.9459 - val_loss: 0.2515 - val_acc: 0.9190\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.3048 - acc: 0.9376 - val_loss: 0.3200 - val_acc: 0.9021\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 77us/step - loss: 0.2464 - acc: 0.9460 - val_loss: 0.2369 - val_acc: 0.9129\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2348 - acc: 0.9513 - val_loss: 0.3721 - val_acc: 0.8810\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 2s 84us/step - loss: 0.2481 - acc: 0.9506 - val_loss: 0.2459 - val_acc: 0.9213\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 2s 81us/step - loss: 0.2287 - acc: 0.9506 - val_loss: 0.1831 - val_acc: 0.9425\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 74us/step - loss: 0.2132 - acc: 0.9557 - val_loss: 0.2051 - val_acc: 0.9309\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2107 - acc: 0.9579 - val_loss: 0.3980 - val_acc: 0.8950\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 80us/step - loss: 0.2738 - acc: 0.9459 - val_loss: 0.2609 - val_acc: 0.9122\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 79us/step - loss: 0.2290 - acc: 0.9533 - val_loss: 0.2296 - val_acc: 0.9370\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 2s 95us/step - loss: 0.2507 - acc: 0.9479 - val_loss: 0.2231 - val_acc: 0.9338\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 2s 116us/step - loss: 0.2213 - acc: 0.9554 - val_loss: 0.2255 - val_acc: 0.9246\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 76us/step - loss: 0.2152 - acc: 0.9567 - val_loss: 0.2048 - val_acc: 0.9322\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.2045 - acc: 0.9593 - val_loss: 0.3207 - val_acc: 0.9023\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2009 - acc: 0.9594 - val_loss: 0.2087 - val_acc: 0.9314\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1825 - acc: 0.9640 - val_loss: 0.1994 - val_acc: 0.9316\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2752 - acc: 0.9493 - val_loss: 0.2812 - val_acc: 0.9290\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2195 - acc: 0.9563 - val_loss: 0.2107 - val_acc: 0.9362\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1878 - acc: 0.9627 - val_loss: 0.1867 - val_acc: 0.9428\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1977 - acc: 0.9605 - val_loss: 0.1997 - val_acc: 0.9399\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 67us/step - loss: 0.1928 - acc: 0.9615 - val_loss: 0.2068 - val_acc: 0.9407\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.1837 - acc: 0.9629 - val_loss: 0.2046 - val_acc: 0.9335\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1715 - acc: 0.9667 - val_loss: 0.1952 - val_acc: 0.9372\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1644 - acc: 0.9685 - val_loss: 0.2154 - val_acc: 0.9327\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1865 - acc: 0.9631 - val_loss: 0.2155 - val_acc: 0.9424\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2275 - acc: 0.9569 - val_loss: 0.2729 - val_acc: 0.9180\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1896 - acc: 0.9637 - val_loss: 0.2244 - val_acc: 0.9298\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2029 - acc: 0.9616 - val_loss: 0.1848 - val_acc: 0.9446\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1774 - acc: 0.9647 - val_loss: 0.1920 - val_acc: 0.9378\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2335 - acc: 0.9575 - val_loss: 0.2259 - val_acc: 0.9362\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1687 - acc: 0.9669 - val_loss: 0.1902 - val_acc: 0.9424\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1567 - acc: 0.9707 - val_loss: 0.1897 - val_acc: 0.9438\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1478 - acc: 0.9733 - val_loss: 0.1938 - val_acc: 0.9490\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1479 - acc: 0.9721 - val_loss: 0.1896 - val_acc: 0.9504\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1601 - acc: 0.9714 - val_loss: 0.2725 - val_acc: 0.9238\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2289 - acc: 0.9584 - val_loss: 0.2997 - val_acc: 0.9116\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2083 - acc: 0.9606 - val_loss: 0.1784 - val_acc: 0.9528\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1498 - acc: 0.9726 - val_loss: 0.1977 - val_acc: 0.9432\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1728 - acc: 0.9680 - val_loss: 0.2354 - val_acc: 0.9359\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.1642 - acc: 0.9700 - val_loss: 0.3249 - val_acc: 0.9288\n",
      "Saving model...\n",
      "Fitting model # 13 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 96us/step - loss: 1.3113 - acc: 0.6131 - val_loss: 0.6559 - val_acc: 0.6480\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 1.1174 - acc: 0.6834 - val_loss: 0.5497 - val_acc: 0.7182\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 1.0649 - acc: 0.7113 - val_loss: 0.5586 - val_acc: 0.7262\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 1.0150 - acc: 0.7315 - val_loss: 0.5787 - val_acc: 0.7147\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.9741 - acc: 0.7505 - val_loss: 0.5148 - val_acc: 0.7562\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.9273 - acc: 0.7676 - val_loss: 0.5744 - val_acc: 0.7483\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.8867 - acc: 0.7824 - val_loss: 0.4990 - val_acc: 0.7795\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.8547 - acc: 0.8011 - val_loss: 0.5237 - val_acc: 0.7692\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7987 - acc: 0.8118 - val_loss: 0.4094 - val_acc: 0.8282\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.7456 - acc: 0.8265 - val_loss: 0.4936 - val_acc: 0.7829\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7228 - acc: 0.8324 - val_loss: 0.3968 - val_acc: 0.8319\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.7052 - acc: 0.8344 - val_loss: 0.4266 - val_acc: 0.8214\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6461 - acc: 0.8519 - val_loss: 0.3555 - val_acc: 0.8512\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6309 - acc: 0.8515 - val_loss: 0.3093 - val_acc: 0.8889\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6172 - acc: 0.8608 - val_loss: 0.3126 - val_acc: 0.8781\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5823 - acc: 0.8654 - val_loss: 0.3550 - val_acc: 0.8485\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5801 - acc: 0.8702 - val_loss: 0.4010 - val_acc: 0.8300\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.5664 - acc: 0.8684 - val_loss: 0.3265 - val_acc: 0.8696\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5351 - acc: 0.8773 - val_loss: 0.3206 - val_acc: 0.8731\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5344 - acc: 0.8802 - val_loss: 0.3457 - val_acc: 0.8580\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5170 - acc: 0.8785 - val_loss: 0.2892 - val_acc: 0.8849\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5005 - acc: 0.8865 - val_loss: 0.2858 - val_acc: 0.8839\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4948 - acc: 0.8867 - val_loss: 0.3534 - val_acc: 0.8643\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4789 - acc: 0.8912 - val_loss: 0.3519 - val_acc: 0.8614\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.4720 - acc: 0.8890 - val_loss: 0.3044 - val_acc: 0.8758\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4507 - acc: 0.8954 - val_loss: 0.2964 - val_acc: 0.8911\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4356 - acc: 0.9008 - val_loss: 0.2922 - val_acc: 0.8841\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4220 - acc: 0.9058 - val_loss: 0.3153 - val_acc: 0.8757\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4156 - acc: 0.9061 - val_loss: 0.3413 - val_acc: 0.8591\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4188 - acc: 0.8991 - val_loss: 0.2660 - val_acc: 0.8995\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3954 - acc: 0.9120 - val_loss: 0.2870 - val_acc: 0.8882\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3922 - acc: 0.9095 - val_loss: 0.2636 - val_acc: 0.8955\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3938 - acc: 0.9148 - val_loss: 0.2813 - val_acc: 0.8866\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3783 - acc: 0.9137 - val_loss: 0.3062 - val_acc: 0.8887\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3918 - acc: 0.9124 - val_loss: 0.2929 - val_acc: 0.9000\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3926 - acc: 0.9157 - val_loss: 0.3212 - val_acc: 0.8684\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3551 - acc: 0.9177 - val_loss: 0.2197 - val_acc: 0.9222\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3931 - acc: 0.9179 - val_loss: 0.2818 - val_acc: 0.8900\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3410 - acc: 0.9215 - val_loss: 0.2449 - val_acc: 0.9055\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3354 - acc: 0.9246 - val_loss: 0.2607 - val_acc: 0.8961\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3221 - acc: 0.9268 - val_loss: 0.2343 - val_acc: 0.9124\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3104 - acc: 0.9323 - val_loss: 0.2135 - val_acc: 0.9153\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3382 - acc: 0.9261 - val_loss: 0.2331 - val_acc: 0.9110\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3121 - acc: 0.9304 - val_loss: 0.2157 - val_acc: 0.9190\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3080 - acc: 0.9338 - val_loss: 0.2701 - val_acc: 0.8924\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3069 - acc: 0.9320 - val_loss: 0.2292 - val_acc: 0.9153\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3553 - acc: 0.9255 - val_loss: 0.2911 - val_acc: 0.9166\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3135 - acc: 0.9340 - val_loss: 0.3335 - val_acc: 0.8792\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.3007 - acc: 0.9353 - val_loss: 0.2137 - val_acc: 0.9259\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2886 - acc: 0.9391 - val_loss: 0.2798 - val_acc: 0.8890\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2721 - acc: 0.9412 - val_loss: 0.3258 - val_acc: 0.8889\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.2715 - acc: 0.9419 - val_loss: 0.2076 - val_acc: 0.9229\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2832 - acc: 0.9385 - val_loss: 0.2327 - val_acc: 0.9221\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2667 - acc: 0.9443 - val_loss: 0.2237 - val_acc: 0.9153\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2620 - acc: 0.9438 - val_loss: 0.2728 - val_acc: 0.9064\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2572 - acc: 0.9436 - val_loss: 0.1888 - val_acc: 0.9316\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2568 - acc: 0.9465 - val_loss: 0.2108 - val_acc: 0.9262\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2512 - acc: 0.9476 - val_loss: 0.2215 - val_acc: 0.9211\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2395 - acc: 0.9474 - val_loss: 0.2450 - val_acc: 0.9201\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2459 - acc: 0.9486 - val_loss: 0.1793 - val_acc: 0.9401\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2549 - acc: 0.9470 - val_loss: 0.1957 - val_acc: 0.9314\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2326 - acc: 0.9501 - val_loss: 0.2096 - val_acc: 0.9230\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2269 - acc: 0.9515 - val_loss: 0.1768 - val_acc: 0.9407\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2432 - acc: 0.9485 - val_loss: 0.1931 - val_acc: 0.9377\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2174 - acc: 0.9539 - val_loss: 0.1715 - val_acc: 0.9433\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2085 - acc: 0.9559 - val_loss: 0.1773 - val_acc: 0.9407\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2066 - acc: 0.9575 - val_loss: 0.3065 - val_acc: 0.9093\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2426 - acc: 0.9525 - val_loss: 0.2448 - val_acc: 0.9166\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2744 - acc: 0.9476 - val_loss: 0.2364 - val_acc: 0.9192\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2650 - acc: 0.9465 - val_loss: 0.2373 - val_acc: 0.9217\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2288 - acc: 0.9529 - val_loss: 0.2038 - val_acc: 0.9330\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2067 - acc: 0.9556 - val_loss: 0.2214 - val_acc: 0.9333\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2030 - acc: 0.9600 - val_loss: 0.1783 - val_acc: 0.9433\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1943 - acc: 0.9601 - val_loss: 0.1454 - val_acc: 0.9568\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2023 - acc: 0.9590 - val_loss: 0.1814 - val_acc: 0.9401\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1818 - acc: 0.9633 - val_loss: 0.1843 - val_acc: 0.9374\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1777 - acc: 0.9649 - val_loss: 0.2066 - val_acc: 0.9279\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1697 - acc: 0.9658 - val_loss: 0.1892 - val_acc: 0.9411\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2159 - acc: 0.9556 - val_loss: 0.2063 - val_acc: 0.9345\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2216 - acc: 0.9591 - val_loss: 0.2420 - val_acc: 0.9304\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2102 - acc: 0.9597 - val_loss: 0.2377 - val_acc: 0.9209\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2095 - acc: 0.9579 - val_loss: 0.2058 - val_acc: 0.9354\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2267 - acc: 0.9582 - val_loss: 0.2384 - val_acc: 0.9311\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2164 - acc: 0.9564 - val_loss: 0.2511 - val_acc: 0.9330\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1874 - acc: 0.9637 - val_loss: 0.1793 - val_acc: 0.9438\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.1991 - acc: 0.9615 - val_loss: 0.2264 - val_acc: 0.9256\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1717 - acc: 0.9659 - val_loss: 0.1779 - val_acc: 0.9457\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1523 - acc: 0.9702 - val_loss: 0.1596 - val_acc: 0.9504\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1967 - acc: 0.9624 - val_loss: 0.1802 - val_acc: 0.9433\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1569 - acc: 0.9704 - val_loss: 0.1710 - val_acc: 0.9469\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1546 - acc: 0.9695 - val_loss: 0.1702 - val_acc: 0.9440\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1653 - acc: 0.9675 - val_loss: 0.1927 - val_acc: 0.9415\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1882 - acc: 0.9651 - val_loss: 0.1753 - val_acc: 0.9493\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1473 - acc: 0.9723 - val_loss: 0.1694 - val_acc: 0.9494\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1400 - acc: 0.9740 - val_loss: 0.1814 - val_acc: 0.9430\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1407 - acc: 0.9742 - val_loss: 0.1572 - val_acc: 0.9520\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1463 - acc: 0.9736 - val_loss: 0.2149 - val_acc: 0.9412\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1814 - acc: 0.9673 - val_loss: 0.1908 - val_acc: 0.9449\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1943 - acc: 0.9635 - val_loss: 0.2275 - val_acc: 0.9224\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1834 - acc: 0.9653 - val_loss: 0.2077 - val_acc: 0.9382\n",
      "Saving model...\n",
      "Fitting model # 14 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 97us/step - loss: 1.2799 - acc: 0.6063 - val_loss: 0.5985 - val_acc: 0.6876\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 1.1223 - acc: 0.6774 - val_loss: 0.5915 - val_acc: 0.6870\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.0634 - acc: 0.7028 - val_loss: 0.5682 - val_acc: 0.7143\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 1.0107 - acc: 0.7384 - val_loss: 0.6507 - val_acc: 0.6987\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.9874 - acc: 0.7528 - val_loss: 0.5608 - val_acc: 0.7250\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.9433 - acc: 0.7593 - val_loss: 0.4467 - val_acc: 0.8050\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.8918 - acc: 0.7901 - val_loss: 0.4739 - val_acc: 0.7897\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.8595 - acc: 0.8007 - val_loss: 0.5060 - val_acc: 0.7676\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.8218 - acc: 0.8099 - val_loss: 0.4313 - val_acc: 0.8169\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.7789 - acc: 0.8215 - val_loss: 0.3461 - val_acc: 0.8615\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.7416 - acc: 0.8336 - val_loss: 0.4484 - val_acc: 0.8043\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.7091 - acc: 0.8408 - val_loss: 0.4059 - val_acc: 0.8333\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.6716 - acc: 0.8517 - val_loss: 0.4107 - val_acc: 0.8377\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6726 - acc: 0.8502 - val_loss: 0.3866 - val_acc: 0.8509\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6234 - acc: 0.8604 - val_loss: 0.3226 - val_acc: 0.8754\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.6245 - acc: 0.8642 - val_loss: 0.5400 - val_acc: 0.7868\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6008 - acc: 0.8671 - val_loss: 0.3477 - val_acc: 0.8649\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.5894 - acc: 0.8704 - val_loss: 0.3018 - val_acc: 0.8940\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5541 - acc: 0.8829 - val_loss: 0.3651 - val_acc: 0.8625\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.5359 - acc: 0.8810 - val_loss: 0.3259 - val_acc: 0.8820\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5699 - acc: 0.8782 - val_loss: 0.3228 - val_acc: 0.8742\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5135 - acc: 0.8831 - val_loss: 0.3897 - val_acc: 0.8531\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5017 - acc: 0.8939 - val_loss: 0.3384 - val_acc: 0.8705\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.4890 - acc: 0.8901 - val_loss: 0.3023 - val_acc: 0.8886\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4719 - acc: 0.8983 - val_loss: 0.2813 - val_acc: 0.8960\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4568 - acc: 0.8993 - val_loss: 0.3214 - val_acc: 0.8767\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4703 - acc: 0.8969 - val_loss: 0.3306 - val_acc: 0.8750\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4463 - acc: 0.9042 - val_loss: 0.2944 - val_acc: 0.8907\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4544 - acc: 0.9030 - val_loss: 0.3694 - val_acc: 0.8643\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4551 - acc: 0.9020 - val_loss: 0.2612 - val_acc: 0.9072\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.4440 - acc: 0.9039 - val_loss: 0.3103 - val_acc: 0.8866\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3996 - acc: 0.9142 - val_loss: 0.2633 - val_acc: 0.9071\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4024 - acc: 0.9153 - val_loss: 0.2880 - val_acc: 0.8887\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4053 - acc: 0.9119 - val_loss: 0.2986 - val_acc: 0.8958\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3798 - acc: 0.9171 - val_loss: 0.2447 - val_acc: 0.9118\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3939 - acc: 0.9152 - val_loss: 0.2593 - val_acc: 0.9090\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3673 - acc: 0.9207 - val_loss: 0.2802 - val_acc: 0.8929\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3874 - acc: 0.9132 - val_loss: 0.2376 - val_acc: 0.9097\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3757 - acc: 0.9199 - val_loss: 0.3132 - val_acc: 0.8934\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3490 - acc: 0.9240 - val_loss: 0.2103 - val_acc: 0.9208\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3474 - acc: 0.9241 - val_loss: 0.2165 - val_acc: 0.9213\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3353 - acc: 0.9295 - val_loss: 0.2623 - val_acc: 0.9184\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3592 - acc: 0.9250 - val_loss: 0.2988 - val_acc: 0.8850\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3406 - acc: 0.9252 - val_loss: 0.2395 - val_acc: 0.9193\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3396 - acc: 0.9303 - val_loss: 0.2360 - val_acc: 0.9187\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3409 - acc: 0.9259 - val_loss: 0.2254 - val_acc: 0.9285\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3307 - acc: 0.9276 - val_loss: 0.2236 - val_acc: 0.9224\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3168 - acc: 0.9338 - val_loss: 0.2294 - val_acc: 0.9176\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3241 - acc: 0.9324 - val_loss: 0.2775 - val_acc: 0.9132\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3061 - acc: 0.9353 - val_loss: 0.2422 - val_acc: 0.9147\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2771 - acc: 0.9418 - val_loss: 0.2097 - val_acc: 0.9275\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2916 - acc: 0.9389 - val_loss: 0.2943 - val_acc: 0.9063\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3339 - acc: 0.9312 - val_loss: 0.2110 - val_acc: 0.9304\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2803 - acc: 0.9413 - val_loss: 0.2464 - val_acc: 0.9122\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2639 - acc: 0.9440 - val_loss: 0.2219 - val_acc: 0.9232\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2891 - acc: 0.9385 - val_loss: 0.1862 - val_acc: 0.9427\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2554 - acc: 0.9479 - val_loss: 0.1984 - val_acc: 0.9329\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.2430 - acc: 0.9507 - val_loss: 0.2243 - val_acc: 0.9216\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2719 - acc: 0.9440 - val_loss: 0.2924 - val_acc: 0.9074\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2798 - acc: 0.9422 - val_loss: 0.2210 - val_acc: 0.9259\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2942 - acc: 0.9393 - val_loss: 0.2141 - val_acc: 0.9290\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.2621 - acc: 0.9445 - val_loss: 0.1892 - val_acc: 0.9364\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2502 - acc: 0.9470 - val_loss: 0.2210 - val_acc: 0.9283\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2451 - acc: 0.9488 - val_loss: 0.2346 - val_acc: 0.9303\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.2405 - acc: 0.9511 - val_loss: 0.1985 - val_acc: 0.9316\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2216 - acc: 0.9542 - val_loss: 0.1922 - val_acc: 0.9362\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2233 - acc: 0.9537 - val_loss: 0.2200 - val_acc: 0.9238\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2579 - acc: 0.9463 - val_loss: 0.2054 - val_acc: 0.9359\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2808 - acc: 0.9456 - val_loss: 0.2372 - val_acc: 0.9253\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2235 - acc: 0.9536 - val_loss: 0.2123 - val_acc: 0.9300\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2664 - acc: 0.9475 - val_loss: 0.2363 - val_acc: 0.9259\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 68us/step - loss: 0.2524 - acc: 0.9490 - val_loss: 0.2112 - val_acc: 0.9333\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2469 - acc: 0.9508 - val_loss: 0.2102 - val_acc: 0.9325\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2239 - acc: 0.9545 - val_loss: 0.1994 - val_acc: 0.9427\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2473 - acc: 0.9517 - val_loss: 0.2031 - val_acc: 0.9401\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2111 - acc: 0.9590 - val_loss: 0.1731 - val_acc: 0.9491\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1873 - acc: 0.9644 - val_loss: 0.1788 - val_acc: 0.9440\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1993 - acc: 0.9597 - val_loss: 0.1951 - val_acc: 0.9382\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1870 - acc: 0.9637 - val_loss: 0.2019 - val_acc: 0.9327\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1786 - acc: 0.9671 - val_loss: 0.1942 - val_acc: 0.9390\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1838 - acc: 0.9633 - val_loss: 0.2206 - val_acc: 0.9256\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1916 - acc: 0.9616 - val_loss: 0.1756 - val_acc: 0.9475\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1884 - acc: 0.9631 - val_loss: 0.2281 - val_acc: 0.9293\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2236 - acc: 0.9585 - val_loss: 0.2836 - val_acc: 0.9177\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.2425 - acc: 0.9516 - val_loss: 0.2099 - val_acc: 0.9369\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2755 - acc: 0.9498 - val_loss: 0.2375 - val_acc: 0.9325\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2465 - acc: 0.9518 - val_loss: 0.2345 - val_acc: 0.9251\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2329 - acc: 0.9518 - val_loss: 0.1956 - val_acc: 0.9469\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1907 - acc: 0.9633 - val_loss: 0.2028 - val_acc: 0.9361\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1728 - acc: 0.9668 - val_loss: 0.1928 - val_acc: 0.9428\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1612 - acc: 0.9694 - val_loss: 0.1674 - val_acc: 0.9527\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1606 - acc: 0.9687 - val_loss: 0.1703 - val_acc: 0.9519\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1592 - acc: 0.9697 - val_loss: 0.1963 - val_acc: 0.9388\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1650 - acc: 0.9683 - val_loss: 0.2084 - val_acc: 0.9404\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2891 - acc: 0.9484 - val_loss: 0.2328 - val_acc: 0.9311\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1912 - acc: 0.9621 - val_loss: 0.2145 - val_acc: 0.9343\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1919 - acc: 0.9618 - val_loss: 0.1885 - val_acc: 0.9470\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.1518 - acc: 0.9721 - val_loss: 0.1907 - val_acc: 0.9457\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1509 - acc: 0.9718 - val_loss: 0.1725 - val_acc: 0.9494\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1479 - acc: 0.9719 - val_loss: 0.1854 - val_acc: 0.9454\n",
      "Saving model...\n",
      "Fitting model # 15 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 101us/step - loss: 1.2658 - acc: 0.6143 - val_loss: 0.6979 - val_acc: 0.6135\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 58us/step - loss: 1.1026 - acc: 0.6845 - val_loss: 0.6302 - val_acc: 0.6523\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 1.0127 - acc: 0.7255 - val_loss: 0.6349 - val_acc: 0.6966\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.9896 - acc: 0.7612 - val_loss: 0.5394 - val_acc: 0.7485\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.8783 - acc: 0.7778 - val_loss: 0.5422 - val_acc: 0.7493\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.8203 - acc: 0.7931 - val_loss: 0.3648 - val_acc: 0.8485\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7941 - acc: 0.8101 - val_loss: 0.4732 - val_acc: 0.7850\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.7351 - acc: 0.8199 - val_loss: 0.3854 - val_acc: 0.8349\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7031 - acc: 0.8296 - val_loss: 0.3739 - val_acc: 0.8398\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.6795 - acc: 0.8337 - val_loss: 0.3786 - val_acc: 0.8383\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6444 - acc: 0.8372 - val_loss: 0.3801 - val_acc: 0.8374\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.6203 - acc: 0.8458 - val_loss: 0.2938 - val_acc: 0.8829\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6068 - acc: 0.8507 - val_loss: 0.4070 - val_acc: 0.8150\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.6219 - acc: 0.8512 - val_loss: 0.4140 - val_acc: 0.7990\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5741 - acc: 0.8516 - val_loss: 0.3199 - val_acc: 0.8707\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5488 - acc: 0.8663 - val_loss: 0.3150 - val_acc: 0.8734\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5380 - acc: 0.8704 - val_loss: 0.2978 - val_acc: 0.8749\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.5212 - acc: 0.8674 - val_loss: 0.2750 - val_acc: 0.8934\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5041 - acc: 0.8786 - val_loss: 0.3095 - val_acc: 0.8742\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4861 - acc: 0.8820 - val_loss: 0.4144 - val_acc: 0.8341\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5241 - acc: 0.8762 - val_loss: 0.2919 - val_acc: 0.8816\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4691 - acc: 0.8880 - val_loss: 0.3311 - val_acc: 0.8614\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4506 - acc: 0.8906 - val_loss: 0.3284 - val_acc: 0.8649\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4468 - acc: 0.8916 - val_loss: 0.2782 - val_acc: 0.9056\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4325 - acc: 0.8971 - val_loss: 0.2804 - val_acc: 0.8918\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4416 - acc: 0.8960 - val_loss: 0.2563 - val_acc: 0.9031\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4170 - acc: 0.8999 - val_loss: 0.2727 - val_acc: 0.8952\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3936 - acc: 0.9068 - val_loss: 0.2833 - val_acc: 0.8881\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3808 - acc: 0.9071 - val_loss: 0.2613 - val_acc: 0.9023\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3930 - acc: 0.9068 - val_loss: 0.3000 - val_acc: 0.8950\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3916 - acc: 0.9069 - val_loss: 0.2231 - val_acc: 0.9188\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3785 - acc: 0.9139 - val_loss: 0.2944 - val_acc: 0.8845\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3626 - acc: 0.9150 - val_loss: 0.2946 - val_acc: 0.8874\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3572 - acc: 0.9154 - val_loss: 0.3312 - val_acc: 0.8795\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3551 - acc: 0.9181 - val_loss: 0.2836 - val_acc: 0.8844\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3224 - acc: 0.9230 - val_loss: 0.2341 - val_acc: 0.9100\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3113 - acc: 0.9247 - val_loss: 0.2239 - val_acc: 0.9158\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3045 - acc: 0.9288 - val_loss: 0.2455 - val_acc: 0.9063\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3214 - acc: 0.9221 - val_loss: 0.2113 - val_acc: 0.9227\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3208 - acc: 0.9253 - val_loss: 0.2544 - val_acc: 0.9066\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3108 - acc: 0.9259 - val_loss: 0.2081 - val_acc: 0.9208\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2930 - acc: 0.9302 - val_loss: 0.2430 - val_acc: 0.9105\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3293 - acc: 0.9239 - val_loss: 0.2682 - val_acc: 0.8994\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.2883 - acc: 0.9325 - val_loss: 0.2696 - val_acc: 0.8966\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2864 - acc: 0.9310 - val_loss: 0.2411 - val_acc: 0.9217\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2652 - acc: 0.9380 - val_loss: 0.2219 - val_acc: 0.9185\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2698 - acc: 0.9353 - val_loss: 0.1935 - val_acc: 0.9345\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2682 - acc: 0.9375 - val_loss: 0.2138 - val_acc: 0.9256\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2420 - acc: 0.9441 - val_loss: 0.1939 - val_acc: 0.9356\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2477 - acc: 0.9416 - val_loss: 0.2486 - val_acc: 0.9118\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.2671 - acc: 0.9387 - val_loss: 0.2263 - val_acc: 0.9243\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2315 - acc: 0.9449 - val_loss: 0.2280 - val_acc: 0.9164\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2354 - acc: 0.9439 - val_loss: 0.1831 - val_acc: 0.9399\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2283 - acc: 0.9479 - val_loss: 0.1932 - val_acc: 0.9375\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2363 - acc: 0.9452 - val_loss: 0.2779 - val_acc: 0.9161\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2646 - acc: 0.9420 - val_loss: 0.2539 - val_acc: 0.9101\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2353 - acc: 0.9441 - val_loss: 0.2203 - val_acc: 0.9214\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2150 - acc: 0.9491 - val_loss: 0.1841 - val_acc: 0.9346\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2143 - acc: 0.9517 - val_loss: 0.1766 - val_acc: 0.9440\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2235 - acc: 0.9489 - val_loss: 0.2049 - val_acc: 0.9345\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1975 - acc: 0.9530 - val_loss: 0.1668 - val_acc: 0.9478\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1937 - acc: 0.9551 - val_loss: 0.2228 - val_acc: 0.9300\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1973 - acc: 0.9545 - val_loss: 0.1940 - val_acc: 0.9391\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2237 - acc: 0.9494 - val_loss: 0.2384 - val_acc: 0.9300\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2197 - acc: 0.9503 - val_loss: 0.1946 - val_acc: 0.9412\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2267 - acc: 0.9493 - val_loss: 0.2002 - val_acc: 0.9395\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1775 - acc: 0.9587 - val_loss: 0.1725 - val_acc: 0.9481\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1687 - acc: 0.9610 - val_loss: 0.2075 - val_acc: 0.9338\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1672 - acc: 0.9612 - val_loss: 0.1871 - val_acc: 0.9436\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1919 - acc: 0.9572 - val_loss: 0.2542 - val_acc: 0.9179\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1982 - acc: 0.9553 - val_loss: 0.2637 - val_acc: 0.9195\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1852 - acc: 0.9572 - val_loss: 0.2023 - val_acc: 0.9435\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1834 - acc: 0.9581 - val_loss: 0.1974 - val_acc: 0.9420\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1642 - acc: 0.9619 - val_loss: 0.1924 - val_acc: 0.9398\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1532 - acc: 0.9646 - val_loss: 0.1699 - val_acc: 0.9554\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1537 - acc: 0.9653 - val_loss: 0.1969 - val_acc: 0.9420\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1527 - acc: 0.9668 - val_loss: 0.2368 - val_acc: 0.9309\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1887 - acc: 0.9582 - val_loss: 0.4264 - val_acc: 0.8910\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2448 - acc: 0.9501 - val_loss: 0.2031 - val_acc: 0.9419\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2262 - acc: 0.9544 - val_loss: 0.2497 - val_acc: 0.9288\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.1700 - acc: 0.9616 - val_loss: 0.3351 - val_acc: 0.9076\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1790 - acc: 0.9601 - val_loss: 0.1741 - val_acc: 0.9527\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1416 - acc: 0.9686 - val_loss: 0.1609 - val_acc: 0.9565\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1279 - acc: 0.9718 - val_loss: 0.1727 - val_acc: 0.9465\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1259 - acc: 0.9708 - val_loss: 0.1650 - val_acc: 0.9535\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1256 - acc: 0.9727 - val_loss: 0.1760 - val_acc: 0.9496\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1285 - acc: 0.9708 - val_loss: 0.1676 - val_acc: 0.9570\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1228 - acc: 0.9732 - val_loss: 0.1937 - val_acc: 0.9427\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1238 - acc: 0.9728 - val_loss: 0.1989 - val_acc: 0.9378\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1251 - acc: 0.9725 - val_loss: 0.2122 - val_acc: 0.9374\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1614 - acc: 0.9658 - val_loss: 0.2198 - val_acc: 0.9327\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1798 - acc: 0.9591 - val_loss: 0.1998 - val_acc: 0.9504\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1400 - acc: 0.9689 - val_loss: 0.1715 - val_acc: 0.9572\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1258 - acc: 0.9722 - val_loss: 0.2326 - val_acc: 0.9301\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1364 - acc: 0.9688 - val_loss: 0.1937 - val_acc: 0.9478\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1279 - acc: 0.9724 - val_loss: 0.2545 - val_acc: 0.9233\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2057 - acc: 0.9584 - val_loss: 0.2697 - val_acc: 0.9325\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1640 - acc: 0.9647 - val_loss: 0.2205 - val_acc: 0.9428\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1232 - acc: 0.9727 - val_loss: 0.1782 - val_acc: 0.9514\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1095 - acc: 0.9757 - val_loss: 0.1745 - val_acc: 0.9522\n",
      "Saving model...\n",
      "Fitting model # 16 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 1.2969 - acc: 0.5992 - val_loss: 0.7000 - val_acc: 0.5986\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.1438 - acc: 0.6507 - val_loss: 0.6954 - val_acc: 0.6324\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.0854 - acc: 0.7027 - val_loss: 0.5625 - val_acc: 0.7037\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.0262 - acc: 0.7159 - val_loss: 0.5636 - val_acc: 0.7071\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.9958 - acc: 0.7314 - val_loss: 0.4711 - val_acc: 0.7852\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.9458 - acc: 0.7563 - val_loss: 0.5454 - val_acc: 0.7311\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.9122 - acc: 0.7773 - val_loss: 0.4916 - val_acc: 0.7775\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.8502 - acc: 0.7913 - val_loss: 0.4482 - val_acc: 0.8034\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.8176 - acc: 0.8067 - val_loss: 0.4844 - val_acc: 0.7742\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7610 - acc: 0.8213 - val_loss: 0.4733 - val_acc: 0.8047\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.7322 - acc: 0.8318 - val_loss: 0.4032 - val_acc: 0.8291\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.7045 - acc: 0.8383 - val_loss: 0.3996 - val_acc: 0.8317\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.6504 - acc: 0.8473 - val_loss: 0.3939 - val_acc: 0.8395\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6519 - acc: 0.8523 - val_loss: 0.3356 - val_acc: 0.8612\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.6183 - acc: 0.8569 - val_loss: 0.3451 - val_acc: 0.8570\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5878 - acc: 0.8666 - val_loss: 0.3558 - val_acc: 0.8625\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.5866 - acc: 0.8669 - val_loss: 0.4022 - val_acc: 0.8224\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 72us/step - loss: 0.5462 - acc: 0.8701 - val_loss: 0.2843 - val_acc: 0.8900\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5549 - acc: 0.8765 - val_loss: 0.4015 - val_acc: 0.8428\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5331 - acc: 0.8780 - val_loss: 0.2896 - val_acc: 0.8857\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5081 - acc: 0.8835 - val_loss: 0.3382 - val_acc: 0.8607\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5095 - acc: 0.8851 - val_loss: 0.3254 - val_acc: 0.8626\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5038 - acc: 0.8846 - val_loss: 0.3186 - val_acc: 0.8725\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4846 - acc: 0.8877 - val_loss: 0.3185 - val_acc: 0.8776\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4942 - acc: 0.8908 - val_loss: 0.2875 - val_acc: 0.8865\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4739 - acc: 0.8955 - val_loss: 0.3163 - val_acc: 0.8700\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4528 - acc: 0.8927 - val_loss: 0.2521 - val_acc: 0.9043\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4408 - acc: 0.8998 - val_loss: 0.2931 - val_acc: 0.8881\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.4320 - acc: 0.9042 - val_loss: 0.3095 - val_acc: 0.8736\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.4118 - acc: 0.9052 - val_loss: 0.2565 - val_acc: 0.8948\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4119 - acc: 0.9072 - val_loss: 0.2805 - val_acc: 0.8905\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4120 - acc: 0.9058 - val_loss: 0.2893 - val_acc: 0.8873\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3914 - acc: 0.9097 - val_loss: 0.2565 - val_acc: 0.9003\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3956 - acc: 0.9131 - val_loss: 0.2991 - val_acc: 0.8897\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3806 - acc: 0.9127 - val_loss: 0.2890 - val_acc: 0.8932\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3841 - acc: 0.9126 - val_loss: 0.2986 - val_acc: 0.8939\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3779 - acc: 0.9171 - val_loss: 0.2968 - val_acc: 0.8884\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3501 - acc: 0.9208 - val_loss: 0.2541 - val_acc: 0.8995\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3994 - acc: 0.9123 - val_loss: 0.2732 - val_acc: 0.8960\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3737 - acc: 0.9160 - val_loss: 0.2904 - val_acc: 0.8918\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3486 - acc: 0.9226 - val_loss: 0.2109 - val_acc: 0.9201\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3383 - acc: 0.9270 - val_loss: 0.2362 - val_acc: 0.9087\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3113 - acc: 0.9303 - val_loss: 0.2405 - val_acc: 0.9093\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3146 - acc: 0.9312 - val_loss: 0.2537 - val_acc: 0.9053\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3043 - acc: 0.9313 - val_loss: 0.2329 - val_acc: 0.9137\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3037 - acc: 0.9335 - val_loss: 0.2545 - val_acc: 0.9011\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3035 - acc: 0.9318 - val_loss: 0.2846 - val_acc: 0.8965\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3003 - acc: 0.9327 - val_loss: 0.2246 - val_acc: 0.9161\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2885 - acc: 0.9365 - val_loss: 0.2101 - val_acc: 0.9264\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3102 - acc: 0.9359 - val_loss: 0.2921 - val_acc: 0.8961\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3150 - acc: 0.9305 - val_loss: 0.1907 - val_acc: 0.9301\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2791 - acc: 0.9394 - val_loss: 0.2159 - val_acc: 0.9166\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2584 - acc: 0.9437 - val_loss: 0.2002 - val_acc: 0.9259\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3093 - acc: 0.9372 - val_loss: 0.2926 - val_acc: 0.8997\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2793 - acc: 0.9399 - val_loss: 0.1924 - val_acc: 0.9301\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2888 - acc: 0.9389 - val_loss: 0.2549 - val_acc: 0.9201\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3889 - acc: 0.9233 - val_loss: 0.2837 - val_acc: 0.9026\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2768 - acc: 0.9411 - val_loss: 0.3840 - val_acc: 0.8749\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2800 - acc: 0.9400 - val_loss: 0.2095 - val_acc: 0.9248\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2407 - acc: 0.9483 - val_loss: 0.2249 - val_acc: 0.9148\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2371 - acc: 0.9487 - val_loss: 0.2592 - val_acc: 0.9045\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2226 - acc: 0.9517 - val_loss: 0.1991 - val_acc: 0.9291\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2254 - acc: 0.9505 - val_loss: 0.1805 - val_acc: 0.9386\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2381 - acc: 0.9524 - val_loss: 0.2116 - val_acc: 0.9221\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2211 - acc: 0.9519 - val_loss: 0.1868 - val_acc: 0.9351\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2441 - acc: 0.9479 - val_loss: 0.2377 - val_acc: 0.9095\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2101 - acc: 0.9556 - val_loss: 0.1865 - val_acc: 0.9333\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2076 - acc: 0.9578 - val_loss: 0.2148 - val_acc: 0.9288\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2166 - acc: 0.9559 - val_loss: 0.2028 - val_acc: 0.9259\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2183 - acc: 0.9538 - val_loss: 0.1632 - val_acc: 0.9420\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2346 - acc: 0.9494 - val_loss: 0.2178 - val_acc: 0.9259\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2095 - acc: 0.9581 - val_loss: 0.1858 - val_acc: 0.9349\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1923 - acc: 0.9594 - val_loss: 0.1839 - val_acc: 0.9370\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2044 - acc: 0.9602 - val_loss: 0.1894 - val_acc: 0.9300\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1917 - acc: 0.9600 - val_loss: 0.1643 - val_acc: 0.9491\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1921 - acc: 0.9613 - val_loss: 0.1859 - val_acc: 0.9354\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1780 - acc: 0.9642 - val_loss: 0.1819 - val_acc: 0.9372\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1773 - acc: 0.9654 - val_loss: 0.2292 - val_acc: 0.9283\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2094 - acc: 0.9567 - val_loss: 0.1966 - val_acc: 0.9366\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1893 - acc: 0.9605 - val_loss: 0.1518 - val_acc: 0.9538\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1759 - acc: 0.9638 - val_loss: 0.1776 - val_acc: 0.9412\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1834 - acc: 0.9637 - val_loss: 0.1702 - val_acc: 0.9424\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2093 - acc: 0.9594 - val_loss: 0.1772 - val_acc: 0.9451\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2063 - acc: 0.9574 - val_loss: 0.1596 - val_acc: 0.9477\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1975 - acc: 0.9597 - val_loss: 0.2852 - val_acc: 0.9153\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2017 - acc: 0.9618 - val_loss: 0.1871 - val_acc: 0.9419\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1999 - acc: 0.9598 - val_loss: 0.2090 - val_acc: 0.9288\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1796 - acc: 0.9642 - val_loss: 0.1937 - val_acc: 0.9403\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2115 - acc: 0.9596 - val_loss: 0.1652 - val_acc: 0.9528\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1694 - acc: 0.9694 - val_loss: 0.1713 - val_acc: 0.9441\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1567 - acc: 0.9700 - val_loss: 0.1883 - val_acc: 0.9378\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1427 - acc: 0.9736 - val_loss: 0.1543 - val_acc: 0.9501\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1369 - acc: 0.9747 - val_loss: 0.1744 - val_acc: 0.9419\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1390 - acc: 0.9742 - val_loss: 0.1527 - val_acc: 0.9504\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1422 - acc: 0.9734 - val_loss: 0.1497 - val_acc: 0.9509\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1466 - acc: 0.9718 - val_loss: 0.1578 - val_acc: 0.9509\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1382 - acc: 0.9742 - val_loss: 0.2135 - val_acc: 0.9308\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1319 - acc: 0.9749 - val_loss: 0.1752 - val_acc: 0.9411\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1355 - acc: 0.9739 - val_loss: 0.1600 - val_acc: 0.9486\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1741 - acc: 0.9695 - val_loss: 0.3508 - val_acc: 0.8944\n",
      "Saving model...\n",
      "Fitting model # 17 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 1.3080 - acc: 0.6104 - val_loss: 0.6259 - val_acc: 0.6404\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 1.1178 - acc: 0.6750 - val_loss: 0.6781 - val_acc: 0.6428\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.0328 - acc: 0.7155 - val_loss: 0.5816 - val_acc: 0.7111\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.9595 - acc: 0.7488 - val_loss: 0.5114 - val_acc: 0.7713\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.8827 - acc: 0.7899 - val_loss: 0.5393 - val_acc: 0.7514\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.8316 - acc: 0.7965 - val_loss: 0.5039 - val_acc: 0.7676\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.7849 - acc: 0.8172 - val_loss: 0.4446 - val_acc: 0.8087\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.7398 - acc: 0.8211 - val_loss: 0.4127 - val_acc: 0.8216\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.7148 - acc: 0.8260 - val_loss: 0.4413 - val_acc: 0.8023\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6651 - acc: 0.8397 - val_loss: 0.4226 - val_acc: 0.8121\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6557 - acc: 0.8418 - val_loss: 0.3982 - val_acc: 0.8385\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.6169 - acc: 0.8475 - val_loss: 0.3629 - val_acc: 0.8543\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.6395 - acc: 0.8494 - val_loss: 0.3908 - val_acc: 0.8317\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5941 - acc: 0.8592 - val_loss: 0.4112 - val_acc: 0.8151\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5658 - acc: 0.8633 - val_loss: 0.3766 - val_acc: 0.8332\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5557 - acc: 0.8642 - val_loss: 0.4051 - val_acc: 0.8393\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5471 - acc: 0.8677 - val_loss: 0.2783 - val_acc: 0.8919\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5143 - acc: 0.8768 - val_loss: 0.3220 - val_acc: 0.8667\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4983 - acc: 0.8791 - val_loss: 0.3146 - val_acc: 0.8755\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4905 - acc: 0.8810 - val_loss: 0.3357 - val_acc: 0.8723\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4852 - acc: 0.8836 - val_loss: 0.3166 - val_acc: 0.8688\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4678 - acc: 0.8879 - val_loss: 0.3410 - val_acc: 0.8562\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4641 - acc: 0.8881 - val_loss: 0.3607 - val_acc: 0.8456\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4639 - acc: 0.8906 - val_loss: 0.3146 - val_acc: 0.8794\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4282 - acc: 0.8966 - val_loss: 0.2918 - val_acc: 0.8863\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4203 - acc: 0.9003 - val_loss: 0.3422 - val_acc: 0.8593\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4152 - acc: 0.8975 - val_loss: 0.4300 - val_acc: 0.8254\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4228 - acc: 0.8997 - val_loss: 0.3359 - val_acc: 0.8646\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4075 - acc: 0.9046 - val_loss: 0.3187 - val_acc: 0.8738\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3968 - acc: 0.9029 - val_loss: 0.2355 - val_acc: 0.9143\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3891 - acc: 0.9079 - val_loss: 0.3009 - val_acc: 0.8842\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3757 - acc: 0.9081 - val_loss: 0.2397 - val_acc: 0.9116\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3678 - acc: 0.9108 - val_loss: 0.3540 - val_acc: 0.8767\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3905 - acc: 0.9110 - val_loss: 0.2316 - val_acc: 0.9119\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3567 - acc: 0.9132 - val_loss: 0.2543 - val_acc: 0.9081\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3418 - acc: 0.9185 - val_loss: 0.2215 - val_acc: 0.9176\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3262 - acc: 0.9219 - val_loss: 0.2618 - val_acc: 0.9027\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3304 - acc: 0.9217 - val_loss: 0.2983 - val_acc: 0.8795\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3246 - acc: 0.9208 - val_loss: 0.2502 - val_acc: 0.9072\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3228 - acc: 0.9206 - val_loss: 0.2212 - val_acc: 0.9193\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3023 - acc: 0.9271 - val_loss: 0.2493 - val_acc: 0.9082\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2918 - acc: 0.9281 - val_loss: 0.2008 - val_acc: 0.9262\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2943 - acc: 0.9270 - val_loss: 0.1959 - val_acc: 0.9287\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 62us/step - loss: 0.3360 - acc: 0.9252 - val_loss: 0.2714 - val_acc: 0.8928\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3073 - acc: 0.9288 - val_loss: 0.2559 - val_acc: 0.9137\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.2903 - acc: 0.9330 - val_loss: 0.2190 - val_acc: 0.9169\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2813 - acc: 0.9295 - val_loss: 0.2608 - val_acc: 0.9053\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2738 - acc: 0.9335 - val_loss: 0.2298 - val_acc: 0.9177\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2827 - acc: 0.9311 - val_loss: 0.2076 - val_acc: 0.9258\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2547 - acc: 0.9374 - val_loss: 0.3046 - val_acc: 0.8923\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2651 - acc: 0.9379 - val_loss: 0.1990 - val_acc: 0.9277\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2327 - acc: 0.9438 - val_loss: 0.1909 - val_acc: 0.9309\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2422 - acc: 0.9408 - val_loss: 0.2925 - val_acc: 0.9113\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2783 - acc: 0.9358 - val_loss: 0.1805 - val_acc: 0.9396\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2507 - acc: 0.9431 - val_loss: 0.2080 - val_acc: 0.9258\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2286 - acc: 0.9439 - val_loss: 0.2319 - val_acc: 0.9196\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.2288 - acc: 0.9466 - val_loss: 0.2384 - val_acc: 0.9158\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2355 - acc: 0.9449 - val_loss: 0.2082 - val_acc: 0.9271\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2330 - acc: 0.9474 - val_loss: 0.2145 - val_acc: 0.9246\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2247 - acc: 0.9468 - val_loss: 0.1729 - val_acc: 0.9422\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2328 - acc: 0.9472 - val_loss: 0.1953 - val_acc: 0.9317\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2084 - acc: 0.9507 - val_loss: 0.1935 - val_acc: 0.9329\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1933 - acc: 0.9538 - val_loss: 0.1925 - val_acc: 0.9320\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2263 - acc: 0.9495 - val_loss: 0.2094 - val_acc: 0.9282\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2318 - acc: 0.9452 - val_loss: 0.1997 - val_acc: 0.9301\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2383 - acc: 0.9481 - val_loss: 0.2628 - val_acc: 0.9208\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2500 - acc: 0.9455 - val_loss: 0.2519 - val_acc: 0.9110\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.1969 - acc: 0.9506 - val_loss: 0.1848 - val_acc: 0.9390\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1945 - acc: 0.9545 - val_loss: 0.1766 - val_acc: 0.9433\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1935 - acc: 0.9530 - val_loss: 0.1732 - val_acc: 0.9449\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1826 - acc: 0.9585 - val_loss: 0.1819 - val_acc: 0.9385\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 66us/step - loss: 0.1913 - acc: 0.9552 - val_loss: 0.2142 - val_acc: 0.9317\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2032 - acc: 0.9538 - val_loss: 0.1659 - val_acc: 0.9491\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.2059 - acc: 0.9534 - val_loss: 0.2182 - val_acc: 0.9203\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 67us/step - loss: 0.1772 - acc: 0.9597 - val_loss: 0.2169 - val_acc: 0.9291\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1703 - acc: 0.9597 - val_loss: 0.1477 - val_acc: 0.9531\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1684 - acc: 0.9623 - val_loss: 0.1922 - val_acc: 0.9356\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1592 - acc: 0.9638 - val_loss: 0.1663 - val_acc: 0.9493\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1676 - acc: 0.9621 - val_loss: 0.2041 - val_acc: 0.9304\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1572 - acc: 0.9640 - val_loss: 0.1790 - val_acc: 0.9399\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1537 - acc: 0.9656 - val_loss: 0.1811 - val_acc: 0.9443\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1574 - acc: 0.9642 - val_loss: 0.1576 - val_acc: 0.9510\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1599 - acc: 0.9629 - val_loss: 0.2018 - val_acc: 0.9382\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1471 - acc: 0.9669 - val_loss: 0.1855 - val_acc: 0.9388\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1406 - acc: 0.9678 - val_loss: 0.1750 - val_acc: 0.9430\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1539 - acc: 0.9658 - val_loss: 0.2036 - val_acc: 0.9359\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1661 - acc: 0.9618 - val_loss: 0.1699 - val_acc: 0.9477\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1281 - acc: 0.9716 - val_loss: 0.1464 - val_acc: 0.9544\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1407 - acc: 0.9685 - val_loss: 0.1679 - val_acc: 0.9462\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1363 - acc: 0.9708 - val_loss: 0.2164 - val_acc: 0.9335\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1267 - acc: 0.9704 - val_loss: 0.1658 - val_acc: 0.9480\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1304 - acc: 0.9717 - val_loss: 0.2196 - val_acc: 0.9280\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1375 - acc: 0.9701 - val_loss: 0.1844 - val_acc: 0.9449\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1863 - acc: 0.9618 - val_loss: 0.2061 - val_acc: 0.9477\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2849 - acc: 0.9446 - val_loss: 0.3166 - val_acc: 0.9111\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1960 - acc: 0.9582 - val_loss: 0.2132 - val_acc: 0.9375\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1994 - acc: 0.9564 - val_loss: 0.2000 - val_acc: 0.9422\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1476 - acc: 0.9666 - val_loss: 0.1671 - val_acc: 0.9488\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1137 - acc: 0.9748 - val_loss: 0.1877 - val_acc: 0.9451\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1156 - acc: 0.9744 - val_loss: 0.1571 - val_acc: 0.9522\n",
      "Saving model...\n",
      "Fitting model # 18 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 1.2714 - acc: 0.6017 - val_loss: 0.6522 - val_acc: 0.6705\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.1238 - acc: 0.6878 - val_loss: 0.6484 - val_acc: 0.6725\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.0663 - acc: 0.7167 - val_loss: 0.5943 - val_acc: 0.7127\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 1.0263 - acc: 0.7360 - val_loss: 0.6047 - val_acc: 0.7084\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.9713 - acc: 0.7490 - val_loss: 0.5426 - val_acc: 0.7383\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.9260 - acc: 0.7792 - val_loss: 0.4942 - val_acc: 0.7821\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.8895 - acc: 0.7841 - val_loss: 0.4879 - val_acc: 0.7899\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.8502 - acc: 0.8073 - val_loss: 0.5424 - val_acc: 0.7643\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 69us/step - loss: 0.8039 - acc: 0.8175 - val_loss: 0.4279 - val_acc: 0.8180\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7838 - acc: 0.8240 - val_loss: 0.4510 - val_acc: 0.8138\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.7317 - acc: 0.8405 - val_loss: 0.4761 - val_acc: 0.7974\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.7094 - acc: 0.8453 - val_loss: 0.3551 - val_acc: 0.8583\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6568 - acc: 0.8590 - val_loss: 0.3346 - val_acc: 0.8625\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.6371 - acc: 0.8590 - val_loss: 0.3868 - val_acc: 0.8370\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.5985 - acc: 0.8697 - val_loss: 0.3524 - val_acc: 0.8559\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5844 - acc: 0.8721 - val_loss: 0.3752 - val_acc: 0.8386\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5670 - acc: 0.8742 - val_loss: 0.3976 - val_acc: 0.8573\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5525 - acc: 0.8798 - val_loss: 0.3710 - val_acc: 0.8386\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5232 - acc: 0.8860 - val_loss: 0.3163 - val_acc: 0.8705\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5212 - acc: 0.8858 - val_loss: 0.3345 - val_acc: 0.8617\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4916 - acc: 0.8882 - val_loss: 0.3498 - val_acc: 0.8575\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4803 - acc: 0.8951 - val_loss: 0.3090 - val_acc: 0.8805\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4692 - acc: 0.8999 - val_loss: 0.3878 - val_acc: 0.8401\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4840 - acc: 0.8930 - val_loss: 0.3240 - val_acc: 0.8799\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4474 - acc: 0.9024 - val_loss: 0.3332 - val_acc: 0.8728\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4327 - acc: 0.9033 - val_loss: 0.2640 - val_acc: 0.8952\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4292 - acc: 0.9061 - val_loss: 0.3465 - val_acc: 0.8596\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4300 - acc: 0.9045 - val_loss: 0.2647 - val_acc: 0.8942\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4262 - acc: 0.9044 - val_loss: 0.2703 - val_acc: 0.8913\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3915 - acc: 0.9158 - val_loss: 0.2926 - val_acc: 0.8815\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3744 - acc: 0.9175 - val_loss: 0.2717 - val_acc: 0.8903\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3572 - acc: 0.9203 - val_loss: 0.2284 - val_acc: 0.9118\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3579 - acc: 0.9217 - val_loss: 0.2497 - val_acc: 0.9013\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3628 - acc: 0.9198 - val_loss: 0.2574 - val_acc: 0.9006\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3556 - acc: 0.9221 - val_loss: 0.2345 - val_acc: 0.9068\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3440 - acc: 0.9247 - val_loss: 0.2583 - val_acc: 0.8969\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3338 - acc: 0.9258 - val_loss: 0.2610 - val_acc: 0.9005\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3457 - acc: 0.9252 - val_loss: 0.1954 - val_acc: 0.9287\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3175 - acc: 0.9340 - val_loss: 0.2491 - val_acc: 0.9010\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2990 - acc: 0.9333 - val_loss: 0.1962 - val_acc: 0.9256\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3275 - acc: 0.9310 - val_loss: 0.2273 - val_acc: 0.9129\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3298 - acc: 0.9296 - val_loss: 0.2980 - val_acc: 0.8936\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3025 - acc: 0.9345 - val_loss: 0.2609 - val_acc: 0.9039\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2972 - acc: 0.9368 - val_loss: 0.2801 - val_acc: 0.9011\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2921 - acc: 0.9355 - val_loss: 0.2100 - val_acc: 0.9176\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3385 - acc: 0.9276 - val_loss: 0.3272 - val_acc: 0.8984\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3204 - acc: 0.9317 - val_loss: 0.2402 - val_acc: 0.9105\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2670 - acc: 0.9423 - val_loss: 0.1737 - val_acc: 0.9382\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2716 - acc: 0.9453 - val_loss: 0.2305 - val_acc: 0.9174\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2697 - acc: 0.9449 - val_loss: 0.1945 - val_acc: 0.9246\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2865 - acc: 0.9417 - val_loss: 0.2649 - val_acc: 0.8989\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2501 - acc: 0.9469 - val_loss: 0.2260 - val_acc: 0.9151\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2482 - acc: 0.9469 - val_loss: 0.2902 - val_acc: 0.8958\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2559 - acc: 0.9441 - val_loss: 0.1871 - val_acc: 0.9325\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2511 - acc: 0.9466 - val_loss: 0.2569 - val_acc: 0.9108\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2533 - acc: 0.9474 - val_loss: 0.2780 - val_acc: 0.8990\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2368 - acc: 0.9498 - val_loss: 0.1999 - val_acc: 0.9280\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2247 - acc: 0.9532 - val_loss: 0.2003 - val_acc: 0.9262\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2297 - acc: 0.9499 - val_loss: 0.2250 - val_acc: 0.9188\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2352 - acc: 0.9513 - val_loss: 0.2164 - val_acc: 0.9264\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2364 - acc: 0.9502 - val_loss: 0.1826 - val_acc: 0.9372\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2972 - acc: 0.9434 - val_loss: 0.2315 - val_acc: 0.9182\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2345 - acc: 0.9507 - val_loss: 0.2310 - val_acc: 0.9185\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2165 - acc: 0.9547 - val_loss: 0.2588 - val_acc: 0.9072\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2123 - acc: 0.9545 - val_loss: 0.2311 - val_acc: 0.9261\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1946 - acc: 0.9595 - val_loss: 0.1969 - val_acc: 0.9291\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1853 - acc: 0.9615 - val_loss: 0.1760 - val_acc: 0.9425\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 2s 100us/step - loss: 0.1852 - acc: 0.9626 - val_loss: 0.1680 - val_acc: 0.9433\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1998 - acc: 0.9604 - val_loss: 0.2276 - val_acc: 0.9274\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2212 - acc: 0.9550 - val_loss: 0.2650 - val_acc: 0.9105\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2627 - acc: 0.9492 - val_loss: 0.2766 - val_acc: 0.9258\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2441 - acc: 0.9525 - val_loss: 0.2203 - val_acc: 0.9285\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1914 - acc: 0.9612 - val_loss: 0.2242 - val_acc: 0.9251\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1816 - acc: 0.9624 - val_loss: 0.2037 - val_acc: 0.9271\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1706 - acc: 0.9667 - val_loss: 0.1977 - val_acc: 0.9329\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1682 - acc: 0.9659 - val_loss: 0.2282 - val_acc: 0.9280\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1746 - acc: 0.9662 - val_loss: 0.2038 - val_acc: 0.9290\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 62us/step - loss: 0.1598 - acc: 0.9686 - val_loss: 0.1735 - val_acc: 0.9436\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2057 - acc: 0.9591 - val_loss: 0.1920 - val_acc: 0.9430\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1843 - acc: 0.9651 - val_loss: 0.1948 - val_acc: 0.9427\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2705 - acc: 0.9499 - val_loss: 0.2839 - val_acc: 0.9174\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1934 - acc: 0.9626 - val_loss: 0.1968 - val_acc: 0.9420\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1614 - acc: 0.9705 - val_loss: 0.1866 - val_acc: 0.9388\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1596 - acc: 0.9697 - val_loss: 0.1702 - val_acc: 0.9449\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1704 - acc: 0.9670 - val_loss: 0.1925 - val_acc: 0.9364\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1532 - acc: 0.9706 - val_loss: 0.1573 - val_acc: 0.9543\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1610 - acc: 0.9712 - val_loss: 0.1683 - val_acc: 0.9539\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 66us/step - loss: 0.1752 - acc: 0.9681 - val_loss: 0.2353 - val_acc: 0.9251\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1567 - acc: 0.9680 - val_loss: 0.1829 - val_acc: 0.9459\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1375 - acc: 0.9744 - val_loss: 0.1748 - val_acc: 0.9457\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1504 - acc: 0.9726 - val_loss: 0.2234 - val_acc: 0.9369\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1404 - acc: 0.9751 - val_loss: 0.1735 - val_acc: 0.9464\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1324 - acc: 0.9768 - val_loss: 0.1672 - val_acc: 0.9499\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1905 - acc: 0.9633 - val_loss: 0.2038 - val_acc: 0.9449\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2065 - acc: 0.9615 - val_loss: 0.2261 - val_acc: 0.9412\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2587 - acc: 0.9584 - val_loss: 0.2803 - val_acc: 0.9271\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2157 - acc: 0.9567 - val_loss: 0.1993 - val_acc: 0.9454\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1495 - acc: 0.9705 - val_loss: 0.1754 - val_acc: 0.9493\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1367 - acc: 0.9741 - val_loss: 0.2483 - val_acc: 0.9291\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1386 - acc: 0.9728 - val_loss: 0.2081 - val_acc: 0.9491\n",
      "Saving model...\n",
      "Fitting model # 19 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 102us/step - loss: 1.2938 - acc: 0.6034 - val_loss: 0.6802 - val_acc: 0.6205\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.1514 - acc: 0.6587 - val_loss: 0.6074 - val_acc: 0.6634\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.0980 - acc: 0.6792 - val_loss: 0.6263 - val_acc: 0.6596\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.0507 - acc: 0.7076 - val_loss: 0.5432 - val_acc: 0.7332\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.0009 - acc: 0.7206 - val_loss: 0.7183 - val_acc: 0.6572\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.9690 - acc: 0.7452 - val_loss: 0.5380 - val_acc: 0.7626\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.9406 - acc: 0.7555 - val_loss: 0.6180 - val_acc: 0.6976\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.9090 - acc: 0.7707 - val_loss: 0.5215 - val_acc: 0.7549\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.8560 - acc: 0.7942 - val_loss: 0.5058 - val_acc: 0.7763\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.8239 - acc: 0.7942 - val_loss: 0.3932 - val_acc: 0.8425\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.7847 - acc: 0.8167 - val_loss: 0.4618 - val_acc: 0.8010\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7464 - acc: 0.8213 - val_loss: 0.4335 - val_acc: 0.8209\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.7179 - acc: 0.8399 - val_loss: 0.3618 - val_acc: 0.8464\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6873 - acc: 0.8414 - val_loss: 0.4599 - val_acc: 0.7971\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.6711 - acc: 0.8493 - val_loss: 0.4197 - val_acc: 0.8158\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6309 - acc: 0.8545 - val_loss: 0.3272 - val_acc: 0.8725\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.6026 - acc: 0.8643 - val_loss: 0.4422 - val_acc: 0.8113\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.6070 - acc: 0.8643 - val_loss: 0.4038 - val_acc: 0.8227\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.5942 - acc: 0.8640 - val_loss: 0.3560 - val_acc: 0.8544\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5826 - acc: 0.8645 - val_loss: 0.2659 - val_acc: 0.9003\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5659 - acc: 0.8738 - val_loss: 0.3570 - val_acc: 0.8512\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5503 - acc: 0.8789 - val_loss: 0.3310 - val_acc: 0.8775\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.5477 - acc: 0.8781 - val_loss: 0.4487 - val_acc: 0.8266\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5342 - acc: 0.8816 - val_loss: 0.3319 - val_acc: 0.8657\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.5029 - acc: 0.8857 - val_loss: 0.3655 - val_acc: 0.8398\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4896 - acc: 0.8887 - val_loss: 0.3785 - val_acc: 0.8448\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4754 - acc: 0.8902 - val_loss: 0.3231 - val_acc: 0.8749\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.4637 - acc: 0.8913 - val_loss: 0.2723 - val_acc: 0.9013\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4672 - acc: 0.8974 - val_loss: 0.3967 - val_acc: 0.8338\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4777 - acc: 0.8909 - val_loss: 0.2750 - val_acc: 0.8923\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4450 - acc: 0.9011 - val_loss: 0.3257 - val_acc: 0.8765\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4392 - acc: 0.9024 - val_loss: 0.2543 - val_acc: 0.9074\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4172 - acc: 0.9058 - val_loss: 0.2631 - val_acc: 0.8924\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3972 - acc: 0.9116 - val_loss: 0.2787 - val_acc: 0.8868\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4041 - acc: 0.9054 - val_loss: 0.2669 - val_acc: 0.9045\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4016 - acc: 0.9104 - val_loss: 0.2177 - val_acc: 0.9206\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3926 - acc: 0.9120 - val_loss: 0.2683 - val_acc: 0.9011\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3872 - acc: 0.9144 - val_loss: 0.3252 - val_acc: 0.8797\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4160 - acc: 0.9085 - val_loss: 0.2573 - val_acc: 0.9045\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3928 - acc: 0.9120 - val_loss: 0.2654 - val_acc: 0.8992\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3598 - acc: 0.9186 - val_loss: 0.2697 - val_acc: 0.8892\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3628 - acc: 0.9189 - val_loss: 0.3106 - val_acc: 0.8873\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3799 - acc: 0.9176 - val_loss: 0.2111 - val_acc: 0.9214\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3554 - acc: 0.9226 - val_loss: 0.2079 - val_acc: 0.9209\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3311 - acc: 0.9291 - val_loss: 0.2018 - val_acc: 0.9245\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3227 - acc: 0.9281 - val_loss: 0.2125 - val_acc: 0.9155\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3585 - acc: 0.9211 - val_loss: 0.3033 - val_acc: 0.8889\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3597 - acc: 0.9237 - val_loss: 0.3169 - val_acc: 0.8905\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3397 - acc: 0.9263 - val_loss: 0.3203 - val_acc: 0.8800\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3638 - acc: 0.9234 - val_loss: 0.2460 - val_acc: 0.9056\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3419 - acc: 0.9266 - val_loss: 0.2390 - val_acc: 0.9113\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2986 - acc: 0.9326 - val_loss: 0.2219 - val_acc: 0.9179\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2906 - acc: 0.9360 - val_loss: 0.2094 - val_acc: 0.9195\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2807 - acc: 0.9372 - val_loss: 0.2699 - val_acc: 0.9037\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2819 - acc: 0.9370 - val_loss: 0.2086 - val_acc: 0.9206\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2749 - acc: 0.9403 - val_loss: 0.2225 - val_acc: 0.9129\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3288 - acc: 0.9281 - val_loss: 0.2474 - val_acc: 0.9111\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2989 - acc: 0.9358 - val_loss: 0.1847 - val_acc: 0.9333\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3458 - acc: 0.9274 - val_loss: 0.2364 - val_acc: 0.9200\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2893 - acc: 0.9388 - val_loss: 0.2287 - val_acc: 0.9137\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2574 - acc: 0.9439 - val_loss: 0.2624 - val_acc: 0.9008\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3010 - acc: 0.9384 - val_loss: 0.2313 - val_acc: 0.9264\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2894 - acc: 0.9382 - val_loss: 0.2222 - val_acc: 0.9172\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3013 - acc: 0.9375 - val_loss: 0.2250 - val_acc: 0.9193\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2830 - acc: 0.9422 - val_loss: 0.1851 - val_acc: 0.9346\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2661 - acc: 0.9433 - val_loss: 0.1929 - val_acc: 0.9327\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2441 - acc: 0.9505 - val_loss: 0.1609 - val_acc: 0.9443\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2408 - acc: 0.9500 - val_loss: 0.1728 - val_acc: 0.9377\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2416 - acc: 0.9497 - val_loss: 0.2218 - val_acc: 0.9142\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2209 - acc: 0.9519 - val_loss: 0.1952 - val_acc: 0.9285\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2194 - acc: 0.9539 - val_loss: 0.1965 - val_acc: 0.9272\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2172 - acc: 0.9536 - val_loss: 0.2035 - val_acc: 0.9262\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2424 - acc: 0.9490 - val_loss: 0.4236 - val_acc: 0.8617\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3063 - acc: 0.9365 - val_loss: 0.2088 - val_acc: 0.9250\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2419 - acc: 0.9490 - val_loss: 0.2721 - val_acc: 0.9116\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2644 - acc: 0.9458 - val_loss: 0.2349 - val_acc: 0.9196\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2155 - acc: 0.9545 - val_loss: 0.2075 - val_acc: 0.9309\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2119 - acc: 0.9565 - val_loss: 0.1930 - val_acc: 0.9316\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2089 - acc: 0.9565 - val_loss: 0.1377 - val_acc: 0.9589\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2881 - acc: 0.9479 - val_loss: 0.2691 - val_acc: 0.9093\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2970 - acc: 0.9426 - val_loss: 0.1861 - val_acc: 0.9444\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2852 - acc: 0.9451 - val_loss: 0.1840 - val_acc: 0.9396\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2138 - acc: 0.9575 - val_loss: 0.1948 - val_acc: 0.9308\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1914 - acc: 0.9613 - val_loss: 0.1546 - val_acc: 0.9472\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1891 - acc: 0.9612 - val_loss: 0.1906 - val_acc: 0.9330\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1847 - acc: 0.9635 - val_loss: 0.1930 - val_acc: 0.9324\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1783 - acc: 0.9641 - val_loss: 0.1706 - val_acc: 0.9428\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1739 - acc: 0.9655 - val_loss: 0.1948 - val_acc: 0.9332\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1739 - acc: 0.9652 - val_loss: 0.1497 - val_acc: 0.9506\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1778 - acc: 0.9652 - val_loss: 0.1888 - val_acc: 0.9386\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1803 - acc: 0.9631 - val_loss: 0.1788 - val_acc: 0.9393\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1725 - acc: 0.9663 - val_loss: 0.1962 - val_acc: 0.9322\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2272 - acc: 0.9558 - val_loss: 0.3409 - val_acc: 0.9000\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2669 - acc: 0.9500 - val_loss: 0.1886 - val_acc: 0.9457\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3345 - acc: 0.9437 - val_loss: 0.2980 - val_acc: 0.9167\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2966 - acc: 0.9456 - val_loss: 0.2838 - val_acc: 0.9140\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2110 - acc: 0.9581 - val_loss: 0.1927 - val_acc: 0.9385\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1924 - acc: 0.9616 - val_loss: 0.2019 - val_acc: 0.9391\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1776 - acc: 0.9652 - val_loss: 0.1885 - val_acc: 0.9378\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1560 - acc: 0.9686 - val_loss: 0.1577 - val_acc: 0.9519\n",
      "Saving model...\n",
      "Fitting model # 20 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 103us/step - loss: 1.2941 - acc: 0.5950 - val_loss: 0.6824 - val_acc: 0.6306\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 1.1234 - acc: 0.6705 - val_loss: 0.6464 - val_acc: 0.6404\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.0576 - acc: 0.6911 - val_loss: 0.5026 - val_acc: 0.7599\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.0139 - acc: 0.7305 - val_loss: 0.5466 - val_acc: 0.7262\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.9781 - acc: 0.7473 - val_loss: 0.5441 - val_acc: 0.7304\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.9426 - acc: 0.7516 - val_loss: 0.4526 - val_acc: 0.8040\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.8911 - acc: 0.7896 - val_loss: 0.4404 - val_acc: 0.8230\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.8386 - acc: 0.8033 - val_loss: 0.4156 - val_acc: 0.8258\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.8069 - acc: 0.8135 - val_loss: 0.3913 - val_acc: 0.8456\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.7594 - acc: 0.8253 - val_loss: 0.4166 - val_acc: 0.8333\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.7105 - acc: 0.8406 - val_loss: 0.4053 - val_acc: 0.8366\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.6806 - acc: 0.8459 - val_loss: 0.3362 - val_acc: 0.8752\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6526 - acc: 0.8532 - val_loss: 0.5356 - val_acc: 0.7948\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6500 - acc: 0.8534 - val_loss: 0.3951 - val_acc: 0.8483\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6130 - acc: 0.8634 - val_loss: 0.3814 - val_acc: 0.8425\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5848 - acc: 0.8704 - val_loss: 0.3930 - val_acc: 0.8491\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5719 - acc: 0.8702 - val_loss: 0.2995 - val_acc: 0.8834\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5455 - acc: 0.8752 - val_loss: 0.3106 - val_acc: 0.8810\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5253 - acc: 0.8825 - val_loss: 0.2898 - val_acc: 0.8945\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5150 - acc: 0.8828 - val_loss: 0.3470 - val_acc: 0.8752\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4954 - acc: 0.8869 - val_loss: 0.3136 - val_acc: 0.8781\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4872 - acc: 0.8911 - val_loss: 0.3351 - val_acc: 0.8744\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4957 - acc: 0.8908 - val_loss: 0.2945 - val_acc: 0.8881\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4586 - acc: 0.8952 - val_loss: 0.3068 - val_acc: 0.8831\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4577 - acc: 0.9005 - val_loss: 0.4283 - val_acc: 0.8361\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.4771 - acc: 0.8898 - val_loss: 0.3580 - val_acc: 0.8911\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4425 - acc: 0.9032 - val_loss: 0.2430 - val_acc: 0.9124\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4302 - acc: 0.9048 - val_loss: 0.3120 - val_acc: 0.8863\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.4141 - acc: 0.9083 - val_loss: 0.3376 - val_acc: 0.8791\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4043 - acc: 0.9085 - val_loss: 0.2724 - val_acc: 0.8963\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3989 - acc: 0.9112 - val_loss: 0.2523 - val_acc: 0.9077\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3952 - acc: 0.9129 - val_loss: 0.3206 - val_acc: 0.8807\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3909 - acc: 0.9134 - val_loss: 0.2852 - val_acc: 0.8860\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3736 - acc: 0.9162 - val_loss: 0.3782 - val_acc: 0.8649\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3984 - acc: 0.9131 - val_loss: 0.2708 - val_acc: 0.8939\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3705 - acc: 0.9174 - val_loss: 0.2937 - val_acc: 0.8908\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3879 - acc: 0.9162 - val_loss: 0.2495 - val_acc: 0.9092\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3458 - acc: 0.9243 - val_loss: 0.2774 - val_acc: 0.8948\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3283 - acc: 0.9268 - val_loss: 0.2648 - val_acc: 0.9037\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3343 - acc: 0.9255 - val_loss: 0.2537 - val_acc: 0.9082\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3144 - acc: 0.9316 - val_loss: 0.2213 - val_acc: 0.9196\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3293 - acc: 0.9287 - val_loss: 0.2739 - val_acc: 0.9060\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3107 - acc: 0.9317 - val_loss: 0.2008 - val_acc: 0.9311\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 69us/step - loss: 0.3405 - acc: 0.9275 - val_loss: 0.3317 - val_acc: 0.8760\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.3158 - acc: 0.9311 - val_loss: 0.2905 - val_acc: 0.8982\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3206 - acc: 0.9300 - val_loss: 0.2268 - val_acc: 0.9196\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2901 - acc: 0.9369 - val_loss: 0.2093 - val_acc: 0.9277\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.3116 - acc: 0.9373 - val_loss: 0.3295 - val_acc: 0.8902\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2798 - acc: 0.9388 - val_loss: 0.2150 - val_acc: 0.9243\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 71us/step - loss: 0.2664 - acc: 0.9434 - val_loss: 0.2455 - val_acc: 0.9060\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2673 - acc: 0.9426 - val_loss: 0.2711 - val_acc: 0.8987\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2753 - acc: 0.9414 - val_loss: 0.2533 - val_acc: 0.9023\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2840 - acc: 0.9373 - val_loss: 0.2206 - val_acc: 0.9275\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2516 - acc: 0.9464 - val_loss: 0.2180 - val_acc: 0.9272\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2543 - acc: 0.9458 - val_loss: 0.2531 - val_acc: 0.9179\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2899 - acc: 0.9395 - val_loss: 0.3642 - val_acc: 0.8813\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2615 - acc: 0.9450 - val_loss: 0.2133 - val_acc: 0.9253\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2637 - acc: 0.9456 - val_loss: 0.2127 - val_acc: 0.9304\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2589 - acc: 0.9460 - val_loss: 0.2198 - val_acc: 0.9253\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2304 - acc: 0.9511 - val_loss: 0.2699 - val_acc: 0.9140\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2295 - acc: 0.9520 - val_loss: 0.1977 - val_acc: 0.9356\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2199 - acc: 0.9536 - val_loss: 0.2156 - val_acc: 0.9267\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2197 - acc: 0.9562 - val_loss: 0.2196 - val_acc: 0.9246\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2163 - acc: 0.9545 - val_loss: 0.2052 - val_acc: 0.9324\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2331 - acc: 0.9515 - val_loss: 0.2232 - val_acc: 0.9308\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2654 - acc: 0.9456 - val_loss: 0.2032 - val_acc: 0.9356\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2135 - acc: 0.9568 - val_loss: 0.2859 - val_acc: 0.9121\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2475 - acc: 0.9515 - val_loss: 0.2989 - val_acc: 0.9266\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2418 - acc: 0.9524 - val_loss: 0.2561 - val_acc: 0.9143\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.2049 - acc: 0.9572 - val_loss: 0.1837 - val_acc: 0.9473\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1916 - acc: 0.9633 - val_loss: 0.2326 - val_acc: 0.9248\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2034 - acc: 0.9595 - val_loss: 0.2112 - val_acc: 0.9304\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1815 - acc: 0.9630 - val_loss: 0.1962 - val_acc: 0.9398\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1785 - acc: 0.9661 - val_loss: 0.2119 - val_acc: 0.9324\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1791 - acc: 0.9639 - val_loss: 0.2046 - val_acc: 0.9356\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2569 - acc: 0.9479 - val_loss: 0.2947 - val_acc: 0.9227\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.2564 - acc: 0.9498 - val_loss: 0.3327 - val_acc: 0.9076\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.2615 - acc: 0.9495 - val_loss: 0.2559 - val_acc: 0.9243\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1991 - acc: 0.9625 - val_loss: 0.2356 - val_acc: 0.9353\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1816 - acc: 0.9653 - val_loss: 0.2602 - val_acc: 0.9217\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1692 - acc: 0.9664 - val_loss: 0.2107 - val_acc: 0.9377\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 59us/step - loss: 0.1588 - acc: 0.9693 - val_loss: 0.1804 - val_acc: 0.9491\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1574 - acc: 0.9695 - val_loss: 0.1893 - val_acc: 0.9448\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1504 - acc: 0.9724 - val_loss: 0.2093 - val_acc: 0.9345\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1839 - acc: 0.9640 - val_loss: 0.2211 - val_acc: 0.9359\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1962 - acc: 0.9618 - val_loss: 0.3488 - val_acc: 0.9047\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1782 - acc: 0.9651 - val_loss: 0.2136 - val_acc: 0.9390\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1538 - acc: 0.9712 - val_loss: 0.2067 - val_acc: 0.9401\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1491 - acc: 0.9734 - val_loss: 0.2129 - val_acc: 0.9374\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1487 - acc: 0.9708 - val_loss: 0.1956 - val_acc: 0.9443\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1588 - acc: 0.9709 - val_loss: 0.2512 - val_acc: 0.9216\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1752 - acc: 0.9686 - val_loss: 0.2230 - val_acc: 0.9345\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2309 - acc: 0.9556 - val_loss: 0.1958 - val_acc: 0.9478\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1863 - acc: 0.9622 - val_loss: 0.2702 - val_acc: 0.9245\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1619 - acc: 0.9704 - val_loss: 0.2765 - val_acc: 0.9203\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1674 - acc: 0.9687 - val_loss: 0.2983 - val_acc: 0.9248\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.1674 - acc: 0.9682 - val_loss: 0.2092 - val_acc: 0.9393\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.1465 - acc: 0.9736 - val_loss: 0.1840 - val_acc: 0.9498\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1317 - acc: 0.9770 - val_loss: 0.1931 - val_acc: 0.9480\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.1541 - acc: 0.9728 - val_loss: 0.1967 - val_acc: 0.9512\n",
      "Saving model...\n",
      "Fitting model # 21 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 102us/step - loss: 1.2874 - acc: 0.5806 - val_loss: 0.6132 - val_acc: 0.6665\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.1453 - acc: 0.6622 - val_loss: 0.6310 - val_acc: 0.6652\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.0935 - acc: 0.6960 - val_loss: 0.6374 - val_acc: 0.6823\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 1.0402 - acc: 0.7192 - val_loss: 0.5853 - val_acc: 0.7245\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 1.0049 - acc: 0.7404 - val_loss: 0.4826 - val_acc: 0.7971\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.9636 - acc: 0.7520 - val_loss: 0.5384 - val_acc: 0.7792\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.9290 - acc: 0.7634 - val_loss: 0.5310 - val_acc: 0.7528\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.8819 - acc: 0.7919 - val_loss: 0.5228 - val_acc: 0.7746\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.8645 - acc: 0.8029 - val_loss: 0.4552 - val_acc: 0.7998\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.8154 - acc: 0.8056 - val_loss: 0.4417 - val_acc: 0.8135\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.7828 - acc: 0.8217 - val_loss: 0.4350 - val_acc: 0.8291\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.7427 - acc: 0.8353 - val_loss: 0.4146 - val_acc: 0.8251\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.7157 - acc: 0.8343 - val_loss: 0.3679 - val_acc: 0.8560\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.6821 - acc: 0.8529 - val_loss: 0.4761 - val_acc: 0.8019\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.6773 - acc: 0.8494 - val_loss: 0.3717 - val_acc: 0.8486\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.6391 - acc: 0.8562 - val_loss: 0.3512 - val_acc: 0.8594\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6224 - acc: 0.8586 - val_loss: 0.4230 - val_acc: 0.8267\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5923 - acc: 0.8689 - val_loss: 0.3488 - val_acc: 0.8573\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5699 - acc: 0.8725 - val_loss: 0.3202 - val_acc: 0.8696\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5847 - acc: 0.8687 - val_loss: 0.4094 - val_acc: 0.8427\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.5627 - acc: 0.8731 - val_loss: 0.2867 - val_acc: 0.8881\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.5320 - acc: 0.8800 - val_loss: 0.3105 - val_acc: 0.8762\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.5348 - acc: 0.8763 - val_loss: 0.2676 - val_acc: 0.8953\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.5042 - acc: 0.8865 - val_loss: 0.2701 - val_acc: 0.8908\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4867 - acc: 0.8931 - val_loss: 0.2725 - val_acc: 0.8940\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4750 - acc: 0.8944 - val_loss: 0.4025 - val_acc: 0.8519\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4664 - acc: 0.8986 - val_loss: 0.2931 - val_acc: 0.8747\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4522 - acc: 0.8989 - val_loss: 0.2714 - val_acc: 0.8895\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.4550 - acc: 0.8969 - val_loss: 0.2674 - val_acc: 0.8928\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4311 - acc: 0.9033 - val_loss: 0.2757 - val_acc: 0.8908\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4330 - acc: 0.9050 - val_loss: 0.3208 - val_acc: 0.8702\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.4191 - acc: 0.9050 - val_loss: 0.2391 - val_acc: 0.9079\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.4030 - acc: 0.9119 - val_loss: 0.2396 - val_acc: 0.9029\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.3861 - acc: 0.9175 - val_loss: 0.3345 - val_acc: 0.8609\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3935 - acc: 0.9177 - val_loss: 0.2883 - val_acc: 0.8812\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4528 - acc: 0.9015 - val_loss: 0.3404 - val_acc: 0.8683\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3890 - acc: 0.9154 - val_loss: 0.2711 - val_acc: 0.8947\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3627 - acc: 0.9190 - val_loss: 0.2529 - val_acc: 0.9034\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3522 - acc: 0.9234 - val_loss: 0.2533 - val_acc: 0.8989\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.3401 - acc: 0.9263 - val_loss: 0.2558 - val_acc: 0.9000\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3403 - acc: 0.9255 - val_loss: 0.2445 - val_acc: 0.9092\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3378 - acc: 0.9255 - val_loss: 0.2096 - val_acc: 0.9211\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3428 - acc: 0.9261 - val_loss: 0.2792 - val_acc: 0.9023\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3395 - acc: 0.9273 - val_loss: 0.2983 - val_acc: 0.8924\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3114 - acc: 0.9339 - val_loss: 0.2838 - val_acc: 0.8952\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2964 - acc: 0.9373 - val_loss: 0.2402 - val_acc: 0.9164\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3008 - acc: 0.9355 - val_loss: 0.2431 - val_acc: 0.9084\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3521 - acc: 0.9280 - val_loss: 0.2318 - val_acc: 0.9148\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3127 - acc: 0.9347 - val_loss: 0.3410 - val_acc: 0.8866\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3237 - acc: 0.9312 - val_loss: 0.2204 - val_acc: 0.9193\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.2849 - acc: 0.9391 - val_loss: 0.2326 - val_acc: 0.9164\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2732 - acc: 0.9435 - val_loss: 0.2210 - val_acc: 0.9177\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2680 - acc: 0.9416 - val_loss: 0.2234 - val_acc: 0.9229\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2670 - acc: 0.9429 - val_loss: 0.1901 - val_acc: 0.9375\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2562 - acc: 0.9484 - val_loss: 0.2071 - val_acc: 0.9298\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2684 - acc: 0.9437 - val_loss: 0.1919 - val_acc: 0.9348\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3123 - acc: 0.9384 - val_loss: 0.3712 - val_acc: 0.8773\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3261 - acc: 0.9324 - val_loss: 0.2342 - val_acc: 0.9206\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2565 - acc: 0.9473 - val_loss: 0.2347 - val_acc: 0.9174\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2407 - acc: 0.9527 - val_loss: 0.2182 - val_acc: 0.9211\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2292 - acc: 0.9525 - val_loss: 0.2057 - val_acc: 0.9298\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 48us/step - loss: 0.2506 - acc: 0.9498 - val_loss: 0.2509 - val_acc: 0.9172\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2351 - acc: 0.9524 - val_loss: 0.1996 - val_acc: 0.9290\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2254 - acc: 0.9547 - val_loss: 0.2402 - val_acc: 0.9122\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2383 - acc: 0.9525 - val_loss: 0.1984 - val_acc: 0.9317\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2151 - acc: 0.9574 - val_loss: 0.2186 - val_acc: 0.9229\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2137 - acc: 0.9558 - val_loss: 0.1738 - val_acc: 0.9427\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2128 - acc: 0.9568 - val_loss: 0.2102 - val_acc: 0.9274\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2486 - acc: 0.9503 - val_loss: 0.1750 - val_acc: 0.9427\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2046 - acc: 0.9600 - val_loss: 0.2005 - val_acc: 0.9306\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.3156 - acc: 0.9421 - val_loss: 0.2331 - val_acc: 0.9237\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2869 - acc: 0.9412 - val_loss: 0.3195 - val_acc: 0.8973\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2399 - acc: 0.9533 - val_loss: 0.2344 - val_acc: 0.9179\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2054 - acc: 0.9595 - val_loss: 0.2064 - val_acc: 0.9311\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2030 - acc: 0.9602 - val_loss: 0.2051 - val_acc: 0.9345\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1948 - acc: 0.9607 - val_loss: 0.1851 - val_acc: 0.9467\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1849 - acc: 0.9649 - val_loss: 0.1973 - val_acc: 0.9380\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1792 - acc: 0.9661 - val_loss: 0.1834 - val_acc: 0.9385\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1756 - acc: 0.9677 - val_loss: 0.1967 - val_acc: 0.9364\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2086 - acc: 0.9616 - val_loss: 0.3647 - val_acc: 0.9171\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2632 - acc: 0.9490 - val_loss: 0.2052 - val_acc: 0.9440\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2285 - acc: 0.9546 - val_loss: 0.2211 - val_acc: 0.9262\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1877 - acc: 0.9639 - val_loss: 0.1909 - val_acc: 0.9391\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1684 - acc: 0.9689 - val_loss: 0.2025 - val_acc: 0.9317\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1585 - acc: 0.9697 - val_loss: 0.1790 - val_acc: 0.9469\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1659 - acc: 0.9695 - val_loss: 0.1893 - val_acc: 0.9377\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1752 - acc: 0.9676 - val_loss: 0.1963 - val_acc: 0.9367\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1612 - acc: 0.9702 - val_loss: 0.1726 - val_acc: 0.9454\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1588 - acc: 0.9700 - val_loss: 0.2209 - val_acc: 0.9309\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1570 - acc: 0.9717 - val_loss: 0.2090 - val_acc: 0.9341\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1646 - acc: 0.9686 - val_loss: 0.1803 - val_acc: 0.9486\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1582 - acc: 0.9691 - val_loss: 0.2166 - val_acc: 0.9378\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2043 - acc: 0.9600 - val_loss: 0.2104 - val_acc: 0.9377\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2122 - acc: 0.9596 - val_loss: 0.2608 - val_acc: 0.9217\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2656 - acc: 0.9509 - val_loss: 0.2125 - val_acc: 0.9375\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1779 - acc: 0.9676 - val_loss: 0.1838 - val_acc: 0.9502\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1695 - acc: 0.9678 - val_loss: 0.1842 - val_acc: 0.9509\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.1488 - acc: 0.9740 - val_loss: 0.1937 - val_acc: 0.9465\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1402 - acc: 0.9755 - val_loss: 0.2132 - val_acc: 0.9357\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1331 - acc: 0.9770 - val_loss: 0.1707 - val_acc: 0.9548\n",
      "Saving model...\n",
      "Fitting model # 22 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 105us/step - loss: 1.3087 - acc: 0.5915 - val_loss: 0.6600 - val_acc: 0.6267\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 1.1200 - acc: 0.6755 - val_loss: 0.6892 - val_acc: 0.6515\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 51us/step - loss: 1.0753 - acc: 0.6949 - val_loss: 0.5866 - val_acc: 0.7056\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 1.0307 - acc: 0.7234 - val_loss: 0.5670 - val_acc: 0.7177\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.9841 - acc: 0.7453 - val_loss: 0.5958 - val_acc: 0.7222\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.9484 - acc: 0.7665 - val_loss: 0.5756 - val_acc: 0.7446\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.9114 - acc: 0.7772 - val_loss: 0.4864 - val_acc: 0.7837\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.8785 - acc: 0.7878 - val_loss: 0.4318 - val_acc: 0.8262\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.8499 - acc: 0.8019 - val_loss: 0.5191 - val_acc: 0.7799\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.8239 - acc: 0.8133 - val_loss: 0.5385 - val_acc: 0.7686\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.7865 - acc: 0.8199 - val_loss: 0.4931 - val_acc: 0.7937\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.7527 - acc: 0.8297 - val_loss: 0.4501 - val_acc: 0.8087\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.7306 - acc: 0.8285 - val_loss: 0.4405 - val_acc: 0.8250\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.6893 - acc: 0.8465 - val_loss: 0.4354 - val_acc: 0.8240\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.6572 - acc: 0.8556 - val_loss: 0.3788 - val_acc: 0.8461\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.6634 - acc: 0.8521 - val_loss: 0.3924 - val_acc: 0.8502\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.6219 - acc: 0.8599 - val_loss: 0.3463 - val_acc: 0.8689\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.6094 - acc: 0.8680 - val_loss: 0.3732 - val_acc: 0.8707\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.5881 - acc: 0.8734 - val_loss: 0.4005 - val_acc: 0.8451\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.5727 - acc: 0.8705 - val_loss: 0.3650 - val_acc: 0.8601\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.5532 - acc: 0.8774 - val_loss: 0.3808 - val_acc: 0.8536\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.5292 - acc: 0.8820 - val_loss: 0.3955 - val_acc: 0.8570\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.5115 - acc: 0.8873 - val_loss: 0.3667 - val_acc: 0.8560\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.5000 - acc: 0.8886 - val_loss: 0.3280 - val_acc: 0.8731\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.4753 - acc: 0.8963 - val_loss: 0.3917 - val_acc: 0.8509\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.4666 - acc: 0.8940 - val_loss: 0.3194 - val_acc: 0.8873\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4783 - acc: 0.8954 - val_loss: 0.3615 - val_acc: 0.8612\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4692 - acc: 0.8956 - val_loss: 0.2591 - val_acc: 0.9140\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4526 - acc: 0.9000 - val_loss: 0.3305 - val_acc: 0.8767\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4315 - acc: 0.9041 - val_loss: 0.3403 - val_acc: 0.8705\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4390 - acc: 0.9005 - val_loss: 0.3264 - val_acc: 0.8807\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4266 - acc: 0.9072 - val_loss: 0.2972 - val_acc: 0.8876\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3965 - acc: 0.9112 - val_loss: 0.3400 - val_acc: 0.8767\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.4666 - acc: 0.9004 - val_loss: 0.3051 - val_acc: 0.8902\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3980 - acc: 0.9137 - val_loss: 0.3028 - val_acc: 0.8932\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.3710 - acc: 0.9188 - val_loss: 0.3025 - val_acc: 0.8963\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3668 - acc: 0.9204 - val_loss: 0.3511 - val_acc: 0.8739\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.4136 - acc: 0.9107 - val_loss: 0.2936 - val_acc: 0.8955\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3499 - acc: 0.9232 - val_loss: 0.3049 - val_acc: 0.8858\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3547 - acc: 0.9212 - val_loss: 0.3056 - val_acc: 0.8950\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3299 - acc: 0.9268 - val_loss: 0.3296 - val_acc: 0.8795\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3334 - acc: 0.9273 - val_loss: 0.2729 - val_acc: 0.9132\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3765 - acc: 0.9215 - val_loss: 0.2965 - val_acc: 0.9076\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3630 - acc: 0.9214 - val_loss: 0.3326 - val_acc: 0.8818\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3629 - acc: 0.9225 - val_loss: 0.3276 - val_acc: 0.8866\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3093 - acc: 0.9331 - val_loss: 0.2792 - val_acc: 0.9072\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2992 - acc: 0.9340 - val_loss: 0.2638 - val_acc: 0.9103\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2932 - acc: 0.9352 - val_loss: 0.2509 - val_acc: 0.9130\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2993 - acc: 0.9343 - val_loss: 0.3526 - val_acc: 0.8784\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2798 - acc: 0.9366 - val_loss: 0.2632 - val_acc: 0.9114\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2853 - acc: 0.9382 - val_loss: 0.2280 - val_acc: 0.9291\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2708 - acc: 0.9399 - val_loss: 0.2406 - val_acc: 0.9203\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2679 - acc: 0.9422 - val_loss: 0.4873 - val_acc: 0.8506\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3078 - acc: 0.9295 - val_loss: 0.2435 - val_acc: 0.9187\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2739 - acc: 0.9387 - val_loss: 0.3716 - val_acc: 0.8787\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2664 - acc: 0.9404 - val_loss: 0.3876 - val_acc: 0.8680\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2554 - acc: 0.9418 - val_loss: 0.2792 - val_acc: 0.9006\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2504 - acc: 0.9422 - val_loss: 0.2459 - val_acc: 0.9171\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3038 - acc: 0.9347 - val_loss: 0.3650 - val_acc: 0.8895\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2553 - acc: 0.9432 - val_loss: 0.3036 - val_acc: 0.9031\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2436 - acc: 0.9471 - val_loss: 0.2782 - val_acc: 0.9076\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2381 - acc: 0.9467 - val_loss: 0.3565 - val_acc: 0.8831\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2229 - acc: 0.9495 - val_loss: 0.2357 - val_acc: 0.9190\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2395 - acc: 0.9471 - val_loss: 0.2199 - val_acc: 0.9256\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2278 - acc: 0.9506 - val_loss: 0.2601 - val_acc: 0.9143\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2359 - acc: 0.9489 - val_loss: 0.2252 - val_acc: 0.9248\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3407 - acc: 0.9305 - val_loss: 0.2545 - val_acc: 0.9208\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2261 - acc: 0.9510 - val_loss: 0.2774 - val_acc: 0.9097\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2370 - acc: 0.9483 - val_loss: 0.2167 - val_acc: 0.9346\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2294 - acc: 0.9498 - val_loss: 0.2843 - val_acc: 0.9097\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1955 - acc: 0.9562 - val_loss: 0.2994 - val_acc: 0.9076\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1937 - acc: 0.9569 - val_loss: 0.2622 - val_acc: 0.9142\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1879 - acc: 0.9571 - val_loss: 0.1954 - val_acc: 0.9409\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1874 - acc: 0.9588 - val_loss: 0.2019 - val_acc: 0.9388\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1959 - acc: 0.9570 - val_loss: 0.2023 - val_acc: 0.9386\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1761 - acc: 0.9592 - val_loss: 0.2001 - val_acc: 0.9407\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1956 - acc: 0.9574 - val_loss: 0.2226 - val_acc: 0.9301\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2111 - acc: 0.9514 - val_loss: 0.2107 - val_acc: 0.9370\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2664 - acc: 0.9433 - val_loss: 0.2650 - val_acc: 0.9271\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2385 - acc: 0.9469 - val_loss: 0.2279 - val_acc: 0.9348\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1763 - acc: 0.9610 - val_loss: 0.2248 - val_acc: 0.9301\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1853 - acc: 0.9579 - val_loss: 0.1978 - val_acc: 0.9425\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1707 - acc: 0.9625 - val_loss: 0.2416 - val_acc: 0.9369\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1739 - acc: 0.9630 - val_loss: 0.2297 - val_acc: 0.9282\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1659 - acc: 0.9633 - val_loss: 0.4847 - val_acc: 0.8794\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2171 - acc: 0.9546 - val_loss: 0.3041 - val_acc: 0.9050\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1844 - acc: 0.9579 - val_loss: 0.2132 - val_acc: 0.9314\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1488 - acc: 0.9656 - val_loss: 0.1847 - val_acc: 0.9472\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1409 - acc: 0.9700 - val_loss: 0.2018 - val_acc: 0.9393\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1488 - acc: 0.9672 - val_loss: 0.2272 - val_acc: 0.9293\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.1696 - acc: 0.9607 - val_loss: 0.1993 - val_acc: 0.9490\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1700 - acc: 0.9636 - val_loss: 0.2492 - val_acc: 0.9324\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1557 - acc: 0.9666 - val_loss: 0.2389 - val_acc: 0.9306\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1473 - acc: 0.9662 - val_loss: 0.1933 - val_acc: 0.9448\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1340 - acc: 0.9706 - val_loss: 0.1984 - val_acc: 0.9425\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1680 - acc: 0.9629 - val_loss: 0.2816 - val_acc: 0.9187\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1798 - acc: 0.9604 - val_loss: 0.2585 - val_acc: 0.9316\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1385 - acc: 0.9684 - val_loss: 0.2049 - val_acc: 0.9435\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2149 - acc: 0.9579 - val_loss: 0.3670 - val_acc: 0.9138\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2131 - acc: 0.9569 - val_loss: 0.3124 - val_acc: 0.9196\n",
      "Saving model...\n",
      "Fitting model # 23 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 98us/step - loss: 1.3331 - acc: 0.5715 - val_loss: 0.6567 - val_acc: 0.6335\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 1.1435 - acc: 0.6774 - val_loss: 0.5597 - val_acc: 0.7147\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 1.0675 - acc: 0.7118 - val_loss: 0.5279 - val_acc: 0.7396\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 1.0125 - acc: 0.7384 - val_loss: 0.5345 - val_acc: 0.7670\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.9774 - acc: 0.7599 - val_loss: 0.6261 - val_acc: 0.6990\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.9254 - acc: 0.7712 - val_loss: 0.5400 - val_acc: 0.7530\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.8886 - acc: 0.7856 - val_loss: 0.4676 - val_acc: 0.7828\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 61us/step - loss: 0.8504 - acc: 0.7988 - val_loss: 0.5376 - val_acc: 0.7469\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.8450 - acc: 0.7990 - val_loss: 0.4900 - val_acc: 0.8072\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.7805 - acc: 0.8206 - val_loss: 0.3957 - val_acc: 0.8401\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.7729 - acc: 0.8250 - val_loss: 0.3839 - val_acc: 0.8535\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.7331 - acc: 0.8402 - val_loss: 0.4007 - val_acc: 0.8414\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.6963 - acc: 0.8412 - val_loss: 0.4133 - val_acc: 0.8335\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.6867 - acc: 0.8496 - val_loss: 0.4336 - val_acc: 0.8185\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.6638 - acc: 0.8511 - val_loss: 0.3414 - val_acc: 0.8602\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.6253 - acc: 0.8611 - val_loss: 0.3921 - val_acc: 0.8430\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.6126 - acc: 0.8620 - val_loss: 0.3917 - val_acc: 0.8481\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.5858 - acc: 0.8707 - val_loss: 0.3842 - val_acc: 0.8327\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.5887 - acc: 0.8679 - val_loss: 0.3301 - val_acc: 0.8671\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.5675 - acc: 0.8740 - val_loss: 0.3602 - val_acc: 0.8538\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.5538 - acc: 0.8772 - val_loss: 0.2988 - val_acc: 0.8795\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5414 - acc: 0.8840 - val_loss: 0.3055 - val_acc: 0.8815\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.5693 - acc: 0.8768 - val_loss: 0.4224 - val_acc: 0.8177\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.5184 - acc: 0.8840 - val_loss: 0.3226 - val_acc: 0.8744\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4991 - acc: 0.8907 - val_loss: 0.3578 - val_acc: 0.8551\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.4879 - acc: 0.8943 - val_loss: 0.2906 - val_acc: 0.8831\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4727 - acc: 0.8955 - val_loss: 0.3253 - val_acc: 0.8709\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4961 - acc: 0.8920 - val_loss: 0.3680 - val_acc: 0.8496\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4727 - acc: 0.8939 - val_loss: 0.2856 - val_acc: 0.8961\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4436 - acc: 0.9033 - val_loss: 0.3050 - val_acc: 0.8874\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4441 - acc: 0.9040 - val_loss: 0.2779 - val_acc: 0.8874\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4545 - acc: 0.8985 - val_loss: 0.2925 - val_acc: 0.8800\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.4308 - acc: 0.9050 - val_loss: 0.2493 - val_acc: 0.8981\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4350 - acc: 0.9075 - val_loss: 0.3444 - val_acc: 0.8665\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4862 - acc: 0.8945 - val_loss: 0.2774 - val_acc: 0.8936\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4054 - acc: 0.9134 - val_loss: 0.2423 - val_acc: 0.9003\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4014 - acc: 0.9136 - val_loss: 0.2796 - val_acc: 0.8868\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.4004 - acc: 0.9129 - val_loss: 0.2760 - val_acc: 0.8945\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 60us/step - loss: 0.3782 - acc: 0.9151 - val_loss: 0.3670 - val_acc: 0.8800\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 55us/step - loss: 0.3865 - acc: 0.9165 - val_loss: 0.2443 - val_acc: 0.9072\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3743 - acc: 0.9194 - val_loss: 0.2348 - val_acc: 0.9140\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3944 - acc: 0.9149 - val_loss: 0.3029 - val_acc: 0.8820\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3610 - acc: 0.9229 - val_loss: 0.2134 - val_acc: 0.9142\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.3419 - acc: 0.9244 - val_loss: 0.2164 - val_acc: 0.9124\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3412 - acc: 0.9274 - val_loss: 0.3009 - val_acc: 0.8781\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3393 - acc: 0.9253 - val_loss: 0.2752 - val_acc: 0.8990\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3323 - acc: 0.9270 - val_loss: 0.2173 - val_acc: 0.9155\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3352 - acc: 0.9262 - val_loss: 0.2415 - val_acc: 0.9056\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3459 - acc: 0.9249 - val_loss: 0.2577 - val_acc: 0.9053\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3460 - acc: 0.9289 - val_loss: 0.2369 - val_acc: 0.9140\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3321 - acc: 0.9263 - val_loss: 0.1899 - val_acc: 0.9298\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3282 - acc: 0.9324 - val_loss: 0.2355 - val_acc: 0.9108\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3999 - acc: 0.9197 - val_loss: 0.2998 - val_acc: 0.9013\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3334 - acc: 0.9279 - val_loss: 0.3124 - val_acc: 0.8842\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3214 - acc: 0.9332 - val_loss: 0.2330 - val_acc: 0.9195\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3256 - acc: 0.9322 - val_loss: 0.2308 - val_acc: 0.9163\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2830 - acc: 0.9383 - val_loss: 0.1922 - val_acc: 0.9290\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2735 - acc: 0.9430 - val_loss: 0.2145 - val_acc: 0.9171\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2880 - acc: 0.9377 - val_loss: 0.2800 - val_acc: 0.8995\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2857 - acc: 0.9405 - val_loss: 0.1799 - val_acc: 0.9361\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2689 - acc: 0.9439 - val_loss: 0.2221 - val_acc: 0.9113\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2616 - acc: 0.9461 - val_loss: 0.2075 - val_acc: 0.9208\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2620 - acc: 0.9445 - val_loss: 0.1898 - val_acc: 0.9359\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2872 - acc: 0.9385 - val_loss: 0.1813 - val_acc: 0.9353\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3000 - acc: 0.9384 - val_loss: 0.2371 - val_acc: 0.9188\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2611 - acc: 0.9443 - val_loss: 0.2202 - val_acc: 0.9179\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2439 - acc: 0.9489 - val_loss: 0.2107 - val_acc: 0.9214\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2417 - acc: 0.9499 - val_loss: 0.2059 - val_acc: 0.9201\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2749 - acc: 0.9419 - val_loss: 0.2267 - val_acc: 0.9238\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2900 - acc: 0.9435 - val_loss: 0.2041 - val_acc: 0.9264\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2529 - acc: 0.9476 - val_loss: 0.1741 - val_acc: 0.9383\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2419 - acc: 0.9485 - val_loss: 0.1728 - val_acc: 0.9383\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2545 - acc: 0.9486 - val_loss: 0.1855 - val_acc: 0.9357\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2402 - acc: 0.9513 - val_loss: 0.1736 - val_acc: 0.9372\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2255 - acc: 0.9546 - val_loss: 0.1915 - val_acc: 0.9330\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2322 - acc: 0.9520 - val_loss: 0.1865 - val_acc: 0.9356\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2489 - acc: 0.9517 - val_loss: 0.2015 - val_acc: 0.9298\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2369 - acc: 0.9502 - val_loss: 0.1833 - val_acc: 0.9364\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.2508 - acc: 0.9495 - val_loss: 0.1864 - val_acc: 0.9374\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2295 - acc: 0.9549 - val_loss: 0.1911 - val_acc: 0.9351\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2115 - acc: 0.9587 - val_loss: 0.1568 - val_acc: 0.9498\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2003 - acc: 0.9605 - val_loss: 0.1703 - val_acc: 0.9444\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1968 - acc: 0.9610 - val_loss: 0.1897 - val_acc: 0.9335\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1940 - acc: 0.9625 - val_loss: 0.2211 - val_acc: 0.9245\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2227 - acc: 0.9549 - val_loss: 0.1912 - val_acc: 0.9370\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3301 - acc: 0.9405 - val_loss: 0.5635 - val_acc: 0.8643\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.3128 - acc: 0.9395 - val_loss: 0.1970 - val_acc: 0.9324\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2390 - acc: 0.9530 - val_loss: 0.1888 - val_acc: 0.9374\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2127 - acc: 0.9597 - val_loss: 0.1719 - val_acc: 0.9494\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.2129 - acc: 0.9590 - val_loss: 0.1937 - val_acc: 0.9412\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1961 - acc: 0.9614 - val_loss: 0.1731 - val_acc: 0.9448\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1930 - acc: 0.9624 - val_loss: 0.1907 - val_acc: 0.9375\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1772 - acc: 0.9652 - val_loss: 0.2223 - val_acc: 0.9277\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1717 - acc: 0.9656 - val_loss: 0.1641 - val_acc: 0.9494\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1734 - acc: 0.9684 - val_loss: 0.1944 - val_acc: 0.9346\n",
      "Epoch 96/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1800 - acc: 0.9640 - val_loss: 0.2741 - val_acc: 0.9147\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 54us/step - loss: 0.1752 - acc: 0.9667 - val_loss: 0.1506 - val_acc: 0.9572\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1640 - acc: 0.9685 - val_loss: 0.1960 - val_acc: 0.9390\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1952 - acc: 0.9628 - val_loss: 0.2208 - val_acc: 0.9304\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1702 - acc: 0.9663 - val_loss: 0.1454 - val_acc: 0.9591\n",
      "Saving model...\n",
      "Fitting model # 24 ...\n",
      "Train on 18628 samples, validate on 6210 samples\n",
      "Epoch 1/100\n",
      "18628/18628 [==============================] - 2s 107us/step - loss: 1.2474 - acc: 0.6011 - val_loss: 0.5759 - val_acc: 0.6863\n",
      "Epoch 2/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 1.1188 - acc: 0.6776 - val_loss: 0.5323 - val_acc: 0.7114\n",
      "Epoch 3/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 1.0655 - acc: 0.6944 - val_loss: 0.5617 - val_acc: 0.7329\n",
      "Epoch 4/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 1.0202 - acc: 0.7341 - val_loss: 0.6307 - val_acc: 0.6404\n",
      "Epoch 5/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.9701 - acc: 0.7373 - val_loss: 0.4691 - val_acc: 0.7736\n",
      "Epoch 6/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.9394 - acc: 0.7623 - val_loss: 0.4904 - val_acc: 0.7821\n",
      "Epoch 7/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.8919 - acc: 0.7799 - val_loss: 0.5063 - val_acc: 0.7506\n",
      "Epoch 8/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.8651 - acc: 0.7857 - val_loss: 0.4843 - val_acc: 0.7773\n",
      "Epoch 9/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.8322 - acc: 0.8033 - val_loss: 0.5180 - val_acc: 0.7804\n",
      "Epoch 10/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.8090 - acc: 0.8151 - val_loss: 0.5423 - val_acc: 0.7501\n",
      "Epoch 11/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.7652 - acc: 0.8221 - val_loss: 0.4000 - val_acc: 0.8324\n",
      "Epoch 12/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.7257 - acc: 0.8369 - val_loss: 0.3720 - val_acc: 0.8501\n",
      "Epoch 13/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.6847 - acc: 0.8500 - val_loss: 0.4395 - val_acc: 0.8119\n",
      "Epoch 14/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.6606 - acc: 0.8524 - val_loss: 0.3922 - val_acc: 0.8398\n",
      "Epoch 15/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.6522 - acc: 0.8578 - val_loss: 0.3522 - val_acc: 0.8548\n",
      "Epoch 16/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.6168 - acc: 0.8629 - val_loss: 0.3129 - val_acc: 0.8760\n",
      "Epoch 17/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.5921 - acc: 0.8724 - val_loss: 0.3612 - val_acc: 0.8562\n",
      "Epoch 18/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.5715 - acc: 0.8775 - val_loss: 0.3564 - val_acc: 0.8496\n",
      "Epoch 19/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.5580 - acc: 0.8761 - val_loss: 0.3625 - val_acc: 0.8519\n",
      "Epoch 20/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.5307 - acc: 0.8835 - val_loss: 0.3470 - val_acc: 0.8626\n",
      "Epoch 21/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.5432 - acc: 0.8804 - val_loss: 0.2799 - val_acc: 0.9031\n",
      "Epoch 22/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.5086 - acc: 0.8941 - val_loss: 0.3029 - val_acc: 0.8839\n",
      "Epoch 23/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4874 - acc: 0.8972 - val_loss: 0.3140 - val_acc: 0.8736\n",
      "Epoch 24/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.5037 - acc: 0.8931 - val_loss: 0.3613 - val_acc: 0.8517\n",
      "Epoch 25/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.5003 - acc: 0.8909 - val_loss: 0.2883 - val_acc: 0.8953\n",
      "Epoch 26/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4621 - acc: 0.9012 - val_loss: 0.3631 - val_acc: 0.8564\n",
      "Epoch 27/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4502 - acc: 0.9008 - val_loss: 0.2719 - val_acc: 0.9005\n",
      "Epoch 28/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.4334 - acc: 0.9102 - val_loss: 0.3842 - val_acc: 0.8504\n",
      "Epoch 29/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4194 - acc: 0.9094 - val_loss: 0.2497 - val_acc: 0.9045\n",
      "Epoch 30/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4110 - acc: 0.9105 - val_loss: 0.3037 - val_acc: 0.8878\n",
      "Epoch 31/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.4125 - acc: 0.9102 - val_loss: 0.2274 - val_acc: 0.9179\n",
      "Epoch 32/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4187 - acc: 0.9120 - val_loss: 0.2656 - val_acc: 0.9013\n",
      "Epoch 33/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.4099 - acc: 0.9150 - val_loss: 0.2575 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.3991 - acc: 0.9145 - val_loss: 0.2350 - val_acc: 0.9184\n",
      "Epoch 35/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3806 - acc: 0.9181 - val_loss: 0.2425 - val_acc: 0.9074\n",
      "Epoch 36/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3659 - acc: 0.9233 - val_loss: 0.3244 - val_acc: 0.8655\n",
      "Epoch 37/100\n",
      "18628/18628 [==============================] - 1s 58us/step - loss: 0.3632 - acc: 0.9210 - val_loss: 0.2334 - val_acc: 0.9190\n",
      "Epoch 38/100\n",
      "18628/18628 [==============================] - 1s 57us/step - loss: 0.3723 - acc: 0.9209 - val_loss: 0.2744 - val_acc: 0.8948\n",
      "Epoch 39/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3581 - acc: 0.9223 - val_loss: 0.2531 - val_acc: 0.8989\n",
      "Epoch 40/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3380 - acc: 0.9274 - val_loss: 0.2708 - val_acc: 0.9024\n",
      "Epoch 41/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.3447 - acc: 0.9276 - val_loss: 0.2328 - val_acc: 0.9156\n",
      "Epoch 42/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3368 - acc: 0.9306 - val_loss: 0.2567 - val_acc: 0.8979\n",
      "Epoch 43/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3303 - acc: 0.9283 - val_loss: 0.2584 - val_acc: 0.9159\n",
      "Epoch 44/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3171 - acc: 0.9355 - val_loss: 0.2251 - val_acc: 0.9205\n",
      "Epoch 45/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3079 - acc: 0.9364 - val_loss: 0.2211 - val_acc: 0.9177\n",
      "Epoch 46/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3129 - acc: 0.9320 - val_loss: 0.2538 - val_acc: 0.9106\n",
      "Epoch 47/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.3455 - acc: 0.9297 - val_loss: 0.2934 - val_acc: 0.9013\n",
      "Epoch 48/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3048 - acc: 0.9377 - val_loss: 0.3241 - val_acc: 0.8747\n",
      "Epoch 49/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.3087 - acc: 0.9347 - val_loss: 0.2142 - val_acc: 0.9245\n",
      "Epoch 50/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2772 - acc: 0.9434 - val_loss: 0.2186 - val_acc: 0.9245\n",
      "Epoch 51/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2867 - acc: 0.9419 - val_loss: 0.2672 - val_acc: 0.9056\n",
      "Epoch 52/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2624 - acc: 0.9446 - val_loss: 0.2309 - val_acc: 0.9153\n",
      "Epoch 53/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2658 - acc: 0.9435 - val_loss: 0.1934 - val_acc: 0.9317\n",
      "Epoch 54/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2575 - acc: 0.9480 - val_loss: 0.2657 - val_acc: 0.9023\n",
      "Epoch 55/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2687 - acc: 0.9418 - val_loss: 0.1817 - val_acc: 0.9367\n",
      "Epoch 56/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2527 - acc: 0.9501 - val_loss: 0.1921 - val_acc: 0.9367\n",
      "Epoch 57/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2601 - acc: 0.9475 - val_loss: 0.1942 - val_acc: 0.9338\n",
      "Epoch 58/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2858 - acc: 0.9421 - val_loss: 0.2244 - val_acc: 0.9169\n",
      "Epoch 59/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2772 - acc: 0.9430 - val_loss: 0.2470 - val_acc: 0.9185\n",
      "Epoch 60/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2383 - acc: 0.9515 - val_loss: 0.2619 - val_acc: 0.9014\n",
      "Epoch 61/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2246 - acc: 0.9532 - val_loss: 0.1948 - val_acc: 0.9366\n",
      "Epoch 62/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2307 - acc: 0.9562 - val_loss: 0.2301 - val_acc: 0.9201\n",
      "Epoch 63/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2722 - acc: 0.9440 - val_loss: 0.2272 - val_acc: 0.9291\n",
      "Epoch 64/100\n",
      "18628/18628 [==============================] - 1s 53us/step - loss: 0.2518 - acc: 0.9487 - val_loss: 0.2084 - val_acc: 0.9291\n",
      "Epoch 65/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2257 - acc: 0.9559 - val_loss: 0.2291 - val_acc: 0.9201\n",
      "Epoch 66/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2123 - acc: 0.9576 - val_loss: 0.1966 - val_acc: 0.9295\n",
      "Epoch 67/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2040 - acc: 0.9600 - val_loss: 0.1936 - val_acc: 0.9353\n",
      "Epoch 68/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2138 - acc: 0.9570 - val_loss: 0.2053 - val_acc: 0.9349\n",
      "Epoch 69/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2057 - acc: 0.9594 - val_loss: 0.1898 - val_acc: 0.9370\n",
      "Epoch 70/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2106 - acc: 0.9593 - val_loss: 0.2192 - val_acc: 0.9367\n",
      "Epoch 71/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2266 - acc: 0.9559 - val_loss: 0.1814 - val_acc: 0.9444\n",
      "Epoch 72/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2242 - acc: 0.9567 - val_loss: 0.2384 - val_acc: 0.9259\n",
      "Epoch 73/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2208 - acc: 0.9575 - val_loss: 0.3126 - val_acc: 0.9018\n",
      "Epoch 74/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.2080 - acc: 0.9578 - val_loss: 0.1987 - val_acc: 0.9380\n",
      "Epoch 75/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1759 - acc: 0.9660 - val_loss: 0.2038 - val_acc: 0.9356\n",
      "Epoch 76/100\n",
      "18628/18628 [==============================] - 1s 56us/step - loss: 0.1766 - acc: 0.9666 - val_loss: 0.1846 - val_acc: 0.9412\n",
      "Epoch 77/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1759 - acc: 0.9651 - val_loss: 0.1735 - val_acc: 0.9506\n",
      "Epoch 78/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2272 - acc: 0.9577 - val_loss: 0.2506 - val_acc: 0.9245\n",
      "Epoch 79/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.2494 - acc: 0.9551 - val_loss: 0.2897 - val_acc: 0.9153\n",
      "Epoch 80/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2533 - acc: 0.9508 - val_loss: 0.2039 - val_acc: 0.9386\n",
      "Epoch 81/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1910 - acc: 0.9638 - val_loss: 0.2096 - val_acc: 0.9407\n",
      "Epoch 82/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1746 - acc: 0.9682 - val_loss: 0.1926 - val_acc: 0.9377\n",
      "Epoch 83/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1770 - acc: 0.9668 - val_loss: 0.2122 - val_acc: 0.9308\n",
      "Epoch 84/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1551 - acc: 0.9713 - val_loss: 0.2156 - val_acc: 0.9301\n",
      "Epoch 85/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1502 - acc: 0.9733 - val_loss: 0.1950 - val_acc: 0.9382\n",
      "Epoch 86/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1503 - acc: 0.9736 - val_loss: 0.2439 - val_acc: 0.9193\n",
      "Epoch 87/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1805 - acc: 0.9650 - val_loss: 0.1964 - val_acc: 0.9391\n",
      "Epoch 88/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1520 - acc: 0.9722 - val_loss: 0.2187 - val_acc: 0.9319\n",
      "Epoch 89/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1898 - acc: 0.9629 - val_loss: 0.2133 - val_acc: 0.9320\n",
      "Epoch 90/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1541 - acc: 0.9712 - val_loss: 0.1993 - val_acc: 0.9401\n",
      "Epoch 91/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1452 - acc: 0.9748 - val_loss: 0.2007 - val_acc: 0.9372\n",
      "Epoch 92/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1825 - acc: 0.9644 - val_loss: 0.1956 - val_acc: 0.9417\n",
      "Epoch 93/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.1551 - acc: 0.9714 - val_loss: 0.2086 - val_acc: 0.9345\n",
      "Epoch 94/100\n",
      "18628/18628 [==============================] - 1s 52us/step - loss: 0.1364 - acc: 0.9765 - val_loss: 0.1908 - val_acc: 0.9430\n",
      "Epoch 95/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1350 - acc: 0.9774 - val_loss: 0.2135 - val_acc: 0.9317\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1860 - acc: 0.9688 - val_loss: 0.1973 - val_acc: 0.9475\n",
      "Epoch 97/100\n",
      "18628/18628 [==============================] - 1s 49us/step - loss: 0.1976 - acc: 0.9632 - val_loss: 0.2828 - val_acc: 0.9330\n",
      "Epoch 98/100\n",
      "18628/18628 [==============================] - 1s 51us/step - loss: 0.1992 - acc: 0.9647 - val_loss: 0.4333 - val_acc: 0.8961\n",
      "Epoch 99/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2613 - acc: 0.9554 - val_loss: 0.2826 - val_acc: 0.9217\n",
      "Epoch 100/100\n",
      "18628/18628 [==============================] - 1s 50us/step - loss: 0.2072 - acc: 0.9642 - val_loss: 0.2198 - val_acc: 0.9361\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "#Fit 24 simultaenous single layer models\n",
    "\n",
    "for i in range(1,25):\n",
    "    #Resample subsample of training data\n",
    "    X_train_temp, X_val_temp, y_train_temp, y_val_temp = train_test_split(X_train, y_train, test_size = 0.25, random_state = 23+i)\n",
    "    y_train_temp = np.array(keras.utils.to_categorical(y_train_temp, 2))\n",
    "    y_val_temp = np.array(keras.utils.to_categorical(y_val_temp, 2))\n",
    "    \n",
    "    #Model parameters same as above single layer model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation = 'relu', input_shape=(64,)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    class_weight = {0: 1., 1: 25.}\n",
    "    print(\"Fitting model #\", i, \"...\")\n",
    "    model.fit(X_train_temp, y_train_temp, validation_data=(X_val_temp, y_val_temp), epochs=100, batch_size=100, class_weight = class_weight, verbose=1)\n",
    "    print(\"Saving model...\")\n",
    "    #Save model sequentially\n",
    "    model.save('NN_models/NN_single{0}.h5'.format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.zeros(np.shape(y_train))\n",
    "y_val_pred = np.zeros(np.shape(y_val))\n",
    "\n",
    "#Reload each model and predict - output is a probability\n",
    "for i in range(1,25):\n",
    "    model = load_model('NN_models/NN_single{0}.h5'.format(i))\n",
    "    train_temp = model.predict(X_train)\n",
    "    val_temp = model.predict(X_val)\n",
    "    y_train_pred += train_temp[:,1]\n",
    "    y_val_pred += val_temp[:,1]\n",
    "    \n",
    "y_val_pred = y_val_pred / 24\n",
    "y_train_pred = y_train_pred / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.998182920879683\n",
      "The validation accuracy is 0.9534682741116752\n",
      "Training Set Confusion Mat.:\n",
      "[[23209   472]\n",
      " [    5  1152]]\n",
      "Precision:  0.7093596059113301\n",
      "Recall:  0.9956784788245462\n",
      "Valid Set Confusion Mat.:\n",
      "[[7635  245]\n",
      " [ 100  300]]\n",
      "Precision:  0.5504587155963303\n",
      "Recall:  0.75\n"
     ]
    }
   ],
   "source": [
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKLearn Gradient Boost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2898            3.92m\n",
      "         2           0.2619            4.47m\n",
      "         3           0.2397            4.73m\n",
      "         4           0.2218            4.90m\n",
      "         5           0.2057            5.08m\n",
      "         6           0.1921            5.15m\n",
      "         7           0.1799            5.29m\n",
      "         8           0.1690            5.36m\n",
      "         9           0.1590            5.43m\n",
      "        10           0.1498            5.48m\n",
      "        20           0.0875            5.52m\n",
      "        30           0.0556            5.26m\n",
      "        40           0.0405            4.87m\n",
      "        50           0.0320            4.48m\n",
      "        60           0.0261            4.11m\n",
      "        70           0.0221            3.76m\n",
      "        80           0.0175            3.46m\n",
      "        90           0.0139            3.16m\n",
      "       100           0.0109            2.87m\n",
      "       200           0.0016            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=24,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=42, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=23, subsample=1.0, verbose=1,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = GradientBoostingClassifier(max_depth=24, min_samples_leaf=42, min_samples_split=2, n_estimators = 200,\n",
    "                                   random_state = 23, verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF_models/SKL_GB.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store model\n",
    "joblib.dump(model, 'RF_models/SKL_GB.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 1.0\n",
      "The validation accuracy is 0.7727982233502537\n",
      "Training Set Confusion Mat.:\n",
      "[[23681     0]\n",
      " [    0  1157]]\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "Valid Set Confusion Mat.:\n",
      "[[7865   15]\n",
      " [ 181  219]]\n",
      "Precision:  0.9358974358974359\n",
      "Recall:  0.5475\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model = joblib.load('RF_models/SKL_GB.pkl')\n",
    "#Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.5\n",
      "Will train until validation_0-auc hasn't improved in 20 rounds.\n",
      "[1]\tvalidation_0-auc:0.5\n",
      "[2]\tvalidation_0-auc:0.5\n",
      "[3]\tvalidation_0-auc:0.5\n",
      "[4]\tvalidation_0-auc:0.5\n",
      "[5]\tvalidation_0-auc:0.5\n",
      "[6]\tvalidation_0-auc:0.5\n",
      "[7]\tvalidation_0-auc:0.5\n",
      "[8]\tvalidation_0-auc:0.5\n",
      "[9]\tvalidation_0-auc:0.5\n",
      "[10]\tvalidation_0-auc:0.5\n",
      "[11]\tvalidation_0-auc:0.5\n",
      "[12]\tvalidation_0-auc:0.5\n",
      "[13]\tvalidation_0-auc:0.5\n",
      "[14]\tvalidation_0-auc:0.655648\n",
      "[15]\tvalidation_0-auc:0.655813\n",
      "[16]\tvalidation_0-auc:0.655846\n",
      "[17]\tvalidation_0-auc:0.655848\n",
      "[18]\tvalidation_0-auc:0.655851\n",
      "[19]\tvalidation_0-auc:0.656324\n",
      "[20]\tvalidation_0-auc:0.656325\n",
      "[21]\tvalidation_0-auc:0.660138\n",
      "[22]\tvalidation_0-auc:0.660166\n",
      "[23]\tvalidation_0-auc:0.678601\n",
      "[24]\tvalidation_0-auc:0.679069\n",
      "[25]\tvalidation_0-auc:0.679111\n",
      "[26]\tvalidation_0-auc:0.679559\n",
      "[27]\tvalidation_0-auc:0.776414\n",
      "[28]\tvalidation_0-auc:0.791856\n",
      "[29]\tvalidation_0-auc:0.794167\n",
      "[30]\tvalidation_0-auc:0.802725\n",
      "[31]\tvalidation_0-auc:0.80363\n",
      "[32]\tvalidation_0-auc:0.804387\n",
      "[33]\tvalidation_0-auc:0.806669\n",
      "[34]\tvalidation_0-auc:0.807061\n",
      "[35]\tvalidation_0-auc:0.807555\n",
      "[36]\tvalidation_0-auc:0.809217\n",
      "[37]\tvalidation_0-auc:0.813684\n",
      "[38]\tvalidation_0-auc:0.814255\n",
      "[39]\tvalidation_0-auc:0.826039\n",
      "[40]\tvalidation_0-auc:0.826353\n",
      "[41]\tvalidation_0-auc:0.827642\n",
      "[42]\tvalidation_0-auc:0.831822\n",
      "[43]\tvalidation_0-auc:0.865891\n",
      "[44]\tvalidation_0-auc:0.872666\n",
      "[45]\tvalidation_0-auc:0.881055\n",
      "[46]\tvalidation_0-auc:0.89128\n",
      "[47]\tvalidation_0-auc:0.894634\n",
      "[48]\tvalidation_0-auc:0.901322\n",
      "[49]\tvalidation_0-auc:0.903962\n",
      "[50]\tvalidation_0-auc:0.908706\n",
      "[51]\tvalidation_0-auc:0.911757\n",
      "[52]\tvalidation_0-auc:0.911958\n",
      "[53]\tvalidation_0-auc:0.916384\n",
      "[54]\tvalidation_0-auc:0.917524\n",
      "[55]\tvalidation_0-auc:0.918401\n",
      "[56]\tvalidation_0-auc:0.917348\n",
      "[57]\tvalidation_0-auc:0.920652\n",
      "[58]\tvalidation_0-auc:0.920486\n",
      "[59]\tvalidation_0-auc:0.921569\n",
      "[60]\tvalidation_0-auc:0.922459\n",
      "[61]\tvalidation_0-auc:0.922745\n",
      "[62]\tvalidation_0-auc:0.926109\n",
      "[63]\tvalidation_0-auc:0.927125\n",
      "[64]\tvalidation_0-auc:0.928966\n",
      "[65]\tvalidation_0-auc:0.930071\n",
      "[66]\tvalidation_0-auc:0.93151\n",
      "[67]\tvalidation_0-auc:0.932697\n",
      "[68]\tvalidation_0-auc:0.933443\n",
      "[69]\tvalidation_0-auc:0.934092\n",
      "[70]\tvalidation_0-auc:0.935007\n",
      "[71]\tvalidation_0-auc:0.935357\n",
      "[72]\tvalidation_0-auc:0.936106\n",
      "[73]\tvalidation_0-auc:0.937143\n",
      "[74]\tvalidation_0-auc:0.937856\n",
      "[75]\tvalidation_0-auc:0.937739\n",
      "[76]\tvalidation_0-auc:0.937941\n",
      "[77]\tvalidation_0-auc:0.938974\n",
      "[78]\tvalidation_0-auc:0.940341\n",
      "[79]\tvalidation_0-auc:0.940578\n",
      "[80]\tvalidation_0-auc:0.94128\n",
      "[81]\tvalidation_0-auc:0.942692\n",
      "[82]\tvalidation_0-auc:0.94276\n",
      "[83]\tvalidation_0-auc:0.942799\n",
      "[84]\tvalidation_0-auc:0.943189\n",
      "[85]\tvalidation_0-auc:0.943974\n",
      "[86]\tvalidation_0-auc:0.944549\n",
      "[87]\tvalidation_0-auc:0.946297\n",
      "[88]\tvalidation_0-auc:0.946563\n",
      "[89]\tvalidation_0-auc:0.947087\n",
      "[90]\tvalidation_0-auc:0.947234\n",
      "[91]\tvalidation_0-auc:0.94791\n",
      "[92]\tvalidation_0-auc:0.948621\n",
      "[93]\tvalidation_0-auc:0.948919\n",
      "[94]\tvalidation_0-auc:0.949296\n",
      "[95]\tvalidation_0-auc:0.950347\n",
      "[96]\tvalidation_0-auc:0.950848\n",
      "[97]\tvalidation_0-auc:0.951544\n",
      "[98]\tvalidation_0-auc:0.951939\n",
      "[99]\tvalidation_0-auc:0.951959\n",
      "[100]\tvalidation_0-auc:0.952398\n",
      "[101]\tvalidation_0-auc:0.952501\n",
      "[102]\tvalidation_0-auc:0.952963\n",
      "[103]\tvalidation_0-auc:0.953086\n",
      "[104]\tvalidation_0-auc:0.953328\n",
      "[105]\tvalidation_0-auc:0.953604\n",
      "[106]\tvalidation_0-auc:0.954103\n",
      "[107]\tvalidation_0-auc:0.954417\n",
      "[108]\tvalidation_0-auc:0.95507\n",
      "[109]\tvalidation_0-auc:0.955236\n",
      "[110]\tvalidation_0-auc:0.955411\n",
      "[111]\tvalidation_0-auc:0.955486\n",
      "[112]\tvalidation_0-auc:0.956007\n",
      "[113]\tvalidation_0-auc:0.956224\n",
      "[114]\tvalidation_0-auc:0.95651\n",
      "[115]\tvalidation_0-auc:0.957121\n",
      "[116]\tvalidation_0-auc:0.957341\n",
      "[117]\tvalidation_0-auc:0.957517\n",
      "[118]\tvalidation_0-auc:0.958097\n",
      "[119]\tvalidation_0-auc:0.958474\n",
      "[120]\tvalidation_0-auc:0.958753\n",
      "[121]\tvalidation_0-auc:0.958767\n",
      "[122]\tvalidation_0-auc:0.958871\n",
      "[123]\tvalidation_0-auc:0.95878\n",
      "[124]\tvalidation_0-auc:0.959061\n",
      "[125]\tvalidation_0-auc:0.959416\n",
      "[126]\tvalidation_0-auc:0.9595\n",
      "[127]\tvalidation_0-auc:0.959791\n",
      "[128]\tvalidation_0-auc:0.959957\n",
      "[129]\tvalidation_0-auc:0.960391\n",
      "[130]\tvalidation_0-auc:0.960546\n",
      "[131]\tvalidation_0-auc:0.960723\n",
      "[132]\tvalidation_0-auc:0.960549\n",
      "[133]\tvalidation_0-auc:0.960611\n",
      "[134]\tvalidation_0-auc:0.960604\n",
      "[135]\tvalidation_0-auc:0.960577\n",
      "[136]\tvalidation_0-auc:0.960628\n",
      "[137]\tvalidation_0-auc:0.960793\n",
      "[138]\tvalidation_0-auc:0.961035\n",
      "[139]\tvalidation_0-auc:0.960917\n",
      "[140]\tvalidation_0-auc:0.961148\n",
      "[141]\tvalidation_0-auc:0.961576\n",
      "[142]\tvalidation_0-auc:0.961573\n",
      "[143]\tvalidation_0-auc:0.961714\n",
      "[144]\tvalidation_0-auc:0.961694\n",
      "[145]\tvalidation_0-auc:0.961834\n",
      "[146]\tvalidation_0-auc:0.961848\n",
      "[147]\tvalidation_0-auc:0.961705\n",
      "[148]\tvalidation_0-auc:0.961776\n",
      "[149]\tvalidation_0-auc:0.962377\n",
      "[150]\tvalidation_0-auc:0.962375\n",
      "[151]\tvalidation_0-auc:0.962431\n",
      "[152]\tvalidation_0-auc:0.962424\n",
      "[153]\tvalidation_0-auc:0.962416\n",
      "[154]\tvalidation_0-auc:0.962708\n",
      "[155]\tvalidation_0-auc:0.962727\n",
      "[156]\tvalidation_0-auc:0.962728\n",
      "[157]\tvalidation_0-auc:0.963073\n",
      "[158]\tvalidation_0-auc:0.963226\n",
      "[159]\tvalidation_0-auc:0.963162\n",
      "[160]\tvalidation_0-auc:0.963118\n",
      "[161]\tvalidation_0-auc:0.963301\n",
      "[162]\tvalidation_0-auc:0.96326\n",
      "[163]\tvalidation_0-auc:0.963312\n",
      "[164]\tvalidation_0-auc:0.963249\n",
      "[165]\tvalidation_0-auc:0.963441\n",
      "[166]\tvalidation_0-auc:0.963788\n",
      "[167]\tvalidation_0-auc:0.963865\n",
      "[168]\tvalidation_0-auc:0.964096\n",
      "[169]\tvalidation_0-auc:0.964219\n",
      "[170]\tvalidation_0-auc:0.964168\n",
      "[171]\tvalidation_0-auc:0.964176\n",
      "[172]\tvalidation_0-auc:0.964283\n",
      "[173]\tvalidation_0-auc:0.964285\n",
      "[174]\tvalidation_0-auc:0.964209\n",
      "[175]\tvalidation_0-auc:0.964097\n",
      "[176]\tvalidation_0-auc:0.964025\n",
      "[177]\tvalidation_0-auc:0.964142\n",
      "[178]\tvalidation_0-auc:0.964161\n",
      "[179]\tvalidation_0-auc:0.964014\n",
      "[180]\tvalidation_0-auc:0.963874\n",
      "[181]\tvalidation_0-auc:0.963904\n",
      "[182]\tvalidation_0-auc:0.963966\n",
      "[183]\tvalidation_0-auc:0.964127\n",
      "[184]\tvalidation_0-auc:0.964004\n",
      "[185]\tvalidation_0-auc:0.963954\n",
      "[186]\tvalidation_0-auc:0.963901\n",
      "[187]\tvalidation_0-auc:0.963893\n",
      "[188]\tvalidation_0-auc:0.963885\n",
      "[189]\tvalidation_0-auc:0.963929\n",
      "[190]\tvalidation_0-auc:0.963993\n",
      "[191]\tvalidation_0-auc:0.964108\n",
      "[192]\tvalidation_0-auc:0.964024\n",
      "[193]\tvalidation_0-auc:0.964215\n",
      "Stopping. Best iteration:\n",
      "[173]\tvalidation_0-auc:0.964285\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=18, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=0.04, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(max_depth = 18, n_estimators = 300, scale_pos_weight=1/25)\n",
    "model.fit(X_train, y_train, early_stopping_rounds=20, eval_metric=\"auc\",\n",
    "        eval_set=[(X_val, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF_models/XGB_model.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store model\n",
    "joblib.dump(model, 'RF_models/XGB_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.7372515125324114\n",
      "The validation accuracy is 0.7036230964467006\n",
      "Training Set Confusion Mat.:\n",
      "[[23681     0]\n",
      " [  608   549]]\n",
      "Precision:  1.0\n",
      "Recall:  0.47450302506482284\n",
      "Valid Set Confusion Mat.:\n",
      "[[7878    2]\n",
      " [ 237  163]]\n",
      "Precision:  0.9878787878787879\n",
      "Recall:  0.4075\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "model = joblib.load('RF_models/XGB_model.pkl')\n",
    "#Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-558d97eb0606>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Dataset needs to be specially formatted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlgb_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlgb_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "#Dataset needs to be specially formatted\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'max_bin': 255,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=800,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=20)\n",
    "\n",
    "joblib.dump(gbm, 'RF_models/LightGBM_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9999934486461636\n",
      "The validation accuracy is 0.9719438451776651\n",
      "Training Set Confusion Mat.:\n",
      "[[23681     0]\n",
      " [   56  1101]]\n",
      "Precision:  1.0\n",
      "Recall:  0.9515989628349178\n",
      "Valid Set Confusion Mat.:\n",
      "[[7871    9]\n",
      " [ 166  234]]\n",
      "Precision:  0.9629629629629629\n",
      "Recall:  0.585\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "gbm = joblib.load('RF_models/LightGBM_model.pkl')\n",
    "#Predictions\n",
    "y_train_pred = gbm.predict(X_train)\n",
    "y_val_pred = gbm.predict(X_val)\n",
    "#Metrics\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\tvalid_0's auc: 0.895057\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\tvalid_0's auc: 0.914065\n",
      "[3]\tvalid_0's auc: 0.923339\n",
      "[4]\tvalid_0's auc: 0.916669\n",
      "[5]\tvalid_0's auc: 0.914295\n",
      "[6]\tvalid_0's auc: 0.910942\n",
      "[7]\tvalid_0's auc: 0.921477\n",
      "[8]\tvalid_0's auc: 0.920524\n",
      "[9]\tvalid_0's auc: 0.924436\n",
      "[10]\tvalid_0's auc: 0.922879\n",
      "[11]\tvalid_0's auc: 0.927175\n",
      "[12]\tvalid_0's auc: 0.929246\n",
      "[13]\tvalid_0's auc: 0.930102\n",
      "[14]\tvalid_0's auc: 0.932476\n",
      "[15]\tvalid_0's auc: 0.932709\n",
      "[16]\tvalid_0's auc: 0.933727\n",
      "[17]\tvalid_0's auc: 0.935717\n",
      "[18]\tvalid_0's auc: 0.935789\n",
      "[19]\tvalid_0's auc: 0.93533\n",
      "[20]\tvalid_0's auc: 0.93659\n",
      "[21]\tvalid_0's auc: 0.937106\n",
      "[22]\tvalid_0's auc: 0.936939\n",
      "[23]\tvalid_0's auc: 0.935968\n",
      "[24]\tvalid_0's auc: 0.93692\n",
      "[25]\tvalid_0's auc: 0.936218\n",
      "[26]\tvalid_0's auc: 0.937738\n",
      "[27]\tvalid_0's auc: 0.937802\n",
      "[28]\tvalid_0's auc: 0.938662\n",
      "[29]\tvalid_0's auc: 0.939397\n",
      "[30]\tvalid_0's auc: 0.940276\n",
      "[31]\tvalid_0's auc: 0.940744\n",
      "[32]\tvalid_0's auc: 0.941153\n",
      "[33]\tvalid_0's auc: 0.942002\n",
      "[34]\tvalid_0's auc: 0.942413\n",
      "[35]\tvalid_0's auc: 0.941957\n",
      "[36]\tvalid_0's auc: 0.94249\n",
      "[37]\tvalid_0's auc: 0.942879\n",
      "[38]\tvalid_0's auc: 0.943259\n",
      "[39]\tvalid_0's auc: 0.943834\n",
      "[40]\tvalid_0's auc: 0.943819\n",
      "[41]\tvalid_0's auc: 0.944059\n",
      "[42]\tvalid_0's auc: 0.944198\n",
      "[43]\tvalid_0's auc: 0.943955\n",
      "[44]\tvalid_0's auc: 0.94416\n",
      "[45]\tvalid_0's auc: 0.944307\n",
      "[46]\tvalid_0's auc: 0.944467\n",
      "[47]\tvalid_0's auc: 0.9451\n",
      "[48]\tvalid_0's auc: 0.944917\n",
      "[49]\tvalid_0's auc: 0.945074\n",
      "[50]\tvalid_0's auc: 0.944956\n",
      "[51]\tvalid_0's auc: 0.94521\n",
      "[52]\tvalid_0's auc: 0.945492\n",
      "[53]\tvalid_0's auc: 0.94561\n",
      "[54]\tvalid_0's auc: 0.945758\n",
      "[55]\tvalid_0's auc: 0.945909\n",
      "[56]\tvalid_0's auc: 0.946244\n",
      "[57]\tvalid_0's auc: 0.946514\n",
      "[58]\tvalid_0's auc: 0.946845\n",
      "[59]\tvalid_0's auc: 0.946767\n",
      "[60]\tvalid_0's auc: 0.946621\n",
      "[61]\tvalid_0's auc: 0.946794\n",
      "[62]\tvalid_0's auc: 0.946687\n",
      "[63]\tvalid_0's auc: 0.946937\n",
      "[64]\tvalid_0's auc: 0.947201\n",
      "[65]\tvalid_0's auc: 0.947164\n",
      "[66]\tvalid_0's auc: 0.946937\n",
      "[67]\tvalid_0's auc: 0.947038\n",
      "[68]\tvalid_0's auc: 0.947321\n",
      "[69]\tvalid_0's auc: 0.947714\n",
      "[70]\tvalid_0's auc: 0.947766\n",
      "[71]\tvalid_0's auc: 0.947893\n",
      "[72]\tvalid_0's auc: 0.948083\n",
      "[73]\tvalid_0's auc: 0.948407\n",
      "[74]\tvalid_0's auc: 0.948833\n",
      "[75]\tvalid_0's auc: 0.948908\n",
      "[76]\tvalid_0's auc: 0.949054\n",
      "[77]\tvalid_0's auc: 0.949503\n",
      "[78]\tvalid_0's auc: 0.949823\n",
      "[79]\tvalid_0's auc: 0.950165\n",
      "[80]\tvalid_0's auc: 0.95029\n",
      "[81]\tvalid_0's auc: 0.950614\n",
      "[82]\tvalid_0's auc: 0.950907\n",
      "[83]\tvalid_0's auc: 0.950892\n",
      "[84]\tvalid_0's auc: 0.951111\n",
      "[85]\tvalid_0's auc: 0.951208\n",
      "[86]\tvalid_0's auc: 0.951207\n",
      "[87]\tvalid_0's auc: 0.951201\n",
      "[88]\tvalid_0's auc: 0.951635\n",
      "[89]\tvalid_0's auc: 0.951578\n",
      "[90]\tvalid_0's auc: 0.951752\n",
      "[91]\tvalid_0's auc: 0.951712\n",
      "[92]\tvalid_0's auc: 0.951682\n",
      "[93]\tvalid_0's auc: 0.95166\n",
      "[94]\tvalid_0's auc: 0.951631\n",
      "[95]\tvalid_0's auc: 0.951889\n",
      "[96]\tvalid_0's auc: 0.952041\n",
      "[97]\tvalid_0's auc: 0.952217\n",
      "[98]\tvalid_0's auc: 0.952591\n",
      "[99]\tvalid_0's auc: 0.952876\n",
      "[100]\tvalid_0's auc: 0.952984\n",
      "[101]\tvalid_0's auc: 0.953061\n",
      "[102]\tvalid_0's auc: 0.952982\n",
      "[103]\tvalid_0's auc: 0.953109\n",
      "[104]\tvalid_0's auc: 0.953165\n",
      "[105]\tvalid_0's auc: 0.953226\n",
      "[106]\tvalid_0's auc: 0.95315\n",
      "[107]\tvalid_0's auc: 0.953186\n",
      "[108]\tvalid_0's auc: 0.953207\n",
      "[109]\tvalid_0's auc: 0.953489\n",
      "[110]\tvalid_0's auc: 0.953539\n",
      "[111]\tvalid_0's auc: 0.953696\n",
      "[112]\tvalid_0's auc: 0.95368\n",
      "[113]\tvalid_0's auc: 0.95361\n",
      "[114]\tvalid_0's auc: 0.953741\n",
      "[115]\tvalid_0's auc: 0.95368\n",
      "[116]\tvalid_0's auc: 0.953673\n",
      "[117]\tvalid_0's auc: 0.953758\n",
      "[118]\tvalid_0's auc: 0.953772\n",
      "[119]\tvalid_0's auc: 0.953757\n",
      "[120]\tvalid_0's auc: 0.95385\n",
      "[121]\tvalid_0's auc: 0.953867\n",
      "[122]\tvalid_0's auc: 0.953845\n",
      "[123]\tvalid_0's auc: 0.953805\n",
      "[124]\tvalid_0's auc: 0.953835\n",
      "[125]\tvalid_0's auc: 0.953799\n",
      "[126]\tvalid_0's auc: 0.953776\n",
      "[127]\tvalid_0's auc: 0.953696\n",
      "[128]\tvalid_0's auc: 0.953696\n",
      "[129]\tvalid_0's auc: 0.953758\n",
      "[130]\tvalid_0's auc: 0.95382\n",
      "[131]\tvalid_0's auc: 0.953907\n",
      "[132]\tvalid_0's auc: 0.953872\n",
      "[133]\tvalid_0's auc: 0.953869\n",
      "[134]\tvalid_0's auc: 0.953849\n",
      "[135]\tvalid_0's auc: 0.953878\n",
      "[136]\tvalid_0's auc: 0.954\n",
      "[137]\tvalid_0's auc: 0.953969\n",
      "[138]\tvalid_0's auc: 0.95403\n",
      "[139]\tvalid_0's auc: 0.95406\n",
      "[140]\tvalid_0's auc: 0.954158\n",
      "[141]\tvalid_0's auc: 0.954129\n",
      "[142]\tvalid_0's auc: 0.954283\n",
      "[143]\tvalid_0's auc: 0.954247\n",
      "[144]\tvalid_0's auc: 0.954142\n",
      "[145]\tvalid_0's auc: 0.954087\n",
      "[146]\tvalid_0's auc: 0.954148\n",
      "[147]\tvalid_0's auc: 0.954204\n",
      "[148]\tvalid_0's auc: 0.954356\n",
      "[149]\tvalid_0's auc: 0.954416\n",
      "[150]\tvalid_0's auc: 0.954538\n",
      "[151]\tvalid_0's auc: 0.954677\n",
      "[152]\tvalid_0's auc: 0.954783\n",
      "[153]\tvalid_0's auc: 0.954856\n",
      "[154]\tvalid_0's auc: 0.95498\n",
      "[155]\tvalid_0's auc: 0.955128\n",
      "[156]\tvalid_0's auc: 0.955199\n",
      "[157]\tvalid_0's auc: 0.955242\n",
      "[158]\tvalid_0's auc: 0.9552\n",
      "[159]\tvalid_0's auc: 0.955323\n",
      "[160]\tvalid_0's auc: 0.955524\n",
      "[161]\tvalid_0's auc: 0.955624\n",
      "[162]\tvalid_0's auc: 0.955719\n",
      "[163]\tvalid_0's auc: 0.955851\n",
      "[164]\tvalid_0's auc: 0.956016\n",
      "[165]\tvalid_0's auc: 0.956058\n",
      "[166]\tvalid_0's auc: 0.956113\n",
      "[167]\tvalid_0's auc: 0.956127\n",
      "[168]\tvalid_0's auc: 0.956163\n",
      "[169]\tvalid_0's auc: 0.956224\n",
      "[170]\tvalid_0's auc: 0.956257\n",
      "[171]\tvalid_0's auc: 0.956298\n",
      "[172]\tvalid_0's auc: 0.956283\n",
      "[173]\tvalid_0's auc: 0.956436\n",
      "[174]\tvalid_0's auc: 0.956476\n",
      "[175]\tvalid_0's auc: 0.956466\n",
      "[176]\tvalid_0's auc: 0.956525\n",
      "[177]\tvalid_0's auc: 0.956637\n",
      "[178]\tvalid_0's auc: 0.956702\n",
      "[179]\tvalid_0's auc: 0.956773\n",
      "[180]\tvalid_0's auc: 0.956796\n",
      "[181]\tvalid_0's auc: 0.956793\n",
      "[182]\tvalid_0's auc: 0.956896\n",
      "[183]\tvalid_0's auc: 0.956957\n",
      "[184]\tvalid_0's auc: 0.956951\n",
      "[185]\tvalid_0's auc: 0.957055\n",
      "[186]\tvalid_0's auc: 0.957108\n",
      "[187]\tvalid_0's auc: 0.957169\n",
      "[188]\tvalid_0's auc: 0.957223\n",
      "[189]\tvalid_0's auc: 0.957318\n",
      "[190]\tvalid_0's auc: 0.957313\n",
      "[191]\tvalid_0's auc: 0.95734\n",
      "[192]\tvalid_0's auc: 0.957395\n",
      "[193]\tvalid_0's auc: 0.957439\n",
      "[194]\tvalid_0's auc: 0.957396\n",
      "[195]\tvalid_0's auc: 0.957349\n",
      "[196]\tvalid_0's auc: 0.957354\n",
      "[197]\tvalid_0's auc: 0.957474\n",
      "[198]\tvalid_0's auc: 0.957548\n",
      "[199]\tvalid_0's auc: 0.957737\n",
      "[200]\tvalid_0's auc: 0.957785\n",
      "[201]\tvalid_0's auc: 0.957904\n",
      "[202]\tvalid_0's auc: 0.957965\n",
      "[203]\tvalid_0's auc: 0.958049\n",
      "[204]\tvalid_0's auc: 0.95804\n",
      "[205]\tvalid_0's auc: 0.958198\n",
      "[206]\tvalid_0's auc: 0.95822\n",
      "[207]\tvalid_0's auc: 0.958312\n",
      "[208]\tvalid_0's auc: 0.958332\n",
      "[209]\tvalid_0's auc: 0.958319\n",
      "[210]\tvalid_0's auc: 0.958404\n",
      "[211]\tvalid_0's auc: 0.958549\n",
      "[212]\tvalid_0's auc: 0.958678\n",
      "[213]\tvalid_0's auc: 0.958686\n",
      "[214]\tvalid_0's auc: 0.958727\n",
      "[215]\tvalid_0's auc: 0.95877\n",
      "[216]\tvalid_0's auc: 0.958896\n",
      "[217]\tvalid_0's auc: 0.958935\n",
      "[218]\tvalid_0's auc: 0.958962\n",
      "[219]\tvalid_0's auc: 0.959021\n",
      "[220]\tvalid_0's auc: 0.95908\n",
      "[221]\tvalid_0's auc: 0.959097\n",
      "[222]\tvalid_0's auc: 0.95919\n",
      "[223]\tvalid_0's auc: 0.959232\n",
      "[224]\tvalid_0's auc: 0.959304\n",
      "[225]\tvalid_0's auc: 0.959385\n",
      "[226]\tvalid_0's auc: 0.959428\n",
      "[227]\tvalid_0's auc: 0.959468\n",
      "[228]\tvalid_0's auc: 0.959468\n",
      "[229]\tvalid_0's auc: 0.959483\n",
      "[230]\tvalid_0's auc: 0.959514\n",
      "[231]\tvalid_0's auc: 0.959548\n",
      "[232]\tvalid_0's auc: 0.959607\n",
      "[233]\tvalid_0's auc: 0.959684\n",
      "[234]\tvalid_0's auc: 0.9597\n",
      "[235]\tvalid_0's auc: 0.95966\n",
      "[236]\tvalid_0's auc: 0.959701\n",
      "[237]\tvalid_0's auc: 0.959734\n",
      "[238]\tvalid_0's auc: 0.959722\n",
      "[239]\tvalid_0's auc: 0.959835\n",
      "[240]\tvalid_0's auc: 0.959845\n",
      "[241]\tvalid_0's auc: 0.959904\n",
      "[242]\tvalid_0's auc: 0.959987\n",
      "[243]\tvalid_0's auc: 0.959996\n",
      "[244]\tvalid_0's auc: 0.960059\n",
      "[245]\tvalid_0's auc: 0.96005\n",
      "[246]\tvalid_0's auc: 0.960114\n",
      "[247]\tvalid_0's auc: 0.960198\n",
      "[248]\tvalid_0's auc: 0.960204\n",
      "[249]\tvalid_0's auc: 0.960266\n",
      "[250]\tvalid_0's auc: 0.960286\n",
      "[251]\tvalid_0's auc: 0.960246\n",
      "[252]\tvalid_0's auc: 0.960265\n",
      "[253]\tvalid_0's auc: 0.960299\n",
      "[254]\tvalid_0's auc: 0.960374\n",
      "[255]\tvalid_0's auc: 0.960421\n",
      "[256]\tvalid_0's auc: 0.960381\n",
      "[257]\tvalid_0's auc: 0.960367\n",
      "[258]\tvalid_0's auc: 0.960403\n",
      "[259]\tvalid_0's auc: 0.960551\n",
      "[260]\tvalid_0's auc: 0.960535\n",
      "[261]\tvalid_0's auc: 0.960575\n",
      "[262]\tvalid_0's auc: 0.960603\n",
      "[263]\tvalid_0's auc: 0.960628\n",
      "[264]\tvalid_0's auc: 0.960641\n",
      "[265]\tvalid_0's auc: 0.960733\n",
      "[266]\tvalid_0's auc: 0.960811\n",
      "[267]\tvalid_0's auc: 0.96081\n",
      "[268]\tvalid_0's auc: 0.960972\n",
      "[269]\tvalid_0's auc: 0.961041\n",
      "[270]\tvalid_0's auc: 0.961003\n",
      "[271]\tvalid_0's auc: 0.961017\n",
      "[272]\tvalid_0's auc: 0.961043\n",
      "[273]\tvalid_0's auc: 0.961165\n",
      "[274]\tvalid_0's auc: 0.961203\n",
      "[275]\tvalid_0's auc: 0.961208\n",
      "[276]\tvalid_0's auc: 0.961269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277]\tvalid_0's auc: 0.961377\n",
      "[278]\tvalid_0's auc: 0.961417\n",
      "[279]\tvalid_0's auc: 0.961484\n",
      "[280]\tvalid_0's auc: 0.961552\n",
      "[281]\tvalid_0's auc: 0.961563\n",
      "[282]\tvalid_0's auc: 0.961549\n",
      "[283]\tvalid_0's auc: 0.961617\n",
      "[284]\tvalid_0's auc: 0.961694\n",
      "[285]\tvalid_0's auc: 0.961763\n",
      "[286]\tvalid_0's auc: 0.961809\n",
      "[287]\tvalid_0's auc: 0.961852\n",
      "[288]\tvalid_0's auc: 0.961927\n",
      "[289]\tvalid_0's auc: 0.961954\n",
      "[290]\tvalid_0's auc: 0.962012\n",
      "[291]\tvalid_0's auc: 0.962085\n",
      "[292]\tvalid_0's auc: 0.96213\n",
      "[293]\tvalid_0's auc: 0.962149\n",
      "[294]\tvalid_0's auc: 0.962175\n",
      "[295]\tvalid_0's auc: 0.962186\n",
      "[296]\tvalid_0's auc: 0.962181\n",
      "[297]\tvalid_0's auc: 0.962161\n",
      "[298]\tvalid_0's auc: 0.962172\n",
      "[299]\tvalid_0's auc: 0.962271\n",
      "[300]\tvalid_0's auc: 0.962292\n",
      "[301]\tvalid_0's auc: 0.962332\n",
      "[302]\tvalid_0's auc: 0.962399\n",
      "[303]\tvalid_0's auc: 0.962476\n",
      "[304]\tvalid_0's auc: 0.962519\n",
      "[305]\tvalid_0's auc: 0.962582\n",
      "[306]\tvalid_0's auc: 0.96268\n",
      "[307]\tvalid_0's auc: 0.962739\n",
      "[308]\tvalid_0's auc: 0.962749\n",
      "[309]\tvalid_0's auc: 0.962752\n",
      "[310]\tvalid_0's auc: 0.962805\n",
      "[311]\tvalid_0's auc: 0.962807\n",
      "[312]\tvalid_0's auc: 0.962766\n",
      "[313]\tvalid_0's auc: 0.962764\n",
      "[314]\tvalid_0's auc: 0.962811\n",
      "[315]\tvalid_0's auc: 0.962769\n",
      "[316]\tvalid_0's auc: 0.962796\n",
      "[317]\tvalid_0's auc: 0.962798\n",
      "[318]\tvalid_0's auc: 0.962808\n",
      "[319]\tvalid_0's auc: 0.962838\n",
      "[320]\tvalid_0's auc: 0.962892\n",
      "[321]\tvalid_0's auc: 0.96293\n",
      "[322]\tvalid_0's auc: 0.962986\n",
      "[323]\tvalid_0's auc: 0.963016\n",
      "[324]\tvalid_0's auc: 0.963048\n",
      "[325]\tvalid_0's auc: 0.963072\n",
      "[326]\tvalid_0's auc: 0.963138\n",
      "[327]\tvalid_0's auc: 0.963178\n",
      "[328]\tvalid_0's auc: 0.963132\n",
      "[329]\tvalid_0's auc: 0.963231\n",
      "[330]\tvalid_0's auc: 0.96323\n",
      "[331]\tvalid_0's auc: 0.963262\n",
      "[332]\tvalid_0's auc: 0.963235\n",
      "[333]\tvalid_0's auc: 0.96329\n",
      "[334]\tvalid_0's auc: 0.963319\n",
      "[335]\tvalid_0's auc: 0.963401\n",
      "[336]\tvalid_0's auc: 0.963447\n",
      "[337]\tvalid_0's auc: 0.963483\n",
      "[338]\tvalid_0's auc: 0.963526\n",
      "[339]\tvalid_0's auc: 0.963541\n",
      "[340]\tvalid_0's auc: 0.963568\n",
      "[341]\tvalid_0's auc: 0.963559\n",
      "[342]\tvalid_0's auc: 0.963578\n",
      "[343]\tvalid_0's auc: 0.963587\n",
      "[344]\tvalid_0's auc: 0.963594\n",
      "[345]\tvalid_0's auc: 0.963632\n",
      "[346]\tvalid_0's auc: 0.9636\n",
      "[347]\tvalid_0's auc: 0.963681\n",
      "[348]\tvalid_0's auc: 0.963748\n",
      "[349]\tvalid_0's auc: 0.963772\n",
      "[350]\tvalid_0's auc: 0.963775\n",
      "[351]\tvalid_0's auc: 0.963848\n",
      "[352]\tvalid_0's auc: 0.963901\n",
      "[353]\tvalid_0's auc: 0.963989\n",
      "[354]\tvalid_0's auc: 0.96406\n",
      "[355]\tvalid_0's auc: 0.964113\n",
      "[356]\tvalid_0's auc: 0.964169\n",
      "[357]\tvalid_0's auc: 0.964221\n",
      "[358]\tvalid_0's auc: 0.964272\n",
      "[359]\tvalid_0's auc: 0.964238\n",
      "[360]\tvalid_0's auc: 0.964301\n",
      "[361]\tvalid_0's auc: 0.964323\n",
      "[362]\tvalid_0's auc: 0.964319\n",
      "[363]\tvalid_0's auc: 0.964301\n",
      "[364]\tvalid_0's auc: 0.96429\n",
      "[365]\tvalid_0's auc: 0.964354\n",
      "[366]\tvalid_0's auc: 0.964349\n",
      "[367]\tvalid_0's auc: 0.964387\n",
      "[368]\tvalid_0's auc: 0.96446\n",
      "[369]\tvalid_0's auc: 0.964514\n",
      "[370]\tvalid_0's auc: 0.964562\n",
      "[371]\tvalid_0's auc: 0.964584\n",
      "[372]\tvalid_0's auc: 0.964631\n",
      "[373]\tvalid_0's auc: 0.964681\n",
      "[374]\tvalid_0's auc: 0.964659\n",
      "[375]\tvalid_0's auc: 0.96474\n",
      "[376]\tvalid_0's auc: 0.964729\n",
      "[377]\tvalid_0's auc: 0.964722\n",
      "[378]\tvalid_0's auc: 0.964762\n",
      "[379]\tvalid_0's auc: 0.96482\n",
      "[380]\tvalid_0's auc: 0.964855\n",
      "[381]\tvalid_0's auc: 0.964931\n",
      "[382]\tvalid_0's auc: 0.964961\n",
      "[383]\tvalid_0's auc: 0.964968\n",
      "[384]\tvalid_0's auc: 0.964963\n",
      "[385]\tvalid_0's auc: 0.964994\n",
      "[386]\tvalid_0's auc: 0.965029\n",
      "[387]\tvalid_0's auc: 0.965011\n",
      "[388]\tvalid_0's auc: 0.965056\n",
      "[389]\tvalid_0's auc: 0.965086\n",
      "[390]\tvalid_0's auc: 0.965103\n",
      "[391]\tvalid_0's auc: 0.965156\n",
      "[392]\tvalid_0's auc: 0.965196\n",
      "[393]\tvalid_0's auc: 0.965243\n",
      "[394]\tvalid_0's auc: 0.965286\n",
      "[395]\tvalid_0's auc: 0.965332\n",
      "[396]\tvalid_0's auc: 0.965346\n",
      "[397]\tvalid_0's auc: 0.965384\n",
      "[398]\tvalid_0's auc: 0.965347\n",
      "[399]\tvalid_0's auc: 0.965292\n",
      "[400]\tvalid_0's auc: 0.965343\n",
      "[401]\tvalid_0's auc: 0.965409\n",
      "[402]\tvalid_0's auc: 0.965465\n",
      "[403]\tvalid_0's auc: 0.965481\n",
      "[404]\tvalid_0's auc: 0.965507\n",
      "[405]\tvalid_0's auc: 0.965522\n",
      "[406]\tvalid_0's auc: 0.965541\n",
      "[407]\tvalid_0's auc: 0.965556\n",
      "[408]\tvalid_0's auc: 0.965531\n",
      "[409]\tvalid_0's auc: 0.965538\n",
      "[410]\tvalid_0's auc: 0.965542\n",
      "[411]\tvalid_0's auc: 0.965599\n",
      "[412]\tvalid_0's auc: 0.965603\n",
      "[413]\tvalid_0's auc: 0.965616\n",
      "[414]\tvalid_0's auc: 0.965641\n",
      "[415]\tvalid_0's auc: 0.965702\n",
      "[416]\tvalid_0's auc: 0.965732\n",
      "[417]\tvalid_0's auc: 0.965722\n",
      "[418]\tvalid_0's auc: 0.965747\n",
      "[419]\tvalid_0's auc: 0.965759\n",
      "[420]\tvalid_0's auc: 0.965745\n",
      "[421]\tvalid_0's auc: 0.965795\n",
      "[422]\tvalid_0's auc: 0.965801\n",
      "[423]\tvalid_0's auc: 0.965845\n",
      "[424]\tvalid_0's auc: 0.965831\n",
      "[425]\tvalid_0's auc: 0.965842\n",
      "[426]\tvalid_0's auc: 0.965818\n",
      "[427]\tvalid_0's auc: 0.965819\n",
      "[428]\tvalid_0's auc: 0.965852\n",
      "[429]\tvalid_0's auc: 0.96583\n",
      "[430]\tvalid_0's auc: 0.965822\n",
      "[431]\tvalid_0's auc: 0.965813\n",
      "[432]\tvalid_0's auc: 0.965843\n",
      "[433]\tvalid_0's auc: 0.965869\n",
      "[434]\tvalid_0's auc: 0.965875\n",
      "[435]\tvalid_0's auc: 0.965841\n",
      "[436]\tvalid_0's auc: 0.965826\n",
      "[437]\tvalid_0's auc: 0.965835\n",
      "[438]\tvalid_0's auc: 0.965834\n",
      "[439]\tvalid_0's auc: 0.965865\n",
      "[440]\tvalid_0's auc: 0.965859\n",
      "[441]\tvalid_0's auc: 0.965876\n",
      "[442]\tvalid_0's auc: 0.965928\n",
      "[443]\tvalid_0's auc: 0.965959\n",
      "[444]\tvalid_0's auc: 0.965995\n",
      "[445]\tvalid_0's auc: 0.966014\n",
      "[446]\tvalid_0's auc: 0.96604\n",
      "[447]\tvalid_0's auc: 0.966058\n",
      "[448]\tvalid_0's auc: 0.966056\n",
      "[449]\tvalid_0's auc: 0.96602\n",
      "[450]\tvalid_0's auc: 0.966029\n",
      "[451]\tvalid_0's auc: 0.966074\n",
      "[452]\tvalid_0's auc: 0.966102\n",
      "[453]\tvalid_0's auc: 0.966203\n",
      "[454]\tvalid_0's auc: 0.966251\n",
      "[455]\tvalid_0's auc: 0.966257\n",
      "[456]\tvalid_0's auc: 0.96626\n",
      "[457]\tvalid_0's auc: 0.966284\n",
      "[458]\tvalid_0's auc: 0.966323\n",
      "[459]\tvalid_0's auc: 0.966341\n",
      "[460]\tvalid_0's auc: 0.966372\n",
      "[461]\tvalid_0's auc: 0.966354\n",
      "[462]\tvalid_0's auc: 0.966384\n",
      "[463]\tvalid_0's auc: 0.966408\n",
      "[464]\tvalid_0's auc: 0.966394\n",
      "[465]\tvalid_0's auc: 0.966411\n",
      "[466]\tvalid_0's auc: 0.966444\n",
      "[467]\tvalid_0's auc: 0.966464\n",
      "[468]\tvalid_0's auc: 0.966494\n",
      "[469]\tvalid_0's auc: 0.966523\n",
      "[470]\tvalid_0's auc: 0.966563\n",
      "[471]\tvalid_0's auc: 0.966591\n",
      "[472]\tvalid_0's auc: 0.9666\n",
      "[473]\tvalid_0's auc: 0.966611\n",
      "[474]\tvalid_0's auc: 0.966635\n",
      "[475]\tvalid_0's auc: 0.966641\n",
      "[476]\tvalid_0's auc: 0.966674\n",
      "[477]\tvalid_0's auc: 0.966692\n",
      "[478]\tvalid_0's auc: 0.966738\n",
      "[479]\tvalid_0's auc: 0.966769\n",
      "[480]\tvalid_0's auc: 0.96683\n",
      "[481]\tvalid_0's auc: 0.966844\n",
      "[482]\tvalid_0's auc: 0.966877\n",
      "[483]\tvalid_0's auc: 0.966875\n",
      "[484]\tvalid_0's auc: 0.966908\n",
      "[485]\tvalid_0's auc: 0.966927\n",
      "[486]\tvalid_0's auc: 0.966961\n",
      "[487]\tvalid_0's auc: 0.96699\n",
      "[488]\tvalid_0's auc: 0.967015\n",
      "[489]\tvalid_0's auc: 0.96704\n",
      "[490]\tvalid_0's auc: 0.967072\n",
      "[491]\tvalid_0's auc: 0.967075\n",
      "[492]\tvalid_0's auc: 0.967107\n",
      "[493]\tvalid_0's auc: 0.967129\n",
      "[494]\tvalid_0's auc: 0.96718\n",
      "[495]\tvalid_0's auc: 0.967175\n",
      "[496]\tvalid_0's auc: 0.967179\n",
      "[497]\tvalid_0's auc: 0.967195\n",
      "[498]\tvalid_0's auc: 0.967211\n",
      "[499]\tvalid_0's auc: 0.967255\n",
      "[500]\tvalid_0's auc: 0.96726\n",
      "[501]\tvalid_0's auc: 0.967275\n",
      "[502]\tvalid_0's auc: 0.967276\n",
      "[503]\tvalid_0's auc: 0.967253\n",
      "[504]\tvalid_0's auc: 0.967256\n",
      "[505]\tvalid_0's auc: 0.967256\n",
      "[506]\tvalid_0's auc: 0.967276\n",
      "[507]\tvalid_0's auc: 0.967264\n",
      "[508]\tvalid_0's auc: 0.967261\n",
      "[509]\tvalid_0's auc: 0.967225\n",
      "[510]\tvalid_0's auc: 0.96722\n",
      "[511]\tvalid_0's auc: 0.967263\n",
      "[512]\tvalid_0's auc: 0.967312\n",
      "[513]\tvalid_0's auc: 0.967299\n",
      "[514]\tvalid_0's auc: 0.967286\n",
      "[515]\tvalid_0's auc: 0.967317\n",
      "[516]\tvalid_0's auc: 0.967379\n",
      "[517]\tvalid_0's auc: 0.967375\n",
      "[518]\tvalid_0's auc: 0.967381\n",
      "[519]\tvalid_0's auc: 0.967407\n",
      "[520]\tvalid_0's auc: 0.967414\n",
      "[521]\tvalid_0's auc: 0.967439\n",
      "[522]\tvalid_0's auc: 0.967459\n",
      "[523]\tvalid_0's auc: 0.967453\n",
      "[524]\tvalid_0's auc: 0.967495\n",
      "[525]\tvalid_0's auc: 0.967477\n",
      "[526]\tvalid_0's auc: 0.967455\n",
      "[527]\tvalid_0's auc: 0.967504\n",
      "[528]\tvalid_0's auc: 0.967557\n",
      "[529]\tvalid_0's auc: 0.9676\n",
      "[530]\tvalid_0's auc: 0.967634\n",
      "[531]\tvalid_0's auc: 0.967675\n",
      "[532]\tvalid_0's auc: 0.967728\n",
      "[533]\tvalid_0's auc: 0.967763\n",
      "[534]\tvalid_0's auc: 0.967806\n",
      "[535]\tvalid_0's auc: 0.967848\n",
      "[536]\tvalid_0's auc: 0.967868\n",
      "[537]\tvalid_0's auc: 0.967861\n",
      "[538]\tvalid_0's auc: 0.96791\n",
      "[539]\tvalid_0's auc: 0.967876\n",
      "[540]\tvalid_0's auc: 0.967905\n",
      "[541]\tvalid_0's auc: 0.967941\n",
      "[542]\tvalid_0's auc: 0.967998\n",
      "[543]\tvalid_0's auc: 0.968013\n",
      "[544]\tvalid_0's auc: 0.968021\n",
      "[545]\tvalid_0's auc: 0.968047\n",
      "[546]\tvalid_0's auc: 0.968046\n",
      "[547]\tvalid_0's auc: 0.968072\n",
      "[548]\tvalid_0's auc: 0.968126\n",
      "[549]\tvalid_0's auc: 0.96812\n",
      "[550]\tvalid_0's auc: 0.968134\n",
      "[551]\tvalid_0's auc: 0.968147\n",
      "[552]\tvalid_0's auc: 0.968155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[553]\tvalid_0's auc: 0.968214\n",
      "[554]\tvalid_0's auc: 0.968221\n",
      "[555]\tvalid_0's auc: 0.968238\n",
      "[556]\tvalid_0's auc: 0.968238\n",
      "[557]\tvalid_0's auc: 0.968252\n",
      "[558]\tvalid_0's auc: 0.968302\n",
      "[559]\tvalid_0's auc: 0.968299\n",
      "[560]\tvalid_0's auc: 0.968305\n",
      "[561]\tvalid_0's auc: 0.968289\n",
      "[562]\tvalid_0's auc: 0.968268\n",
      "[563]\tvalid_0's auc: 0.968281\n",
      "[564]\tvalid_0's auc: 0.968292\n",
      "[565]\tvalid_0's auc: 0.968331\n",
      "[566]\tvalid_0's auc: 0.968322\n",
      "[567]\tvalid_0's auc: 0.96838\n",
      "[568]\tvalid_0's auc: 0.968393\n",
      "[569]\tvalid_0's auc: 0.968402\n",
      "[570]\tvalid_0's auc: 0.968455\n",
      "[571]\tvalid_0's auc: 0.968501\n",
      "[572]\tvalid_0's auc: 0.968556\n",
      "[573]\tvalid_0's auc: 0.968554\n",
      "[574]\tvalid_0's auc: 0.968583\n",
      "[575]\tvalid_0's auc: 0.968651\n",
      "[576]\tvalid_0's auc: 0.968732\n",
      "[577]\tvalid_0's auc: 0.968756\n",
      "[578]\tvalid_0's auc: 0.96876\n",
      "[579]\tvalid_0's auc: 0.96876\n",
      "[580]\tvalid_0's auc: 0.968784\n",
      "[581]\tvalid_0's auc: 0.968797\n",
      "[582]\tvalid_0's auc: 0.968824\n",
      "[583]\tvalid_0's auc: 0.968852\n",
      "[584]\tvalid_0's auc: 0.968862\n",
      "[585]\tvalid_0's auc: 0.968862\n",
      "[586]\tvalid_0's auc: 0.968888\n",
      "[587]\tvalid_0's auc: 0.968971\n",
      "[588]\tvalid_0's auc: 0.968993\n",
      "[589]\tvalid_0's auc: 0.969025\n",
      "[590]\tvalid_0's auc: 0.969058\n",
      "[591]\tvalid_0's auc: 0.96907\n",
      "[592]\tvalid_0's auc: 0.969072\n",
      "[593]\tvalid_0's auc: 0.969097\n",
      "[594]\tvalid_0's auc: 0.969113\n",
      "[595]\tvalid_0's auc: 0.969148\n",
      "[596]\tvalid_0's auc: 0.969177\n",
      "[597]\tvalid_0's auc: 0.969198\n",
      "[598]\tvalid_0's auc: 0.969192\n",
      "[599]\tvalid_0's auc: 0.969227\n",
      "[600]\tvalid_0's auc: 0.969231\n",
      "[601]\tvalid_0's auc: 0.969217\n",
      "[602]\tvalid_0's auc: 0.969219\n",
      "[603]\tvalid_0's auc: 0.969192\n",
      "[604]\tvalid_0's auc: 0.969172\n",
      "[605]\tvalid_0's auc: 0.969138\n",
      "[606]\tvalid_0's auc: 0.969124\n",
      "[607]\tvalid_0's auc: 0.969146\n",
      "[608]\tvalid_0's auc: 0.969163\n",
      "[609]\tvalid_0's auc: 0.969182\n",
      "[610]\tvalid_0's auc: 0.969164\n",
      "[611]\tvalid_0's auc: 0.969177\n",
      "[612]\tvalid_0's auc: 0.969166\n",
      "[613]\tvalid_0's auc: 0.969147\n",
      "[614]\tvalid_0's auc: 0.969167\n",
      "[615]\tvalid_0's auc: 0.969187\n",
      "[616]\tvalid_0's auc: 0.969182\n",
      "[617]\tvalid_0's auc: 0.969163\n",
      "[618]\tvalid_0's auc: 0.969114\n",
      "[619]\tvalid_0's auc: 0.969119\n",
      "[620]\tvalid_0's auc: 0.969112\n",
      "Early stopping, best iteration is:\n",
      "[600]\tvalid_0's auc: 0.969231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RF_models/LightGBM_model_balanced.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_col = (np.array(y_train == 1)*14)+1 ##15x weight on bankruptcies\n",
    "lgb_train = lgb.Dataset(X_train, y_train, weight = weight_col)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'max_bin': 1023,\n",
    "    'num_leaves': 47,\n",
    "    'learning_rate': 0.01,\n",
    "    'subsample':0.75,\n",
    "    'feature_fraction': 0.75,\n",
    "    'weight_column' : weight_col,\n",
    "    'bagging_freq': 25,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=800,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=20)\n",
    "\n",
    "joblib.dump(gbm, 'RF_models/LightGBM_model_balanced.pkl') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9995932685952515\n",
      "The validation accuracy is 0.9692306472081218\n",
      "Training Set Confusion Mat.:\n",
      "[[23332   349]\n",
      " [    1  1156]]\n",
      "Precision:  0.7681063122923588\n",
      "Recall:  0.9991356957649092\n",
      "Valid Set Confusion Mat.:\n",
      "[[7719  161]\n",
      " [ 106  294]]\n",
      "Precision:  0.6461538461538462\n",
      "Recall:  0.735\n"
     ]
    }
   ],
   "source": [
    "gbm = joblib.load('RF_models/LightGBM_model_balanced.pkl')\n",
    "y_train_pred = gbm.predict(X_train)\n",
    "y_val_pred = gbm.predict(X_val)\n",
    "\n",
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the Bagged NN, LightGBM, XGB, and tuned LightGBM models together and then votes for classification. XGB receives half weight due to inferior performance in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load bagged model\n",
    "y_train_pred_NN = np.zeros(np.shape(y_train))\n",
    "y_val_pred_NN = np.zeros(np.shape(y_val))\n",
    "\n",
    "for i in range(1,25):\n",
    "    model = load_model('NN_models/NN_single{0}.h5'.format(i))\n",
    "    train_temp = model.predict(X_train)\n",
    "    val_temp = model.predict(X_val)\n",
    "    y_train_pred_NN += train_temp[:,1]\n",
    "    y_val_pred_NN += val_temp[:,1]\n",
    "    \n",
    "y_val_pred_NN = y_val_pred_NN / 24\n",
    "y_train_pred_NN = y_train_pred_NN / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Models\n",
    "gbm1 =joblib.load('RF_models/LightGBM_model.pkl')\n",
    "gbm2 = joblib.load('RF_models/LightGBM_model_balanced.pkl')\n",
    "xgb = joblib.load('RF_models/XGB_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Predictions for each model\n",
    "y_train_pred_gbm1 = gbm1.predict(X_train)\n",
    "y_val_pred_gbm1 = gbm1.predict(X_val)\n",
    "y_train_pred_gbm2 = gbm2.predict(X_train)\n",
    "y_val_pred_gbm2 = gbm2.predict(X_val)\n",
    "y_train_pred_xgb = xgb.predict(X_train)\n",
    "y_val_pred_xgb = xgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting model\n",
    "y_val_pred = (y_val_pred_NN > 0.5)*1. + (y_val_pred_gbm1 > 0.5)*1. + (y_val_pred_gbm2 > 0.5)*1. + (y_val_pred_xgb > 0.5)*1 \n",
    "y_train_pred = (y_train_pred_NN > 0.5)*1. + (y_train_pred_gbm1 > 0.5)*1. + (y_train_pred_gbm2 > 0.5)*1. + (y_train_pred_xgb > 0.5)*1\n",
    "y_val_pred = y_val_pred > 0\n",
    "y_train_pred = y_train_pred > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9854735864194923\n",
      "The validation accuracy is 0.8968781725888325\n",
      "Training Set Confusion Mat.:\n",
      "[[22993   688]\n",
      " [    0  1157]]\n",
      "Precision:  0.62710027100271\n",
      "Recall:  1.0\n",
      "Valid Set Confusion Mat.:\n",
      "[[7555  325]\n",
      " [  66  334]]\n",
      "Precision:  0.5068285280728376\n",
      "Recall:  0.835\n"
     ]
    }
   ],
   "source": [
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability model\n",
    "y_val_pred = (y_val_pred_NN*1.) + (y_val_pred_gbm1*1.) + (y_val_pred_gbm2*1.) + (y_val_pred_xgb*0.5)\n",
    "y_train_pred = (y_train_pred_NN*1.) + (y_train_pred_gbm1*1.) + (y_train_pred_gbm2*1.) + (y_train_pred_xgb*0.5)\n",
    "#Re-weight probabilities, maximum value of 3.5 due to XGB underweighting.\n",
    "y_val_pred = y_val_pred/3.5 \n",
    "y_train_pred = y_train_pred/3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9999730646287954\n",
      "The validation accuracy is 0.9732154187817259\n",
      "Training Set Confusion Mat.:\n",
      "[[23668    13]\n",
      " [   14  1143]]\n",
      "Precision:  0.9887543252595156\n",
      "Recall:  0.9878997407087294\n",
      "Valid Set Confusion Mat.:\n",
      "[[7856   24]\n",
      " [ 157  243]]\n",
      "Precision:  0.9101123595505618\n",
      "Recall:  0.6075\n"
     ]
    }
   ],
   "source": [
    "perf_metrics(y_train, y_val, y_train_pred, y_val_pred, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LGBM1</th>\n",
       "      <th>LGBM2</th>\n",
       "      <th>NN</th>\n",
       "      <th>XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8280.000000</td>\n",
       "      <td>8280.000000</td>\n",
       "      <td>8280.000000</td>\n",
       "      <td>8280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.038891</td>\n",
       "      <td>0.096127</td>\n",
       "      <td>0.093003</td>\n",
       "      <td>0.019928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.156049</td>\n",
       "      <td>0.192526</td>\n",
       "      <td>0.208454</td>\n",
       "      <td>0.139760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.007590</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.023090</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.008878</td>\n",
       "      <td>0.074603</td>\n",
       "      <td>0.060827</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.993984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LGBM1        LGBM2           NN          XGB\n",
       "count  8280.000000  8280.000000  8280.000000  8280.000000\n",
       "mean      0.038891     0.096127     0.093003     0.019928\n",
       "std       0.156049     0.192526     0.208454     0.139760\n",
       "min       0.000016     0.002158     0.000000     0.000000\n",
       "25%       0.000509     0.007590     0.000069     0.000000\n",
       "50%       0.002042     0.023090     0.003231     0.000000\n",
       "75%       0.008878     0.074603     0.060827     0.000000\n",
       "max       0.997765     0.993984     1.000000     1.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examination of model output probabilities\n",
    "#Large means may indicate that one model is overpowering the others.\n",
    "\n",
    "pd.DataFrame(data = {'LGBM1':y_val_pred_gbm1,'LGBM2':y_val_pred_gbm2, 'NN':y_val_pred_NN, 'XGB':y_val_pred_xgb}).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect predictions to see errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Actual': y_val,'NN_pred': (y_val_pred_NN > 0.5)*1 ,'LGBM1_pred' : (y_val_pred_gbm1> 0.5)*1,'LGBM2_pred':(y_val_pred_gbm2 > 0.5)*1, 'XGB_pred':(y_val_pred_xgb > 0.5)*1}\n",
    "y_val_collection = pd.DataFrame(data=d)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the recall/precision bias can be explained by many algorithms under or overestimating the number of bankruptcies in the sample. Generally, LGBM1 tends to have very few false positives, while the bagged NN has few false negatives. Problematically, these predictors tend to compete and the addition of LGBM2 and XGB does not seem to significantly improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual        400\n",
       "LGBM1_pred    243\n",
       "LGBM2_pred    455\n",
       "NN_pred       545\n",
       "XGB_pred      165\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_val_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12724498287001107\n",
      "0.5486958063204471\n",
      "0.6225300377933303\n",
      "0.34519183356195665\n",
      "0.036916642404857154\n",
      "0.2604602057476298\n",
      "0.3104058517651238\n",
      "0.17219699405118985\n"
     ]
    }
   ],
   "source": [
    "## Determine new probability thresholds:\n",
    "\n",
    "\n",
    "# 5th Percentile\n",
    "# LightGBM 1\n",
    "print(np.percentile(y_val_pred_gbm1, 95))\n",
    "# LightGBM 2\n",
    "print(np.percentile(y_val_pred_gbm2, 95))\n",
    "# Bagged NN\n",
    "print(np.percentile(y_val_pred_NN, 95))\n",
    "# Probability Ensemble\n",
    "print(np.percentile(y_val_pred, 95))\n",
    "\n",
    "# 10th Percentile\n",
    "# LightGBM 1\n",
    "print(np.percentile(y_val_pred_gbm1, 90))\n",
    "# LightGBM 2\n",
    "print(np.percentile(y_val_pred_gbm2, 90))\n",
    "# Bagged NN\n",
    "print(np.percentile(y_val_pred_NN, 90))\n",
    "# Probability Ensemble\n",
    "print(np.percentile(y_val_pred, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM 1 5th %:\n",
      "[[7765  115]\n",
      " [ 100  300]]\n",
      "LightGBM 2 5th %:\n",
      "[[7748  132]\n",
      " [ 118  282]]\n",
      "Bagged NN 5th %:\n",
      "[[7736  144]\n",
      " [ 130  270]]\n",
      "Ensemble 1 5th %:\n",
      "[[7765  115]\n",
      " [ 100  300]]\n",
      "\n",
      "LightGBM 1 10th %:\n",
      "[[7394  486]\n",
      " [  58  342]]\n",
      "LightGBM 2 10th %:\n",
      "[[7391  489]\n",
      " [  59  341]]\n",
      "Bagged NN 10th %:\n",
      "[[7379  501]\n",
      " [  72  328]]\n",
      "Ensemble 1 10th %:\n",
      "[[7397  483]\n",
      " [  52  348]]\n"
     ]
    }
   ],
   "source": [
    "print('LightGBM 1 5th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred_gbm1 > round(np.percentile(y_val_pred_gbm1, 95),3)))\n",
    "print('LightGBM 2 5th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred_gbm2 > round(np.percentile(y_val_pred_gbm2, 95),3)))\n",
    "print('Bagged NN 5th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred_NN > round(np.percentile(y_val_pred_NN, 95),3)))\n",
    "print('Ensemble 1 5th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred > round(np.percentile(y_val_pred, 95),3)))\n",
    "print(\"\")\n",
    "print('LightGBM 1 10th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred_gbm1 > round(np.percentile(y_val_pred_gbm1, 90),3)))\n",
    "print('LightGBM 2 10th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred_gbm2 > round(np.percentile(y_val_pred_gbm2, 90),3)))\n",
    "print('Bagged NN 10th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred_NN > round(np.percentile(y_val_pred_NN, 90),3)))\n",
    "print('Ensemble 1 10th %:')\n",
    "print(metrics.confusion_matrix(y_val,y_val_pred > round(np.percentile(y_val_pred, 90),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check all previous models using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar to above perf_metrics function\n",
    "\n",
    "# y_test, y_test_pred are test vector and model output vector\n",
    "# vtype = 1 for 2-column array of probabilities (NN output)\n",
    "# vtype = 2 for 1-column array of probabilities (LGBM output)\n",
    "def test_metrics(y_test, y_test_pred, vtype): \n",
    "# Making predictions\n",
    "    if(vtype == 1):\n",
    "        y_test_pred = y_test_pred[:,1] \n",
    "    if(vtype == 2):\n",
    "        y_test_pred = y_test_pred[:,] \n",
    "        \n",
    "    test_accuracy = roc_auc_score(y_test, y_test_pred)\n",
    "    print('The test ROC is', test_accuracy)\n",
    "\n",
    "    \n",
    "    test_mat = metrics.confusion_matrix(y_test,y_test_pred > 0.5)\n",
    "    print(\"Test Set Confusion Mat.:\")\n",
    "    print(test_mat)\n",
    "    print(\"Precision: \", test_mat[1][1]/(test_mat[0][1]+test_mat[1][1]))\n",
    "    print(\"Recall: \", test_mat[1][1]/(test_mat[1][0]+test_mat[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test ROC is 0.49315122151113044\n",
      "Test Set Confusion Mat.:\n",
      "[[6644 1258]\n",
      " [ 323   55]]\n",
      "Precision:  0.041888804265041886\n",
      "Recall:  0.1455026455026455\n"
     ]
    }
   ],
   "source": [
    "#Base model\n",
    "class_weight = {0: 1.0, 1: 50.0}\n",
    "model = RandomForestClassifier(max_depth=24, min_samples_leaf=42, min_samples_split=2, class_weight = class_weight)\n",
    "model.fit(X_train, y_train)\n",
    "pred_base = model.predict(X_val)\n",
    "test_metrics(y_test, pred_base, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test ROC is 0.8083517132492076\n",
      "Test Set Confusion Mat.:\n",
      "[[6232 1670]\n",
      " [  65  313]]\n",
      "Precision:  0.1578416540595058\n",
      "Recall:  0.828042328042328\n",
      "The test ROC is 0.8593936435622085\n",
      "Test Set Confusion Mat.:\n",
      "[[7185  717]\n",
      " [  72  306]]\n",
      "Precision:  0.2991202346041056\n",
      "Recall:  0.8095238095238095\n",
      "The test ROC is 0.7382971158597582\n",
      "Test Set Confusion Mat.:\n",
      "[[6630 1272]\n",
      " [ 137  241]]\n",
      "Precision:  0.15928618638466624\n",
      "Recall:  0.6375661375661376\n"
     ]
    }
   ],
   "source": [
    "#SVMs\n",
    "\n",
    "#Load models\n",
    "model_poly = joblib.load('SVM_models/poly_model.pkl')\n",
    "model_rbf = joblib.load('SVM_models/rbf_model.pkl')\n",
    "model_linear = joblib.load('SVM_models/linear_model.pkl')\n",
    "\n",
    "#Predictions and metrics for each model\n",
    "pred_poly = model_poly.predict(X_test)\n",
    "test_metrics(y_test, pred_poly, 0)\n",
    "pred_rbf = model_rbf.predict(X_test)\n",
    "test_metrics(y_test, pred_rbf, 0)\n",
    "pred_linear = model_linear.predict(X_test)\n",
    "test_metrics(y_test, pred_linear, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test ROC is 0.7550951537284112\n",
      "Test Set Confusion Mat.:\n",
      "[[7878   24]\n",
      " [ 184  194]]\n",
      "Precision:  0.8899082568807339\n",
      "Recall:  0.5132275132275133\n",
      "The test ROC is 0.6944444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Confusion Mat.:\n",
      "[[7902    0]\n",
      " [ 231  147]]\n",
      "Precision:  1.0\n",
      "Recall:  0.3888888888888889\n",
      "The test ROC is 0.9740814729108833\n",
      "Test Set Confusion Mat.:\n",
      "[[7892   10]\n",
      " [ 176  202]]\n",
      "Precision:  0.9528301886792453\n",
      "Recall:  0.5343915343915344\n",
      "The test ROC is 0.9708164432284907\n",
      "Test Set Confusion Mat.:\n",
      "[[7741  161]\n",
      " [ 102  276]]\n",
      "Precision:  0.631578947368421\n",
      "Recall:  0.7301587301587301\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosted Forests\n",
    "\n",
    "#Load models\n",
    "model_skl = joblib.load('RF_models/SKL_GB.pkl')\n",
    "model_xgb = joblib.load('RF_models/XGB_model.pkl')\n",
    "model_lgbm1 = joblib.load('RF_models/LightGBM_model.pkl')\n",
    "model_lgbm2 = joblib.load('RF_models/LightGBM_model_balanced.pkl')\n",
    "\n",
    "#Predictions and metrics for each model\n",
    "pred_skl = model_skl.predict(X_test)\n",
    "test_metrics(y_test, pred_skl, 0)\n",
    "pred_xgb = model_xgb.predict(X_test)\n",
    "test_metrics(y_test, pred_xgb, 0)\n",
    "pred_lgbm1 = model_lgbm1.predict(X_test)\n",
    "test_metrics(y_test, pred_lgbm1, 2)\n",
    "pred_lgbm2 = model_lgbm2.predict(X_test)\n",
    "test_metrics(y_test, pred_lgbm2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Layer Model:\n",
      "The test ROC is 0.9178118124270997\n",
      "Test Set Confusion Mat.:\n",
      "[[7519  383]\n",
      " [ 100  278]]\n",
      "Precision:  0.4205748865355522\n",
      "Recall:  0.7354497354497355\n",
      "\n",
      "Deep Layer Model:\n",
      "The test ROC is 0.9521790746164321\n",
      "Test Set Confusion Mat.:\n",
      "[[7553  349]\n",
      " [  85  293]]\n",
      "Precision:  0.45638629283489096\n",
      "Recall:  0.7751322751322751\n"
     ]
    }
   ],
   "source": [
    "#NN\n",
    "\n",
    "#Load model\n",
    "model_single = load_model('NN_models/single_layer.h5')\n",
    "model_deep = load_model('NN_models/deep_layer.h5')\n",
    "\n",
    "#Predictions and metrics for each model\n",
    "print(\"Single Layer Model:\")\n",
    "pred_single = model_single.predict(X_test)\n",
    "test_metrics(y_test, pred_single, 1)\n",
    "print(\"\")\n",
    "print(\"Deep Layer Model:\")\n",
    "pred_deep = model_deep.predict(X_test)\n",
    "test_metrics(y_test, pred_deep, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagged model\n",
    "from keras.models import load_model\n",
    "\n",
    "pred_NN = np.zeros(len(y_test))\n",
    "\n",
    "for i in range(1,25):\n",
    "    model = load_model('NN_models/NN_single{0}.h5'.format(i))\n",
    "    test_temp = model.predict(X_test)\n",
    "    pred_NN += test_temp[:,1]\n",
    "    \n",
    "pred_NN = pred_NN /24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test ROC is 0.9471065191452435\n",
      "Test Set Confusion Mat.:\n",
      "[[7639  263]\n",
      " [ 106  272]]\n",
      "Precision:  0.508411214953271\n",
      "Recall:  0.7195767195767195\n"
     ]
    }
   ],
   "source": [
    "test_metrics(y_test, pred_NN, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test ROC is 0.973496094351574\n",
      "Test Set Confusion Mat.:\n",
      "[[7258  644]\n",
      " [  39  339]]\n",
      "Precision:  0.34486266531027465\n",
      "Recall:  0.8968253968253969\n",
      "None\n",
      "The test ROC is 0.8974822528353281\n",
      "Test Set Confusion Mat.:\n",
      "[[7557  345]\n",
      " [  61  317]]\n",
      "Precision:  0.4788519637462236\n",
      "Recall:  0.8386243386243386\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Ensemble Model - Probability\n",
    "pred_ensemble_prob = (pred_NN*1.) + (pred_lgbm1*1.) + (pred_lgbm2*1.) + (pred_xgb*0.5)\n",
    "print(test_metrics(y_test, pred_ensemble_prob, 2))\n",
    "\n",
    "\n",
    "#Ensemble Model - Voting\n",
    "pred_ensemble_vote = (pred_NN > 0.5)*1. + (pred_lgbm1 > 0.5)*1. + (pred_lgbm2 > 0.5)*1 + (pred_xgb > 0.5)*1. \n",
    "pred_ensemble_vote = pred_ensemble_vote > 0\n",
    "print(test_metrics(y_test, pred_ensemble_vote, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morga\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: This function is deprecated. Please call randint(0, 8279 + 1) instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap #1\n",
      "Bootstrap #2\n",
      "Bootstrap #3\n",
      "Bootstrap #4\n",
      "Bootstrap #5\n",
      "Bootstrap #6\n",
      "Bootstrap #7\n",
      "Bootstrap #8\n",
      "Bootstrap #9\n",
      "Bootstrap #10\n",
      "Bootstrap #11\n",
      "Bootstrap #12\n",
      "Bootstrap #13\n",
      "Bootstrap #14\n",
      "Bootstrap #15\n",
      "Bootstrap #16\n",
      "Bootstrap #17\n",
      "Bootstrap #18\n",
      "Bootstrap #19\n",
      "Bootstrap #20\n",
      "Bootstrap #21\n",
      "Bootstrap #22\n",
      "Bootstrap #23\n",
      "Bootstrap #24\n",
      "Bootstrap #25\n",
      "Bootstrap #26\n",
      "Bootstrap #27\n",
      "Bootstrap #28\n",
      "Bootstrap #29\n",
      "Bootstrap #30\n",
      "Bootstrap #31\n",
      "Bootstrap #32\n",
      "Bootstrap #33\n",
      "Bootstrap #34\n",
      "Bootstrap #35\n",
      "Bootstrap #36\n",
      "Bootstrap #37\n",
      "Bootstrap #38\n",
      "Bootstrap #39\n",
      "Bootstrap #40\n",
      "Bootstrap #41\n",
      "Bootstrap #42\n",
      "Bootstrap #43\n",
      "Bootstrap #44\n",
      "Bootstrap #45\n",
      "Bootstrap #46\n",
      "Bootstrap #47\n",
      "Bootstrap #48\n",
      "Bootstrap #49\n",
      "Bootstrap #50\n",
      "Bootstrap #51\n",
      "Bootstrap #52\n",
      "Bootstrap #53\n",
      "Bootstrap #54\n",
      "Bootstrap #55\n",
      "Bootstrap #56\n",
      "Bootstrap #57\n",
      "Bootstrap #58\n",
      "Bootstrap #59\n",
      "Bootstrap #60\n",
      "Bootstrap #61\n",
      "Bootstrap #62\n",
      "Bootstrap #63\n",
      "Bootstrap #64\n",
      "Bootstrap #65\n",
      "Bootstrap #66\n",
      "Bootstrap #67\n",
      "Bootstrap #68\n",
      "Bootstrap #69\n",
      "Bootstrap #70\n",
      "Bootstrap #71\n",
      "Bootstrap #72\n",
      "Bootstrap #73\n",
      "Bootstrap #74\n",
      "Bootstrap #75\n",
      "Bootstrap #76\n",
      "Bootstrap #77\n",
      "Bootstrap #78\n",
      "Bootstrap #79\n",
      "Bootstrap #80\n",
      "Bootstrap #81\n",
      "Bootstrap #82\n",
      "Bootstrap #83\n",
      "Bootstrap #84\n",
      "Bootstrap #85\n",
      "Bootstrap #86\n",
      "Bootstrap #87\n",
      "Bootstrap #88\n",
      "Bootstrap #89\n",
      "Bootstrap #90\n",
      "Bootstrap #91\n",
      "Bootstrap #92\n",
      "Bootstrap #93\n",
      "Bootstrap #94\n",
      "Bootstrap #95\n",
      "Bootstrap #96\n",
      "Bootstrap #97\n",
      "Bootstrap #98\n",
      "Bootstrap #99\n",
      "Bootstrap #100\n",
      "Bootstrap #101\n",
      "Bootstrap #102\n",
      "Bootstrap #103\n",
      "Bootstrap #104\n",
      "Bootstrap #105\n",
      "Bootstrap #106\n",
      "Bootstrap #107\n",
      "Bootstrap #108\n",
      "Bootstrap #109\n",
      "Bootstrap #110\n",
      "Bootstrap #111\n",
      "Bootstrap #112\n",
      "Bootstrap #113\n",
      "Bootstrap #114\n",
      "Bootstrap #115\n",
      "Bootstrap #116\n",
      "Bootstrap #117\n",
      "Bootstrap #118\n",
      "Bootstrap #119\n",
      "Bootstrap #120\n",
      "Bootstrap #121\n",
      "Bootstrap #122\n",
      "Bootstrap #123\n",
      "Bootstrap #124\n",
      "Bootstrap #125\n",
      "Bootstrap #126\n",
      "Bootstrap #127\n",
      "Bootstrap #128\n",
      "Bootstrap #129\n",
      "Bootstrap #130\n",
      "Bootstrap #131\n",
      "Bootstrap #132\n",
      "Bootstrap #133\n",
      "Bootstrap #134\n",
      "Bootstrap #135\n",
      "Bootstrap #136\n",
      "Bootstrap #137\n",
      "Bootstrap #138\n",
      "Bootstrap #139\n",
      "Bootstrap #140\n",
      "Bootstrap #141\n",
      "Bootstrap #142\n",
      "Bootstrap #143\n",
      "Bootstrap #144\n",
      "Bootstrap #145\n",
      "Bootstrap #146\n",
      "Bootstrap #147\n",
      "Bootstrap #148\n",
      "Bootstrap #149\n",
      "Bootstrap #150\n",
      "Bootstrap #151\n",
      "Bootstrap #152\n",
      "Bootstrap #153\n",
      "Bootstrap #154\n",
      "Bootstrap #155\n",
      "Bootstrap #156\n",
      "Bootstrap #157\n",
      "Bootstrap #158\n",
      "Bootstrap #159\n",
      "Bootstrap #160\n",
      "Bootstrap #161\n",
      "Bootstrap #162\n",
      "Bootstrap #163\n",
      "Bootstrap #164\n",
      "Bootstrap #165\n",
      "Bootstrap #166\n",
      "Bootstrap #167\n",
      "Bootstrap #168\n",
      "Bootstrap #169\n",
      "Bootstrap #170\n",
      "Bootstrap #171\n",
      "Bootstrap #172\n",
      "Bootstrap #173\n",
      "Bootstrap #174\n",
      "Bootstrap #175\n",
      "Bootstrap #176\n",
      "Bootstrap #177\n",
      "Bootstrap #178\n",
      "Bootstrap #179\n",
      "Bootstrap #180\n",
      "Bootstrap #181\n",
      "Bootstrap #182\n",
      "Bootstrap #183\n",
      "Bootstrap #184\n",
      "Bootstrap #185\n",
      "Bootstrap #186\n",
      "Bootstrap #187\n",
      "Bootstrap #188\n",
      "Bootstrap #189\n",
      "Bootstrap #190\n",
      "Bootstrap #191\n",
      "Bootstrap #192\n",
      "Bootstrap #193\n",
      "Bootstrap #194\n",
      "Bootstrap #195\n",
      "Bootstrap #196\n",
      "Bootstrap #197\n",
      "Bootstrap #198\n",
      "Bootstrap #199\n",
      "Bootstrap #200\n",
      "Bootstrap #201\n",
      "Bootstrap #202\n",
      "Bootstrap #203\n",
      "Bootstrap #204\n",
      "Bootstrap #205\n",
      "Bootstrap #206\n",
      "Bootstrap #207\n",
      "Bootstrap #208\n",
      "Bootstrap #209\n",
      "Bootstrap #210\n",
      "Bootstrap #211\n",
      "Bootstrap #212\n",
      "Bootstrap #213\n",
      "Bootstrap #214\n",
      "Bootstrap #215\n",
      "Bootstrap #216\n",
      "Bootstrap #217\n",
      "Bootstrap #218\n",
      "Bootstrap #219\n",
      "Bootstrap #220\n",
      "Bootstrap #221\n",
      "Bootstrap #222\n",
      "Bootstrap #223\n",
      "Bootstrap #224\n",
      "Bootstrap #225\n",
      "Bootstrap #226\n",
      "Bootstrap #227\n",
      "Bootstrap #228\n",
      "Bootstrap #229\n",
      "Bootstrap #230\n",
      "Bootstrap #231\n",
      "Bootstrap #232\n",
      "Bootstrap #233\n",
      "Bootstrap #234\n",
      "Bootstrap #235\n",
      "Bootstrap #236\n",
      "Bootstrap #237\n",
      "Bootstrap #238\n",
      "Bootstrap #239\n",
      "Bootstrap #240\n",
      "Bootstrap #241\n",
      "Bootstrap #242\n",
      "Bootstrap #243\n",
      "Bootstrap #244\n",
      "Bootstrap #245\n",
      "Bootstrap #246\n",
      "Bootstrap #247\n",
      "Bootstrap #248\n",
      "Bootstrap #249\n",
      "Bootstrap #250\n",
      "Bootstrap #251\n",
      "Bootstrap #252\n",
      "Bootstrap #253\n",
      "Bootstrap #254\n",
      "Bootstrap #255\n",
      "Bootstrap #256\n",
      "Bootstrap #257\n",
      "Bootstrap #258\n",
      "Bootstrap #259\n",
      "Bootstrap #260\n",
      "Bootstrap #261\n",
      "Bootstrap #262\n",
      "Bootstrap #263\n",
      "Bootstrap #264\n",
      "Bootstrap #265\n",
      "Bootstrap #266\n",
      "Bootstrap #267\n",
      "Bootstrap #268\n",
      "Bootstrap #269\n",
      "Bootstrap #270\n",
      "Bootstrap #271\n",
      "Bootstrap #272\n",
      "Bootstrap #273\n",
      "Bootstrap #274\n",
      "Bootstrap #275\n",
      "Bootstrap #276\n",
      "Bootstrap #277\n",
      "Bootstrap #278\n",
      "Bootstrap #279\n",
      "Bootstrap #280\n",
      "Bootstrap #281\n",
      "Bootstrap #282\n",
      "Bootstrap #283\n",
      "Bootstrap #284\n",
      "Bootstrap #285\n",
      "Bootstrap #286\n",
      "Bootstrap #287\n",
      "Bootstrap #288\n",
      "Bootstrap #289\n",
      "Bootstrap #290\n",
      "Bootstrap #291\n",
      "Bootstrap #292\n",
      "Bootstrap #293\n",
      "Bootstrap #294\n",
      "Bootstrap #295\n",
      "Bootstrap #296\n",
      "Bootstrap #297\n",
      "Bootstrap #298\n",
      "Bootstrap #299\n",
      "Bootstrap #300\n",
      "Bootstrap #301\n",
      "Bootstrap #302\n",
      "Bootstrap #303\n",
      "Bootstrap #304\n",
      "Bootstrap #305\n",
      "Bootstrap #306\n",
      "Bootstrap #307\n",
      "Bootstrap #308\n",
      "Bootstrap #309\n",
      "Bootstrap #310\n",
      "Bootstrap #311\n",
      "Bootstrap #312\n",
      "Bootstrap #313\n",
      "Bootstrap #314\n",
      "Bootstrap #315\n",
      "Bootstrap #316\n",
      "Bootstrap #317\n",
      "Bootstrap #318\n",
      "Bootstrap #319\n",
      "Bootstrap #320\n",
      "Bootstrap #321\n",
      "Bootstrap #322\n",
      "Bootstrap #323\n",
      "Bootstrap #324\n",
      "Bootstrap #325\n",
      "Bootstrap #326\n",
      "Bootstrap #327\n",
      "Bootstrap #328\n",
      "Bootstrap #329\n",
      "Bootstrap #330\n",
      "Bootstrap #331\n",
      "Bootstrap #332\n",
      "Bootstrap #333\n",
      "Bootstrap #334\n",
      "Bootstrap #335\n",
      "Bootstrap #336\n",
      "Bootstrap #337\n",
      "Bootstrap #338\n",
      "Bootstrap #339\n",
      "Bootstrap #340\n",
      "Bootstrap #341\n",
      "Bootstrap #342\n",
      "Bootstrap #343\n",
      "Bootstrap #344\n",
      "Bootstrap #345\n",
      "Bootstrap #346\n",
      "Bootstrap #347\n",
      "Bootstrap #348\n",
      "Bootstrap #349\n",
      "Bootstrap #350\n",
      "Bootstrap #351\n",
      "Bootstrap #352\n",
      "Bootstrap #353\n",
      "Bootstrap #354\n",
      "Bootstrap #355\n",
      "Bootstrap #356\n",
      "Bootstrap #357\n",
      "Bootstrap #358\n",
      "Bootstrap #359\n",
      "Bootstrap #360\n",
      "Bootstrap #361\n",
      "Bootstrap #362\n",
      "Bootstrap #363\n",
      "Bootstrap #364\n",
      "Bootstrap #365\n",
      "Bootstrap #366\n",
      "Bootstrap #367\n",
      "Bootstrap #368\n",
      "Bootstrap #369\n",
      "Bootstrap #370\n",
      "Bootstrap #371\n",
      "Bootstrap #372\n",
      "Bootstrap #373\n",
      "Bootstrap #374\n",
      "Bootstrap #375\n",
      "Bootstrap #376\n",
      "Bootstrap #377\n",
      "Bootstrap #378\n",
      "Bootstrap #379\n",
      "Bootstrap #380\n",
      "Bootstrap #381\n",
      "Bootstrap #382\n",
      "Bootstrap #383\n",
      "Bootstrap #384\n",
      "Bootstrap #385\n",
      "Bootstrap #386\n",
      "Bootstrap #387\n",
      "Bootstrap #388\n",
      "Bootstrap #389\n",
      "Bootstrap #390\n",
      "Bootstrap #391\n",
      "Bootstrap #392\n",
      "Bootstrap #393\n",
      "Bootstrap #394\n",
      "Bootstrap #395\n",
      "Bootstrap #396\n",
      "Bootstrap #397\n",
      "Bootstrap #398\n",
      "Bootstrap #399\n",
      "Bootstrap #400\n",
      "Bootstrap #401\n",
      "Bootstrap #402\n",
      "Bootstrap #403\n",
      "Bootstrap #404\n",
      "Bootstrap #405\n",
      "Bootstrap #406\n",
      "Bootstrap #407\n",
      "Bootstrap #408\n",
      "Bootstrap #409\n",
      "Bootstrap #410\n",
      "Bootstrap #411\n",
      "Bootstrap #412\n",
      "Bootstrap #413\n",
      "Bootstrap #414\n",
      "Bootstrap #415\n",
      "Bootstrap #416\n",
      "Bootstrap #417\n",
      "Bootstrap #418\n",
      "Bootstrap #419\n",
      "Bootstrap #420\n",
      "Bootstrap #421\n",
      "Bootstrap #422\n",
      "Bootstrap #423\n",
      "Bootstrap #424\n",
      "Bootstrap #425\n",
      "Bootstrap #426\n",
      "Bootstrap #427\n",
      "Bootstrap #428\n",
      "Bootstrap #429\n",
      "Bootstrap #430\n",
      "Bootstrap #431\n",
      "Bootstrap #432\n",
      "Bootstrap #433\n",
      "Bootstrap #434\n",
      "Bootstrap #435\n",
      "Bootstrap #436\n",
      "Bootstrap #437\n",
      "Bootstrap #438\n",
      "Bootstrap #439\n",
      "Bootstrap #440\n",
      "Bootstrap #441\n",
      "Bootstrap #442\n",
      "Bootstrap #443\n",
      "Bootstrap #444\n",
      "Bootstrap #445\n",
      "Bootstrap #446\n",
      "Bootstrap #447\n",
      "Bootstrap #448\n",
      "Bootstrap #449\n",
      "Bootstrap #450\n",
      "Bootstrap #451\n",
      "Bootstrap #452\n",
      "Bootstrap #453\n",
      "Bootstrap #454\n",
      "Bootstrap #455\n",
      "Bootstrap #456\n",
      "Bootstrap #457\n",
      "Bootstrap #458\n",
      "Bootstrap #459\n",
      "Bootstrap #460\n",
      "Bootstrap #461\n",
      "Bootstrap #462\n",
      "Bootstrap #463\n",
      "Bootstrap #464\n",
      "Bootstrap #465\n",
      "Bootstrap #466\n",
      "Bootstrap #467\n",
      "Bootstrap #468\n",
      "Bootstrap #469\n",
      "Bootstrap #470\n",
      "Bootstrap #471\n",
      "Bootstrap #472\n",
      "Bootstrap #473\n",
      "Bootstrap #474\n",
      "Bootstrap #475\n",
      "Bootstrap #476\n",
      "Bootstrap #477\n",
      "Bootstrap #478\n",
      "Bootstrap #479\n",
      "Bootstrap #480\n",
      "Bootstrap #481\n",
      "Bootstrap #482\n",
      "Bootstrap #483\n",
      "Bootstrap #484\n",
      "Bootstrap #485\n",
      "Bootstrap #486\n",
      "Bootstrap #487\n",
      "Bootstrap #488\n",
      "Bootstrap #489\n",
      "Bootstrap #490\n",
      "Bootstrap #491\n",
      "Bootstrap #492\n",
      "Bootstrap #493\n",
      "Bootstrap #494\n",
      "Bootstrap #495\n",
      "Bootstrap #496\n",
      "Bootstrap #497\n",
      "Bootstrap #498\n",
      "Bootstrap #499\n",
      "Bootstrap #500\n",
      "Bootstrap #501\n",
      "Bootstrap #502\n",
      "Bootstrap #503\n",
      "Bootstrap #504\n",
      "Bootstrap #505\n",
      "Bootstrap #506\n",
      "Bootstrap #507\n",
      "Bootstrap #508\n",
      "Bootstrap #509\n",
      "Bootstrap #510\n",
      "Bootstrap #511\n",
      "Bootstrap #512\n",
      "Bootstrap #513\n",
      "Bootstrap #514\n",
      "Bootstrap #515\n",
      "Bootstrap #516\n",
      "Bootstrap #517\n",
      "Bootstrap #518\n",
      "Bootstrap #519\n",
      "Bootstrap #520\n",
      "Bootstrap #521\n",
      "Bootstrap #522\n",
      "Bootstrap #523\n",
      "Bootstrap #524\n",
      "Bootstrap #525\n",
      "Bootstrap #526\n",
      "Bootstrap #527\n",
      "Bootstrap #528\n",
      "Bootstrap #529\n",
      "Bootstrap #530\n",
      "Bootstrap #531\n",
      "Bootstrap #532\n",
      "Bootstrap #533\n",
      "Bootstrap #534\n",
      "Bootstrap #535\n",
      "Bootstrap #536\n",
      "Bootstrap #537\n",
      "Bootstrap #538\n",
      "Bootstrap #539\n",
      "Bootstrap #540\n",
      "Bootstrap #541\n",
      "Bootstrap #542\n",
      "Bootstrap #543\n",
      "Bootstrap #544\n",
      "Bootstrap #545\n",
      "Bootstrap #546\n",
      "Bootstrap #547\n",
      "Bootstrap #548\n",
      "Bootstrap #549\n",
      "Bootstrap #550\n",
      "Bootstrap #551\n",
      "Bootstrap #552\n",
      "Bootstrap #553\n",
      "Bootstrap #554\n",
      "Bootstrap #555\n",
      "Bootstrap #556\n",
      "Bootstrap #557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap #558\n",
      "Bootstrap #559\n",
      "Bootstrap #560\n",
      "Bootstrap #561\n",
      "Bootstrap #562\n",
      "Bootstrap #563\n",
      "Bootstrap #564\n",
      "Bootstrap #565\n",
      "Bootstrap #566\n",
      "Bootstrap #567\n",
      "Bootstrap #568\n",
      "Bootstrap #569\n",
      "Bootstrap #570\n",
      "Bootstrap #571\n",
      "Bootstrap #572\n",
      "Bootstrap #573\n",
      "Bootstrap #574\n",
      "Bootstrap #575\n",
      "Bootstrap #576\n",
      "Bootstrap #577\n",
      "Bootstrap #578\n",
      "Bootstrap #579\n",
      "Bootstrap #580\n",
      "Bootstrap #581\n",
      "Bootstrap #582\n",
      "Bootstrap #583\n",
      "Bootstrap #584\n",
      "Bootstrap #585\n",
      "Bootstrap #586\n",
      "Bootstrap #587\n",
      "Bootstrap #588\n",
      "Bootstrap #589\n",
      "Bootstrap #590\n",
      "Bootstrap #591\n",
      "Bootstrap #592\n",
      "Bootstrap #593\n",
      "Bootstrap #594\n",
      "Bootstrap #595\n",
      "Bootstrap #596\n",
      "Bootstrap #597\n",
      "Bootstrap #598\n",
      "Bootstrap #599\n",
      "Bootstrap #600\n",
      "Bootstrap #601\n",
      "Bootstrap #602\n",
      "Bootstrap #603\n",
      "Bootstrap #604\n",
      "Bootstrap #605\n",
      "Bootstrap #606\n",
      "Bootstrap #607\n",
      "Bootstrap #608\n",
      "Bootstrap #609\n",
      "Bootstrap #610\n",
      "Bootstrap #611\n",
      "Bootstrap #612\n",
      "Bootstrap #613\n",
      "Bootstrap #614\n",
      "Bootstrap #615\n",
      "Bootstrap #616\n",
      "Bootstrap #617\n",
      "Bootstrap #618\n",
      "Bootstrap #619\n",
      "Bootstrap #620\n",
      "Bootstrap #621\n",
      "Bootstrap #622\n",
      "Bootstrap #623\n",
      "Bootstrap #624\n",
      "Bootstrap #625\n",
      "Bootstrap #626\n",
      "Bootstrap #627\n",
      "Bootstrap #628\n",
      "Bootstrap #629\n",
      "Bootstrap #630\n",
      "Bootstrap #631\n",
      "Bootstrap #632\n",
      "Bootstrap #633\n",
      "Bootstrap #634\n",
      "Bootstrap #635\n",
      "Bootstrap #636\n",
      "Bootstrap #637\n",
      "Bootstrap #638\n",
      "Bootstrap #639\n",
      "Bootstrap #640\n",
      "Bootstrap #641\n",
      "Bootstrap #642\n",
      "Bootstrap #643\n",
      "Bootstrap #644\n",
      "Bootstrap #645\n",
      "Bootstrap #646\n",
      "Bootstrap #647\n",
      "Bootstrap #648\n",
      "Bootstrap #649\n",
      "Bootstrap #650\n",
      "Bootstrap #651\n",
      "Bootstrap #652\n",
      "Bootstrap #653\n",
      "Bootstrap #654\n",
      "Bootstrap #655\n",
      "Bootstrap #656\n",
      "Bootstrap #657\n",
      "Bootstrap #658\n",
      "Bootstrap #659\n",
      "Bootstrap #660\n",
      "Bootstrap #661\n",
      "Bootstrap #662\n",
      "Bootstrap #663\n",
      "Bootstrap #664\n",
      "Bootstrap #665\n",
      "Bootstrap #666\n",
      "Bootstrap #667\n",
      "Bootstrap #668\n",
      "Bootstrap #669\n",
      "Bootstrap #670\n",
      "Bootstrap #671\n",
      "Bootstrap #672\n",
      "Bootstrap #673\n",
      "Bootstrap #674\n",
      "Bootstrap #675\n",
      "Bootstrap #676\n",
      "Bootstrap #677\n",
      "Bootstrap #678\n",
      "Bootstrap #679\n",
      "Bootstrap #680\n",
      "Bootstrap #681\n",
      "Bootstrap #682\n",
      "Bootstrap #683\n",
      "Bootstrap #684\n",
      "Bootstrap #685\n",
      "Bootstrap #686\n",
      "Bootstrap #687\n",
      "Bootstrap #688\n",
      "Bootstrap #689\n",
      "Bootstrap #690\n",
      "Bootstrap #691\n",
      "Bootstrap #692\n",
      "Bootstrap #693\n",
      "Bootstrap #694\n",
      "Bootstrap #695\n",
      "Bootstrap #696\n",
      "Bootstrap #697\n",
      "Bootstrap #698\n",
      "Bootstrap #699\n",
      "Bootstrap #700\n",
      "Bootstrap #701\n",
      "Bootstrap #702\n",
      "Bootstrap #703\n",
      "Bootstrap #704\n",
      "Bootstrap #705\n",
      "Bootstrap #706\n",
      "Bootstrap #707\n",
      "Bootstrap #708\n",
      "Bootstrap #709\n",
      "Bootstrap #710\n",
      "Bootstrap #711\n",
      "Bootstrap #712\n",
      "Bootstrap #713\n",
      "Bootstrap #714\n",
      "Bootstrap #715\n",
      "Bootstrap #716\n",
      "Bootstrap #717\n",
      "Bootstrap #718\n",
      "Bootstrap #719\n",
      "Bootstrap #720\n",
      "Bootstrap #721\n",
      "Bootstrap #722\n",
      "Bootstrap #723\n",
      "Bootstrap #724\n",
      "Bootstrap #725\n",
      "Bootstrap #726\n",
      "Bootstrap #727\n",
      "Bootstrap #728\n",
      "Bootstrap #729\n",
      "Bootstrap #730\n",
      "Bootstrap #731\n",
      "Bootstrap #732\n",
      "Bootstrap #733\n",
      "Bootstrap #734\n",
      "Bootstrap #735\n",
      "Bootstrap #736\n",
      "Bootstrap #737\n",
      "Bootstrap #738\n",
      "Bootstrap #739\n",
      "Bootstrap #740\n",
      "Bootstrap #741\n",
      "Bootstrap #742\n",
      "Bootstrap #743\n",
      "Bootstrap #744\n",
      "Bootstrap #745\n",
      "Bootstrap #746\n",
      "Bootstrap #747\n",
      "Bootstrap #748\n",
      "Bootstrap #749\n",
      "Bootstrap #750\n",
      "Bootstrap #751\n",
      "Bootstrap #752\n",
      "Bootstrap #753\n",
      "Bootstrap #754\n",
      "Bootstrap #755\n",
      "Bootstrap #756\n",
      "Bootstrap #757\n",
      "Bootstrap #758\n",
      "Bootstrap #759\n",
      "Bootstrap #760\n",
      "Bootstrap #761\n",
      "Bootstrap #762\n",
      "Bootstrap #763\n",
      "Bootstrap #764\n",
      "Bootstrap #765\n",
      "Bootstrap #766\n",
      "Bootstrap #767\n",
      "Bootstrap #768\n",
      "Bootstrap #769\n",
      "Bootstrap #770\n",
      "Bootstrap #771\n",
      "Bootstrap #772\n",
      "Bootstrap #773\n",
      "Bootstrap #774\n",
      "Bootstrap #775\n",
      "Bootstrap #776\n",
      "Bootstrap #777\n",
      "Bootstrap #778\n",
      "Bootstrap #779\n",
      "Bootstrap #780\n",
      "Bootstrap #781\n",
      "Bootstrap #782\n",
      "Bootstrap #783\n",
      "Bootstrap #784\n",
      "Bootstrap #785\n",
      "Bootstrap #786\n",
      "Bootstrap #787\n",
      "Bootstrap #788\n",
      "Bootstrap #789\n",
      "Bootstrap #790\n",
      "Bootstrap #791\n",
      "Bootstrap #792\n",
      "Bootstrap #793\n",
      "Bootstrap #794\n",
      "Bootstrap #795\n",
      "Bootstrap #796\n",
      "Bootstrap #797\n",
      "Bootstrap #798\n",
      "Bootstrap #799\n",
      "Bootstrap #800\n",
      "Bootstrap #801\n",
      "Bootstrap #802\n",
      "Bootstrap #803\n",
      "Bootstrap #804\n",
      "Bootstrap #805\n",
      "Bootstrap #806\n",
      "Bootstrap #807\n",
      "Bootstrap #808\n",
      "Bootstrap #809\n",
      "Bootstrap #810\n",
      "Bootstrap #811\n",
      "Bootstrap #812\n",
      "Bootstrap #813\n",
      "Bootstrap #814\n",
      "Bootstrap #815\n",
      "Bootstrap #816\n",
      "Bootstrap #817\n",
      "Bootstrap #818\n",
      "Bootstrap #819\n",
      "Bootstrap #820\n",
      "Bootstrap #821\n",
      "Bootstrap #822\n",
      "Bootstrap #823\n",
      "Bootstrap #824\n",
      "Bootstrap #825\n",
      "Bootstrap #826\n",
      "Bootstrap #827\n",
      "Bootstrap #828\n",
      "Bootstrap #829\n",
      "Bootstrap #830\n",
      "Bootstrap #831\n",
      "Bootstrap #832\n",
      "Bootstrap #833\n",
      "Bootstrap #834\n",
      "Bootstrap #835\n",
      "Bootstrap #836\n",
      "Bootstrap #837\n",
      "Bootstrap #838\n",
      "Bootstrap #839\n",
      "Bootstrap #840\n",
      "Bootstrap #841\n",
      "Bootstrap #842\n",
      "Bootstrap #843\n",
      "Bootstrap #844\n",
      "Bootstrap #845\n",
      "Bootstrap #846\n",
      "Bootstrap #847\n",
      "Bootstrap #848\n",
      "Bootstrap #849\n",
      "Bootstrap #850\n",
      "Bootstrap #851\n",
      "Bootstrap #852\n",
      "Bootstrap #853\n",
      "Bootstrap #854\n",
      "Bootstrap #855\n",
      "Bootstrap #856\n",
      "Bootstrap #857\n",
      "Bootstrap #858\n",
      "Bootstrap #859\n",
      "Bootstrap #860\n",
      "Bootstrap #861\n",
      "Bootstrap #862\n",
      "Bootstrap #863\n",
      "Bootstrap #864\n",
      "Bootstrap #865\n",
      "Bootstrap #866\n",
      "Bootstrap #867\n",
      "Bootstrap #868\n",
      "Bootstrap #869\n",
      "Bootstrap #870\n",
      "Bootstrap #871\n",
      "Bootstrap #872\n",
      "Bootstrap #873\n",
      "Bootstrap #874\n",
      "Bootstrap #875\n",
      "Bootstrap #876\n",
      "Bootstrap #877\n",
      "Bootstrap #878\n",
      "Bootstrap #879\n",
      "Bootstrap #880\n",
      "Bootstrap #881\n",
      "Bootstrap #882\n",
      "Bootstrap #883\n",
      "Bootstrap #884\n",
      "Bootstrap #885\n",
      "Bootstrap #886\n",
      "Bootstrap #887\n",
      "Bootstrap #888\n",
      "Bootstrap #889\n",
      "Bootstrap #890\n",
      "Bootstrap #891\n",
      "Bootstrap #892\n",
      "Bootstrap #893\n",
      "Bootstrap #894\n",
      "Bootstrap #895\n",
      "Bootstrap #896\n",
      "Bootstrap #897\n",
      "Bootstrap #898\n",
      "Bootstrap #899\n",
      "Bootstrap #900\n",
      "Bootstrap #901\n",
      "Bootstrap #902\n",
      "Bootstrap #903\n",
      "Bootstrap #904\n",
      "Bootstrap #905\n",
      "Bootstrap #906\n",
      "Bootstrap #907\n",
      "Bootstrap #908\n",
      "Bootstrap #909\n",
      "Bootstrap #910\n",
      "Bootstrap #911\n",
      "Bootstrap #912\n",
      "Bootstrap #913\n",
      "Bootstrap #914\n",
      "Bootstrap #915\n",
      "Bootstrap #916\n",
      "Bootstrap #917\n",
      "Bootstrap #918\n",
      "Bootstrap #919\n",
      "Bootstrap #920\n",
      "Bootstrap #921\n",
      "Bootstrap #922\n",
      "Bootstrap #923\n",
      "Bootstrap #924\n",
      "Bootstrap #925\n",
      "Bootstrap #926\n",
      "Bootstrap #927\n",
      "Bootstrap #928\n",
      "Bootstrap #929\n",
      "Bootstrap #930\n",
      "Bootstrap #931\n",
      "Bootstrap #932\n",
      "Bootstrap #933\n",
      "Bootstrap #934\n",
      "Bootstrap #935\n",
      "Bootstrap #936\n",
      "Bootstrap #937\n",
      "Bootstrap #938\n",
      "Bootstrap #939\n",
      "Bootstrap #940\n",
      "Bootstrap #941\n",
      "Bootstrap #942\n",
      "Bootstrap #943\n",
      "Bootstrap #944\n",
      "Bootstrap #945\n",
      "Bootstrap #946\n",
      "Bootstrap #947\n",
      "Bootstrap #948\n",
      "Bootstrap #949\n",
      "Bootstrap #950\n",
      "Bootstrap #951\n",
      "Bootstrap #952\n",
      "Bootstrap #953\n",
      "Bootstrap #954\n",
      "Bootstrap #955\n",
      "Bootstrap #956\n",
      "Bootstrap #957\n",
      "Bootstrap #958\n",
      "Bootstrap #959\n",
      "Bootstrap #960\n",
      "Bootstrap #961\n",
      "Bootstrap #962\n",
      "Bootstrap #963\n",
      "Bootstrap #964\n",
      "Bootstrap #965\n",
      "Bootstrap #966\n",
      "Bootstrap #967\n",
      "Bootstrap #968\n",
      "Bootstrap #969\n",
      "Bootstrap #970\n",
      "Bootstrap #971\n",
      "Bootstrap #972\n",
      "Bootstrap #973\n",
      "Bootstrap #974\n",
      "Bootstrap #975\n",
      "Bootstrap #976\n",
      "Bootstrap #977\n",
      "Bootstrap #978\n",
      "Bootstrap #979\n",
      "Bootstrap #980\n",
      "Bootstrap #981\n",
      "Bootstrap #982\n",
      "Bootstrap #983\n",
      "Bootstrap #984\n",
      "Bootstrap #985\n",
      "Bootstrap #986\n",
      "Bootstrap #987\n",
      "Bootstrap #988\n",
      "Bootstrap #989\n",
      "Bootstrap #990\n",
      "Bootstrap #991\n",
      "Bootstrap #992\n",
      "Bootstrap #993\n",
      "Bootstrap #994\n",
      "Bootstrap #995\n",
      "Bootstrap #996\n",
      "Bootstrap #997\n",
      "Bootstrap #998\n",
      "Bootstrap #999\n",
      "Bootstrap #1000\n",
      "Base:\n",
      "97.5th Percentile: 0.5128499287652685\n",
      "2.5th Percentile:  0.4742831293285829\n",
      "LGBM1:\n",
      "97.5th Percentile: 0.9801801950677889\n",
      "2.5th Percentile:  0.9671089167274304\n",
      "LGBM2:\n",
      "97.5th Percentile: 0.9765438035458966\n",
      "2.5th Percentile:  0.9644102005763281\n",
      "Deep NN:\n",
      "97.5th Percentile: 0.9630555024976375\n",
      "2.5th Percentile: 0.940196528270325\n",
      "Bagged NN:\n",
      "97.5th Percentile: 0.9583958020700976\n",
      "2.5th Percentile:  0.9335323768402202\n",
      "Ensemble:\n",
      "97.5th Percentile: 0.9794644365395556\n",
      "2.5th Percentile:  0.9666156317022521\n"
     ]
    }
   ],
   "source": [
    "# Compute confidence intervals of high performance models\n",
    "\n",
    "n_bootstraps = 1000\n",
    "rng_seed = 23  # control reproducibility\n",
    "bootstrapped_scores_base = []\n",
    "bootstrapped_scores_lgbm1 = []\n",
    "bootstrapped_scores_lgbm2 = []\n",
    "bootstrapped_scores_deep = []\n",
    "bootstrapped_scores_bagged = []\n",
    "bootstrapped_scores_ensemble = []\n",
    "\n",
    "rng = np.random.RandomState(rng_seed)\n",
    "for i in range(n_bootstraps):\n",
    "    # bootstrap by sampling with replacement on the prediction indices\n",
    "    indices = rng.random_integers(0, len(y_test) - 1, len(y_test))\n",
    "\n",
    "    score_base = roc_auc_score(np.array(y_test)[indices], pred_base[indices])\n",
    "    score_lgbm1 = roc_auc_score(np.array(y_test)[indices], pred_lgbm1[indices])\n",
    "    score_lgbm2 = roc_auc_score(np.array(y_test)[indices], pred_lgbm2[indices])\n",
    "    score_deep = roc_auc_score(np.array(y_test)[indices], pred_deep[indices,1])\n",
    "    score_bagged = roc_auc_score(np.array(y_test)[indices], pred_NN[indices])\n",
    "    score_ensemble = roc_auc_score(np.array(y_test)[indices], pred_ensemble_prob[indices])\n",
    "    \n",
    "    bootstrapped_scores_base.append(score_base)\n",
    "    bootstrapped_scores_lgbm1.append(score_lgbm1)\n",
    "    bootstrapped_scores_lgbm2.append(score_lgbm2)\n",
    "    bootstrapped_scores_deep.append(score_deep)\n",
    "    bootstrapped_scores_bagged.append(score_bagged)\n",
    "    bootstrapped_scores_ensemble.append(score_ensemble)\n",
    "    #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "    print(\"Bootstrap #{}\".format(i + 1))\n",
    "\n",
    "\n",
    "print('Base:')\n",
    "print(\"97.5th Percentile:\", np.percentile(bootstrapped_scores_base,97.5))\n",
    "print(\"2.5th Percentile: \", np.percentile(bootstrapped_scores_base,2.5))\n",
    "\n",
    "\n",
    "print('LGBM1:')\n",
    "print(\"97.5th Percentile:\", np.percentile(bootstrapped_scores_lgbm1,97.5))\n",
    "print(\"2.5th Percentile: \", np.percentile(bootstrapped_scores_lgbm1,2.5))\n",
    "\n",
    "print('LGBM2:')\n",
    "print(\"97.5th Percentile:\",np.percentile(bootstrapped_scores_lgbm2,97.5))\n",
    "print(\"2.5th Percentile: \", np.percentile(bootstrapped_scores_lgbm2,2.5))\n",
    "\n",
    "print('Deep NN:')\n",
    "print(\"97.5th Percentile:\",np.percentile(bootstrapped_scores_deep,97.5))\n",
    "print(\"2.5th Percentile:\" ,np.percentile(bootstrapped_scores_deep,2.5))\n",
    "\n",
    "print('Bagged NN:')\n",
    "print(\"97.5th Percentile:\", np.percentile(bootstrapped_scores_bagged,97.5))\n",
    "print(\"2.5th Percentile: \", np.percentile(bootstrapped_scores_bagged,2.5))\n",
    "\n",
    "print('Ensemble:')\n",
    "print(\"97.5th Percentile:\",np.percentile(bootstrapped_scores_ensemble,97.5))\n",
    "print(\"2.5th Percentile: \", np.percentile(bootstrapped_scores_ensemble,2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True y_test: 378\n",
      "LightGBM 1 5th %:\n",
      "[[7790  112]\n",
      " [  99  279]]\n",
      "LightGBM 2 5th %:\n",
      "[[7769  133]\n",
      " [ 117  261]]\n",
      "Ensemble 1 5th %:\n",
      "[[6956  946]\n",
      " [  22  356]]\n",
      "\n",
      "LightGBM 1 10th %:\n",
      "[[7710  192]\n",
      " [  78  300]]\n",
      "LightGBM 2 10th %:\n",
      "[[7722  180]\n",
      " [ 100  278]]\n",
      "Ensemble 1 10th %:\n",
      "[[6632 1270]\n",
      " [  20  358]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion mats based on earlier determined probability cutoffs\n",
    "print('True y_test:', np.sum(y_test))\n",
    "print('LightGBM 1 5th %:')\n",
    "print(metrics.confusion_matrix(y_test,pred_lgbm1 > 0.127))\n",
    "print('LightGBM 2 5th %:')\n",
    "print(metrics.confusion_matrix(y_test,pred_lgbm2 > 0.548))\n",
    "print('Ensemble 1 5th %:')\n",
    "print(metrics.confusion_matrix(y_test,pred_ensemble_prob > 0.346))\n",
    "print(\"\")\n",
    "print('LightGBM 1 10th %:')\n",
    "print(metrics.confusion_matrix(y_test,pred_lgbm1 > 0.083))\n",
    "print('LightGBM 2 10th %:')\n",
    "print(metrics.confusion_matrix(y_test,pred_lgbm2 > 0.479))\n",
    "print('Ensemble 1 10th %:')\n",
    "print(metrics.confusion_matrix(y_test,pred_ensemble_prob > 0.252))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
